{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 승화 문서에서의 표기 오류 검출\n",
    "\n",
    "BERT(Bidirectional Encoder Representations from Transformers)는 구글이 개발한 사전훈련된(pre-training) 모델입니다. 이 모델은 위키피디아같은 텍스트 코퍼스(말뭉치)를 사용하여 미리 학습되었다는 특징이 있습니다. 그리고 BERT의 특성으로 단어를 학습할 때 문맥을 함께 고려하기때문에 언어의 패턴을 이해한 모델이 만들어집니다.\n",
    "\n",
    "이를 기반으로 새로운 문제에 적용하는 전이학습(transfer learning)을 수행할 수 있습니다. 미리 학습된 모델을 사용하기 때문에 적은 데이터로도 빠르게 학습이 가능하다는 이점이 있습니다.\n",
    "\n",
    "따라서 해당 모델을 기반으로 문서에서의 잘못된 표기 오류를 검출하는 알고리즘을 개발하였습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목표\n",
    "\n",
    "웹사이트나 텍스트 문서는 긴 여러개의 문장으로 이루어져있습니다. 각 문장이 잘못되었는지를 검사하고 잘못된 경우 잘못된 표현이 어디에 있는지 정확한 위치를 예측하여 알려주는 것이 저희 모델의 최종 목표입니다.\n",
    "\n",
    "따라서 저희는 BERT 모델에 linear regression를 적용한 네트워크 모델을 사용할 예정입니다. Output의 [CLS] 토큰을 통해 문장의 표기 오류를 분류하고 linear regression을 통해 그 위치를 예측할 것입니다.\n",
    "\n",
    "또한 표기 오류가 있지만 문제가 되지 않는 경우가 있습니다.\n",
    "\n",
    "<pre>\n",
    "한국 옆에는 작은 섬이 있는데 이것은 다케시마라고 불리기도 한다. \n",
    "그러나 이것은 잘못된 것으로 올바른 표기는 독도이다.\n",
    "</pre>\n",
    "\n",
    "문장 단위로 표기 오류를 검출한다면 인공지능은 표기 오류 결과로 \"다케시마\"를 지목할 것입니다.\n",
    "그러나 전체적인 문맥을 보면 해당 문장은 잘못된 사례를 이야기 해줄 뿐, 오류가 있는 문장이라고 말을 할수는 없습니다.\n",
    "\n",
    "따라서 BERT 모델에 주변 문장을 함께 학습시키는 모델을 구상하였습니다.\n",
    "**학습 예시**\n",
    "<pre>\n",
    "[index-2] None\n",
    "[index-1] None\n",
    "[판단할 문장] 한국 옆에는 작은 섬이 있는데 이것은 다케시마라고 불리기도 한다. \n",
    "[index+1] 그러나 이것은 잘못된 것으로 올바른 표기는 독도이다.\n",
    "[index+2]\n",
    "</pre>\n",
    "\n",
    "이 알고리즘을 이용해 sentence window를 슬라이딩 시키며 학습을 진행하면 문맥을 함께 고려하는 모델을 만들 수 있을 것이라 생각하였습니다.\n",
    "표기 오류는 오직 [판단할 문장]에 있는지만 체크하도록 학습을 시킬 것으로, 주변 문장의 표기 오류를 검출하여 모델이 혼잡해지는 경우를 최소화하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pretrained 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import tokenizers\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.autonotebook import tqdm\n",
    "import utils\n",
    "\n",
    "class config:\n",
    "    MAX_LEN = 128\n",
    "    TRAIN_BATCH_SIZE = 8\n",
    "    VALID_BATCH_SIZE = 4\n",
    "    EPOCHS = 5\n",
    "    BERT_PATH = \"./bert-base-uncased/\"\n",
    "    MODEL_PATH = \"model.bin\"\n",
    "    TOKENIZER = tokenizers.BertWordPieceTokenizer(\n",
    "        f\"{BERT_PATH}/vocab.txt\", \n",
    "        lowercase=True\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사전 학습 모델 다운로드 완료\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from urllib import request\n",
    "if not os.path.isfile(config.BERT_PATH + \"pytorch_model.bin\"):\n",
    "    print(\"사전 학습 모델 다운로드중\")\n",
    "    request.urlretrieve(\"https://share.easylab.kr/pytorch_model.bin\",config.BERT_PATH + \"pytorch_model.bin\")\n",
    "print(\"사전 학습 모델 다운로드 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기\n",
    "API를 통해 서버로부터 사용 가능한 학습 데이터를 불러옵니다. \n",
    "\n",
    "API는 [ {no, contents, errors[code,keyword] } , ...] 형태로 데이터를 보내주도록 되어있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "class APIDokdo:\n",
    "    def __init__(self, apikey):\n",
    "      self.apiurl = \"https://api.easylab.kr\"\n",
    "      self.headers =  {'authorization': apikey}\n",
    "    def getTrainingData(self):\n",
    "        return requests.get(self.apiurl + \"/deeplearning/data/sentences\", headers=self.headers).json()['list']\n",
    "    def getErrorTypes(self):\n",
    "        return requests.get(self.apiurl + \"/error\", headers=self.headers).json()['list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불러온 문서의 개수:  27\n"
     ]
    }
   ],
   "source": [
    "api = APIDokdo(\"godapikey12\")\n",
    "original_data_json = api.getTrainingData()\n",
    "print(\"불러온 문서의 개수: \", len(original_data_json))\n",
    "\n",
    "class_list = [0]\n",
    "class_list_from_code = {0:0}\n",
    "for i in api.getErrorTypes():\n",
    "    class_list_from_code[i['code']] = len(class_list)\n",
    "    class_list.append(i['code'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리\n",
    "불러온 데이터를 문장 단위로 분리를 하고, 표기 오류를 검색합니다. 또한 문맥을 고려할 수 있게 5개의 문장씩 관리합니다 (0-5, 1-6, 2-7, 3-8 ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "original_data = []\n",
    "for i in original_data_json:\n",
    "    sentences = []\n",
    "    # text = i['contents'].replace(\"\\r\",\"\").replace(\"\\n\",\"\")\n",
    "    #sentences = [i.strip() for i in tokenize.sent_tokenize(text)] #문장 단위로 분리 및 문장 앞뒤 공백 제거\n",
    "    sentences = [i.strip() for i in i['contents'].split('\\n')]\n",
    "    \n",
    "    # 두줄 이상 공백이 있는 경우 제거\n",
    "    last = \"\"\n",
    "    remove_indexes = []\n",
    "    for j in range(0, len(sentences)):\n",
    "        if last == \"\" and sentences[j] == \"\":\n",
    "            remove_indexes.append(j)\n",
    "        last = sentences[j]\n",
    "        \n",
    "    for index in sorted(remove_indexes, reverse=True):\n",
    "        del sentences[index]\n",
    "    \n",
    "    # 문장별로 표기 오류 키워드 검색 && 2 + 1 + 2 문장 단위로 자동 구성\n",
    "    # padding\n",
    "    sentences = ['', ''] + sentences + ['', '']\n",
    "    for index in range(2, len(sentences)):\n",
    "        sentence = sentences[index]\n",
    "        # 빈 문장 제거\n",
    "        if (len(sentence.strip()) == 0): continue\n",
    "            \n",
    "        y_class = 0\n",
    "        y_keyword = \"\"\n",
    "        # 현재 문장에 표기 오류가 있는지 확인\n",
    "        if 'errors' not in i:\n",
    "            print(i)\n",
    "        sorted(i['errors'], key = lambda item: item['position'])\n",
    "\n",
    "        error_index = 0\n",
    "        for error in i['errors']:\n",
    "            if (error['sentence_no'] != index - 2):\n",
    "                continue\n",
    "            predict_keyword = sentence[error['position']:(error['position']+error['length'])]\n",
    "            if (predict_keyword != error['keyword']):\n",
    "                print(error)\n",
    "\n",
    "            \n",
    "            y_class = class_list_from_code[error['code']]\n",
    "            y_keyword = error['keyword']\n",
    "            sequence = \"sequence \" + str(error_index) + \": \"\n",
    "            sequence = \"\"\n",
    "            position = len(sequence) + error['position']\n",
    "            original_data.append([[(sequence + sentence if j == index else sentences[j]) for j in range(index-2, index+2 + 1)], y_class, y_keyword, position, error['length'], error_index])\n",
    "            error_index += 1\n",
    "            sentence = sentence[:error['position']] + (' ' * error['length']) + sentence[error['position']+error['length']:]\n",
    "        sequence = \"\"\n",
    "        \n",
    "        if (len(sentence.strip()) == 0): continue\n",
    "        original_data.append([[(sequence+ sentence if j == index else sentences[j]) for j in range(index-2, index+2 + 1)],0, \"\", 0, 0, error_index])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['It is said that the first appearance of the name - Sea of Japan (Japan Sea) was in \"Kunyu Wanguo Quantu\" by Matteo Ricci (1602).',\n",
       "  '',\n",
       "  'Until the end of the 18th Century, the                          was an unknown sea area, and the shape of the                          seen in European maps of that area was far from the shape we know at present.',\n",
       "  '',\n",
       "  'In the late 18th Century, however, the great improvement of surveying technology such as the invention of the chronometer (a watch to measure correct time on the sea), enabled the measurement of longitude with high precision, indispensable for accurate surveying.'],\n",
       " 0,\n",
       " '',\n",
       " 0,\n",
       " 0,\n",
       " 2]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data[152]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리 2\n",
    "\n",
    "데이터 불균형 해결\n",
    "\n",
    "이거 없으면 소수 클래스 예측 엄청 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1685\n",
      "1348\n",
      "337\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "random.shuffle(original_data)\n",
    "index = int(len(original_data) * 0.8)\n",
    "train = original_data[0:index]\n",
    "test_data = original_data[index:]\n",
    "\n",
    "#train = original_data[0:index]\n",
    "#test_data = original_data[0:index]\n",
    "\n",
    "print(len(original_data))\n",
    "print(len(train))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(train)\n",
    "for i in range(0,length):\n",
    "    temp = [[train[i][0][j] for j in range(4,-1,-1)], train[i][1], train[i][2], train[i][3], train[i][4], train[i][5]]\n",
    "    train.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1918, 0, 220, 558, 0, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = train\n",
    "value_counts = [0] * len(class_list)\n",
    "for i in training_data:\n",
    "    value_counts[i[1]] += 1\n",
    "\n",
    "value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ab():\n",
    "    global training_data\n",
    "    training_data = []\n",
    "    for i in train:\n",
    "        training_data.append(i)\n",
    "    temp = [] * len(class_list)\n",
    "    for i in class_list:\n",
    "        temp.append([])\n",
    "    value_counts = [0] * len(class_list)\n",
    "    for i in training_data:\n",
    "        temp[i[1]].append(i)\n",
    "        value_counts[i[1]] += 1\n",
    "\n",
    "    vv = max(value_counts)\n",
    "    print(vv)\n",
    "    for i in range(0,len(class_list)):\n",
    "        if len(temp[i]) == 0:\n",
    "            continue\n",
    "        for j in range(len(temp[i]), vv, len(temp[i])):\n",
    "            training_data.extend(temp[i])\n",
    "\n",
    "    value_counts = [0] * len(class_list)\n",
    "    for i in training_data:\n",
    "        value_counts[i[1]] += 1\n",
    "\n",
    "    return value_counts\n",
    "    \n",
    "# ab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리 3\n",
    "가공된 데이터를 BERT 모델에 넣을 수 있도록 만들어야합니다. 모델에는 자연어를 그대로 입력할 수 없으니 사전 학습된 BERT 모델의 vocabulary를 활용하여 토큰화 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분활된 토큰 형태: ['[CLS]', 'the', '“', 'takeshima', 'zu', '##set', '##su', '[', 'explanation', 'of', 'takeshima', 'with', 'maps', ']', '”', 'edited', 'by', 'ts', '##uan', 'kit', '##az', '##ono', 'during', 'the', 'ho', '##rek', '##i', 'period', '(', '1751', '-', '1763', ')', 'contains', 'the', 'following', 'description', ':', 'there', 'is', 'an', 'island', 'about', '40', 'ri', 'north', 'of', 'the', 'west', 'island', '(', 'ni', '##shi', '##jima', ')', 'of', 'mats', '##ush', '##ima', '(', 'takeshima', ')', 'in', '3', 'ok', '##i', 'county', '.', '[SEP]']\n",
      "숫자형 토큰 형태: [101, 1996, 1523, 3, 16950, 13462, 6342, 1031, 7526, 1997, 3, 2007, 7341, 1033, 1524, 5493, 2011, 24529, 13860, 8934, 10936, 17175, 2076, 1996, 7570, 16816, 2072, 2558, 1006, 24440, 1011, 18432, 1007, 3397, 1996, 2206, 6412, 1024, 2045, 2003, 2019, 2479, 2055, 2871, 15544, 2167, 1997, 1996, 2225, 2479, 1006, 9152, 6182, 19417, 1007, 1997, 22281, 20668, 9581, 1006, 3, 1007, 1999, 1017, 7929, 2072, 2221, 1012, 102]\n",
      "offsets: [(0, 0), (0, 3), (4, 5), (5, 14), (15, 17), (17, 20), (20, 22), (23, 24), (24, 35), (36, 38), (39, 48), (49, 53), (54, 58), (58, 59), (59, 60), (61, 67), (68, 70), (71, 73), (73, 76), (77, 80), (80, 82), (82, 85), (86, 92), (93, 96), (97, 99), (99, 102), (102, 103), (104, 110), (111, 112), (112, 116), (116, 117), (117, 121), (121, 122), (123, 131), (132, 135), (136, 145), (146, 157), (157, 158), (159, 164), (165, 167), (168, 170), (171, 177), (178, 183), (184, 186), (187, 189), (190, 195), (196, 198), (199, 202), (203, 207), (208, 214), (215, 216), (216, 218), (218, 221), (221, 225), (225, 226), (227, 229), (230, 234), (234, 237), (237, 240), (241, 242), (242, 251), (251, 252), (253, 255), (256, 257), (258, 260), (260, 261), (262, 268), (268, 269), (0, 0)]\n",
      "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "special_tokens_mask: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "special_tokens_mask: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "overflowing: []\n"
     ]
    }
   ],
   "source": [
    "tok_tweet = config.TOKENIZER.encode(\"The “Takeshima Zusetsu [Explanation of Takeshima with Maps]” edited by Tsuan Kitazono during the Horeki Period (1751-1763) contains the following description: There is an island about 40 ri north of the west island (Nishijima) of Matsushima (Takeshima) in 3 Oki county.\")\n",
    "print(\"분활된 토큰 형태: \" + str(tok_tweet.tokens))\n",
    "print(\"숫자형 토큰 형태: \" + str(tok_tweet.ids))\n",
    "print(\"offsets: \" + str(tok_tweet.offsets))\n",
    "print(\"attention_mask: \" + str(tok_tweet.attention_mask))\n",
    "print(\"special_tokens_mask: \" + str(tok_tweet.special_tokens_mask))\n",
    "print(\"special_tokens_mask: \" + str(tok_tweet.special_tokens_mask))\n",
    "print(\"overflowing: \" + str(tok_tweet.overflowing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text, y_class, y_keyword, position, length, error_index):\n",
    "    if (len(text[1]) > 50):\n",
    "        before = text[1]\n",
    "    else:\n",
    "        before = text[0] + \" \" + text[1]\n",
    "        \n",
    "    main = text[2]\n",
    "    \n",
    "    if (len(text[3]) > 50):\n",
    "        after = text[3]\n",
    "    else:\n",
    "        after = text[3] + \" \" + text[4]\n",
    "    # before = \"\"\n",
    "    # after = \"\"\n",
    "    before = config.TOKENIZER.encode(before)\n",
    "    main = config.TOKENIZER.encode(main)\n",
    "    after = config.TOKENIZER.encode(after)\n",
    "\n",
    "\n",
    "    # 토큰 기준으로 키워드가 어디있는지 확인\n",
    "    keyword_position_in_token = -1\n",
    "    keyword_end_in_token = -1\n",
    "    if y_class != 0:\n",
    "        keyword_position_in_string = position\n",
    "        keyword_length_in_string = length\n",
    "        for j in range(len(main.offsets)):\n",
    "            if keyword_position_in_token == -1 and main.offsets[j][0] >= keyword_position_in_string:\n",
    "                keyword_position_in_token = j\n",
    "            if main.offsets[j][1] == 0: continue\n",
    "            if main.offsets[j][1] <= (keyword_position_in_string + keyword_length_in_string):\n",
    "                keyword_end_in_token = j\n",
    "    else:        \n",
    "        keyword_position_in_token = 0\n",
    "        keyword_end_in_token = 0\n",
    "        \n",
    "    # ids = cls, classification number, sep, token, sep\n",
    "    ids = [101, 9999, 102] + main.ids[1:] + before.ids[1:] + after.ids[1:]\n",
    "    # mask = len(cls, classification number, sep, token, sep) = 1, else 0\n",
    "    mask = [1] * len(ids)\n",
    "    # token_type_ids len(token, sep) = 1, else 0\n",
    "    token_type_ids = [0,0,0] +  [1] * (len(main) - 1) + [0] * (len(before) - 1) + [0] * (len(after) - 1)\n",
    "\n",
    "    targets_start = keyword_position_in_token\n",
    "    targets_end = keyword_end_in_token\n",
    "\n",
    "    # offsets based on ids, token offsets (0,0)(0,0)(0,0)(0,a)...(0,0)\n",
    "    offsets = main.offsets\n",
    "    \n",
    "    # Pad sequence if its length < `max_len`\n",
    "    padding_length = 380 - len(ids)\n",
    "    if padding_length > 0:\n",
    "        ids = ids + ([0] * padding_length)\n",
    "        mask = mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        padding_length = 380 - len(offsets)\n",
    "        offsets = offsets + ([(0, 0)] * padding_length)\n",
    "        \n",
    "    temp = []\n",
    "    temp = [0] * len(class_list)\n",
    "    temp[y_class] = 1\n",
    "            \n",
    "    return {\n",
    "            'ids': ids,\n",
    "            'mask': mask, \n",
    "            'token_type_ids': token_type_ids,\n",
    "            'targets_start': targets_start, \n",
    "            'targets_end': targets_end, \n",
    "            'orig_text': text,\n",
    "            'orig_keyword': y_keyword,\n",
    "            'class': y_class,\n",
    "            'offsets': offsets ,\n",
    "            'targets_class': temp,\n",
    "            'error_index':[error_index] * 380\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [101, 9999, 102, 1010, 1999, 2422, 1997, 3439, 8866, 1998, 2241, 2006, 2248, 2375, 1012, 102, 2009, 2003, 10358, 2013, 1996, 3906, 1010, 2214, 7341, 1010, 1998, 2060, 4216, 2008, 2144, 3418, 2335, 1010, 2900, 2354, 2055, 2556, 1011, 2154, 3, 2011, 1996, 2171, 1997, 22281, 20668, 9581, 1998, 2641, 2009, 1037, 2112, 1997, 2887, 3700, 1012, 102, 3188, 2000, 1996, 2900, 1011, 4420, 18985, 1010, 1996, 2887, 2231, 2525, 2872, 3, 2104, 1996, 7360, 1997, 1996, 8911, 1997, 7929, 5740, 24772, 2479, 1997, 11895, 2386, 2063, 7498, 2083, 11895, 2386, 2063, 19402, 11137, 5060, 2053, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'targets_start': 0, 'targets_end': 0, 'orig_text': ['In addition, the Japanese Ministry of Foreign Affairs issued a statement dated the same day which expressed its opinions regarding Takeshima to the following effect: Takeshima, over which disputes arose between Japan and Korea after 1693 and where fishing by Japanese people was later prohibited by an order of the Bakufu, concerns the period when present-day Utsuryo Island was called Takeshima or Kantakeshima and is not present-day Takeshima.', 'It is evident from the literature, old maps, and other sources that since ancient times, Japan knew about present-day Takeshima by the name of Matsushima and considered it a part of Japanese territory.', '                                                             , in light of historical facts and based on international law.', 'Prior to the Japan-Korea annexation, the Japanese government already placed Takeshima under the jurisdiction of the administrator of Okinoshima Island of Shimane Prefecture through Shimane Prefectural Notice No.', '40 dated February 23, 1905.'], 'orig_keyword': '', 'class': 0, 'offsets': [(0, 0), (61, 62), (63, 65), (66, 71), (72, 74), (75, 85), (86, 91), (92, 95), (96, 101), (102, 104), (105, 118), (119, 122), (122, 123), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)], 'targets_class': [1, 0, 0, 0, 0, 0], 'error_index': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "i = 4\n",
    "a = preprocessing(training_data[i][0], training_data[i][1],training_data[i][2], training_data[i][3], training_data[i][4], training_data[i][5])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이토치를 위한 데이터셋 클래스 생성\n",
    "이 클래스는 파이토치에서 데이터를 로드할 때 사용되는 인터페이스입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset:\n",
    "    \"\"\"\n",
    "    Dataset which stores the tweets and returns them as processed features\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        data = preprocessing(\n",
    "            self.dataset[item][0], \n",
    "            self.dataset[item][1], \n",
    "            self.dataset[item][2], \n",
    "            self.dataset[item][3], \n",
    "            self.dataset[item][4], \n",
    "            self.dataset[item][5]\n",
    "        )\n",
    "        temp = []\n",
    "        for i in data[\"targets_class\"]:\n",
    "            temp.append(torch.tensor(i, dtype=torch.long))\n",
    "            \n",
    "        # Return the processed data where the lists are converted to `torch.tensor`s\n",
    "        return {\n",
    "            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n",
    "            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n",
    "            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n",
    "            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n",
    "            'targets_class': torch.tensor(data[\"targets_class\"], dtype=torch.long),\n",
    "            'orig_tweet': data[\"orig_text\"],\n",
    "            'orig_selected': data[\"orig_keyword\"],\n",
    "            'sentiment': data[\"class\"],\n",
    "            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long),\n",
    "            'error_index': torch.tensor(data[\"error_index\"], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = TextDataset(training_data)\n",
    "train_dataset[4]['error_index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이토치 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "class TweetModel(transformers.BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Model class that combines a pretrained bert model with a linear later\n",
    "    \"\"\"\n",
    "    def __init__(self, conf, num_labels):\n",
    "        super(TweetModel, self).__init__(conf)\n",
    "        # pretrained BERT model을 불러옵니다.\n",
    "        self.bert = transformers.BertModel.from_pretrained(config.BERT_PATH, config=conf)\n",
    "        \n",
    "        # Set 10% dropout to be applied to the BERT backbone's output\n",
    "        # dropout은 은닉층에서 일정 확률로 유닛을 사용하지 않도록(=0) 합니다.\n",
    "        # 따라서 해당 케이스에서는 사용된 유닛만을 이용해 loss를 구하고 grident를 수행합니다.\n",
    "        # 결국 오버피팅 방지 가능!! (하나의 유닛에 의존하는 현상을 제거)\n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "        \n",
    "        # 우리가 쓰는 bert-base-uncased 모델은 768의 hidden representation을 가지고 있음\n",
    "        # 그래서 새로운 레이어를 이어 붙일 때에도 768개씩 붙여야함.\n",
    "        \n",
    "        # 우리의 데이터를 추가로 학습하는 용도로 사용할 추가적인 레이어가 필요함. (hidden_layer 추가)\n",
    "        # 히든 레이어를 추가할수록 복잡한 딥러닝 네트워크를 만들 수 있지만... 데이터가 많이 필요할 듯\n",
    "        \n",
    "        # 여기에서는 단어 임베딩 결과를 활용할 수 있게 레이어 정의\n",
    "        # BERT를 수행하며 나온 hidden layer 12개중, 마지막 10번째 11번째를 사용할 것임.\n",
    "        # 12번째는 오버피팅 가능성이 높기 때문\n",
    "        # 따라서 10번째(768) 11번째 (768) 두개의 레이어를 input으로 받을 것이기 때문에 레이어의 input은 768 * 2\n",
    "        \n",
    "        # layer0만으로 결과를 내기에는 제대로 학습이 안된다고 판단이 되어 같은 크기의 레이어 layer1를 추가할 예정\n",
    "        # 따라서 768*2 -> 768*2 레이어 정의\n",
    "        self.l0 = nn.Linear(768 * 2, 768 * 2)\n",
    "        \n",
    "        # l0으로부터 768*2 결과를 전달받아 최종적으로 start, end, class를 판단하기 위한 layer1를 정의\n",
    "        self.l1 = nn.Linear(768 * 2, 2 + num_labels)\n",
    "        \n",
    "        # 사용할 activation 함수\n",
    "        self.gelu = nn.GELU()\n",
    "        \n",
    "        # 가중치 랜덤 초기화\n",
    "        torch.nn.init.normal_(self.l0.weight, std=0.02)\n",
    "        torch.nn.init.normal_(self.l1.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids, error_index):\n",
    "        # BERT backbone으로부터 hidden states를 얻어옴.\n",
    "        _, _, out = self.bert(\n",
    "            ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        ) # bert_layers x bs x SL x (768)\n",
    "\n",
    "        # Concatenate the last two hidden states\n",
    "        # This is done since experiments have shown that just getting the last layer\n",
    "        # gives out vectors that may be too taylored to the original BERT training objectives (MLM + NSP)\n",
    "        # Sample explanation: https://bert-as-service.readthedocs.io/en/latest/section/faq.html#why-not-the-last-hidden-layer-why-second-to-last\n",
    "        \n",
    "        # BERT를 수행하며 나온 hidden layer의 output에서 -2번째, -1번째만 가져옴. 그리고 한줄로 이어 붙이기\n",
    "\n",
    "        error_index = error_index.view(out[-1].shape[0],out[-1].shape[1],1)\n",
    "        out = torch.cat((out[-1], out[-2]), dim=-1) # bs x SL x (768 * 2)\n",
    "        \n",
    "        # 위에서 말했던것 처럼 10%의 노드를 제거\n",
    "        out = self.drop_out(out) # bs x SL x (768 * 2)\n",
    "        # The \"dropped out\" hidden vectors are now fed into the linear layer to output two scores\n",
    "        \n",
    "        # 해당 결과를 layer0에 통과\n",
    "        out = self.l0(out) # bs x SL x 2\n",
    "        \n",
    "        # 이 결과를 바로 사용할 것은 아니기에 gelu functaion을 거치게 만듦.\n",
    "        out = self.gelu(out)\n",
    "        \n",
    "        # layer1로 가기 전에도 똑같이 drop oup 진행\n",
    "        out = self.drop_out(out)\n",
    "        \n",
    "        # layer1로 전달\n",
    "        logits = self.l1(out)\n",
    "        \n",
    "        # 현재 layer1은 n개의 output을 내기때문에 이것을 분리\n",
    "        # (bs x SL x n) -> (bs x SL x 1), (bs x SL x 1) ...\n",
    "        outputs = list(logits.split(1, dim=-1))\n",
    "        for i in range(0,len(outputs)):\n",
    "            outputs[i] = outputs[i].squeeze(-1)\n",
    "            \n",
    "\n",
    "        start_logits = outputs[0] # (bs x SL)\n",
    "        end_logits = outputs[1] # (bs x SL)\n",
    "        class_logits = outputs[2:]\n",
    "        return start_logits, end_logits, class_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(start_logits, end_logits, class_logits, start_positions, end_positions, class_targets):\n",
    "    \"\"\"\n",
    "    Return the sum of the cross entropy losses for both the start and end logits\n",
    "    \"\"\"\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    start_loss = loss_fct(start_logits, start_positions)\n",
    "    end_loss = loss_fct(end_logits, end_positions)\n",
    "    total_loss = start_loss + end_loss\n",
    "        \n",
    "    class_targets = class_targets.t()\n",
    "\n",
    "    for i in range(0, len(class_list)):\n",
    "        total_loss += loss_fct(class_logits[i], class_targets[i]) / len(class_list)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, optimizer, device, scheduler=None):\n",
    "    \"\"\"\n",
    "    Trains the bert model on the twitter data\n",
    "    \"\"\"\n",
    "    # Set model to training mode (dropout + sampled batch norm is activated)\n",
    "    model.train()\n",
    "    losses = utils.AverageMeter()\n",
    "    jaccards = utils.AverageMeter()\n",
    "\n",
    "    # Set tqdm to add loading screen and set the length\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    \n",
    "    # Train the model on each batch\n",
    "    for bi, d in enumerate(tk0):\n",
    "\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        targets_start = d[\"targets_start\"]\n",
    "        targets_end = d[\"targets_end\"]\n",
    "        sentiment = d[\"sentiment\"]\n",
    "        orig_selected = d[\"orig_selected\"]\n",
    "        orig_tweet = d[\"orig_tweet\"][2]\n",
    "        offsets = d[\"offsets\"]\n",
    "        targets_class = d[\"targets_class\"]\n",
    "        error_index = d[\"error_index\"]\n",
    "\n",
    "        # Move ids, masks, and targets to gpu while setting as torch.long\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets_start = targets_start.to(device, dtype=torch.long)\n",
    "        targets_end = targets_end.to(device, dtype=torch.long)\n",
    "        targets_class = targets_class.to(device, dtype=torch.long)\n",
    "        error_index = error_index.to(device, dtype=torch.long)\n",
    "        # Reset gradients\n",
    "        model.zero_grad()\n",
    "        # Use ids, masks, and token types as input to the model\n",
    "        # Predict logits for each of the input tokens for each batch\n",
    "        outputs_start, outputs_end, outputs_class = model(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            error_index=error_index\n",
    "        ) # (bs x SL), (bs x SL)\n",
    "        # Calculate batch loss based on CrossEntropy\n",
    "        loss = loss_fn(outputs_start, outputs_end, outputs_class, targets_start, targets_end, targets_class)\n",
    "        # Calculate gradients based on loss\n",
    "        loss.backward()\n",
    "        # Adjust weights based on calculated gradients\n",
    "        optimizer.step()\n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Apply softmax to the start and end logits\n",
    "        # This squeezes each of the logits in a sequence to a value between 0 and 1, while ensuring that they sum to 1\n",
    "        # This is similar to the characteristics of \"probabilities\"\n",
    "        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "        outputs_class = [torch.softmax(i, dim=1).cpu().detach().numpy() for i in outputs_class]\n",
    "        \n",
    "        # Calculate the jaccard score based on the predictions for this batch\n",
    "        jaccard_scores = []\n",
    "        for px, tweet in enumerate(orig_tweet):\n",
    "            selected_tweet = orig_selected[px]\n",
    "            tweet_sentiment = sentiment[px]\n",
    "            ont_hot_class = [np.argmax(i[px, :]) for i in outputs_class]\n",
    "            class_number = ont_hot_class.index(max(ont_hot_class))\n",
    "            jaccard_score, _ = calculate_jaccard_score(\n",
    "                original_tweet=tweet, # Full text of the px'th tweet in the batch\n",
    "                target_string=selected_tweet, # Span containing the specified sentiment for the px'th tweet in the batch\n",
    "                sentiment_val=tweet_sentiment, # Sentiment of the px'th tweet in the batch\n",
    "                idx_start=np.argmax(outputs_start[px, :]), # Predicted start index for the px'th tweet in the batch\n",
    "                idx_end=np.argmax(outputs_end[px, :]), # Predicted end index for the px'th tweet in the batch\n",
    "                offsets=offsets[px], # Offsets for each of the tokens for the px'th tweet in the batch\n",
    "                class_number=class_number\n",
    "            )\n",
    "            jaccard_scores.append(jaccard_score)\n",
    "        # Update the jaccard score and loss\n",
    "        # For details, refer to `AverageMeter` in https://www.kaggle.com/abhishek/utils\n",
    "        jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
    "        losses.update(loss.item(), ids.size(0))\n",
    "        # Print the average loss and jaccard score at the end of each batch\n",
    "        tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(data_loader, model, device):\n",
    "    \"\"\"\n",
    "    Evaluation function to predict on the test set\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    # I.e., turn off dropout and set batchnorm to use overall mean and variance (from training), rather than batch level mean and variance\n",
    "    # Reference: https://github.com/pytorch/pytorch/issues/5406\n",
    "    model.eval()\n",
    "    losses = utils.AverageMeter()\n",
    "    jaccards = utils.AverageMeter()\n",
    "    \n",
    "    # Turns off gradient calculations (https://datascience.stackexchange.com/questions/32651/what-is-the-use-of-torch-no-grad-in-pytorch)\n",
    "    with torch.no_grad():\n",
    "        tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "        # Make predictions and calculate loss / jaccard score for each batch\n",
    "        for bi, d in enumerate(tk0):\n",
    "            ids = d[\"ids\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            sentiment = d[\"sentiment\"]\n",
    "            orig_selected = d[\"orig_selected\"]\n",
    "            orig_tweet = d[\"orig_tweet\"][2]\n",
    "            targets_start = d[\"targets_start\"]\n",
    "            targets_end = d[\"targets_end\"]\n",
    "            offsets = d[\"offsets\"].numpy()\n",
    "            targets_class = d[\"targets_class\"]\n",
    "            error_index = d[\"error_index\"]\n",
    "\n",
    "            # Move tensors to GPU for faster matrix calculations\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            targets_start = targets_start.to(device, dtype=torch.long)\n",
    "            targets_end = targets_end.to(device, dtype=torch.long)\n",
    "            targets_class = targets_class.to(device, dtype=torch.long)\n",
    "            error_index = error_index.to(device, dtype=torch.long)\n",
    "\n",
    "            # Predict logits for start and end indexes\n",
    "            outputs_start, outputs_end, outputs_class = model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                error_index=error_index\n",
    "            )\n",
    "            # Calculate loss for the batch\n",
    "            loss = loss_fn(outputs_start, outputs_end, outputs_class, targets_start, targets_end, targets_class)\n",
    "            # Apply softmax to the predicted logits for the start and end indexes\n",
    "            # This converts the \"logits\" to \"probability-like\" scores\n",
    "            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "            outputs_class = [torch.softmax(i, dim=1).cpu().detach().numpy() for i in outputs_class]\n",
    "            \n",
    "            # Calculate jaccard scores for each tweet in the batch\n",
    "            jaccard_scores = []\n",
    "            for px, tweet in enumerate(orig_tweet):\n",
    "                selected_tweet = orig_selected[px]\n",
    "                tweet_sentiment = sentiment[px]\n",
    "                ont_hot_class = [np.argmax(i[px, :]) for i in outputs_class]\n",
    "                class_number = ont_hot_class.index(max(ont_hot_class))\n",
    "                \n",
    "                jaccard_score, _ = calculate_jaccard_score(\n",
    "                    original_tweet=tweet,\n",
    "                    target_string=selected_tweet,\n",
    "                    sentiment_val=tweet_sentiment,\n",
    "                    idx_start=np.argmax(outputs_start[px, :]),\n",
    "                    idx_end=np.argmax(outputs_end[px, :]),\n",
    "                    offsets=offsets[px],\n",
    "                    class_number=class_number\n",
    "                )\n",
    "                jaccard_scores.append(jaccard_score)\n",
    "\n",
    "            # Update running jaccard score and loss\n",
    "            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
    "            losses.update(loss.item(), ids.size(0))\n",
    "            # Print the running average loss and jaccard score\n",
    "            tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n",
    "    \n",
    "    print(f\"Jaccard = {jaccards.avg}\")\n",
    "    return jaccards.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jaccard_score(\n",
    "    original_tweet, \n",
    "    target_string, \n",
    "    sentiment_val, \n",
    "    idx_start, \n",
    "    idx_end, \n",
    "    offsets,\n",
    "    class_number,\n",
    "    verbose=False):\n",
    "    \"\"\"\n",
    "    Calculate the jaccard score from the predicted span and the actual span for a batch of tweets\n",
    "    \"\"\"\n",
    "    # A span's end index has to be greater than or equal to the start index\n",
    "    # If this doesn't hold, the start index is set to equal the end index (the span is a single token)\n",
    "    if idx_end < idx_start:\n",
    "        idx_end = idx_start\n",
    "    \n",
    "    # Combine into a string the tokens that belong to the predicted span\n",
    "    filtered_output  = \"\"\n",
    "    for ix in range(idx_start, idx_end + 1):\n",
    "        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n",
    "        # If the token is not the last token in the tweet, and the ending offset of the current token is less\n",
    "        # than the beginning offset of the following token, add a space.\n",
    "        # Basically, add a space when the next token (word piece) corresponds to a new word\n",
    "        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n",
    "            filtered_output += \" \"\n",
    "    #print(filtered_output)\n",
    "    # Set the predicted output as the original tweet when the tweet's sentiment is \"neutral\", or the tweet only contains one word\n",
    "    if sentiment_val == 0 or len(original_tweet.split()) < 2:\n",
    "        filtered_output = original_tweet\n",
    "    # Calculate the jaccard score between the predicted span, and the actual span\n",
    "    # The IOU (intersection over union) approach is detailed in the utils module's `jaccard` function:\n",
    "    # https://www.kaggle.com/abhishek/utils\n",
    "    \n",
    "    jac = utils.jaccard(target_string.strip(), filtered_output.strip())\n",
    "    if class_number != sentiment_val:\n",
    "        jac *= 0.5\n",
    "    else:  \n",
    "        if sentiment_val == 0:\n",
    "            jac = 1\n",
    "        \n",
    "    return jac, filtered_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is Starting for fold=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc7d667fbf74468b2d1bbe32f4ef0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=337.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7731f545cff4ace9a0dc982ccc0606a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=43.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.8541147023806333\n",
      "Jaccard Score = 0.8541147023806333\n",
      "Validation score improved (-inf --> 0.8541147023806333). Saving model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03fc48f54b1498e9ce8f2cbf9e23b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=337.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82652c589c894684ba10757a2cef59d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=43.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.8841197023667987\n",
      "Jaccard Score = 0.8841197023667987\n",
      "Validation score improved (0.8541147023806333 --> 0.8841197023667987). Saving model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a41dec5839a46c4bae51eea02829181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=337.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cfebd2aa77b47089c77d29f11f0021f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=43.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.9017647626244034\n",
      "Jaccard Score = 0.9017647626244034\n",
      "Validation score improved (0.8841197023667987 --> 0.9017647626244034). Saving model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b5434b95034bbbb2610c1419c6d2dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=337.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e1b427f571f46138d7ee23177cdcd21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=43.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.9041785865643708\n",
      "Jaccard Score = 0.9041785865643708\n",
      "Validation score improved (0.9017647626244034 --> 0.9041785865643708). Saving model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe1b98d82a8c461a87cd86a9a80f0e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=337.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021f443acd1d433e82bb55475d672cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=43.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.8988495342528611\n",
      "Jaccard Score = 0.8988495342528611\n",
      "EarlyStopping counter: 1 out of 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf686f7e61754c599528a715292a1cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=337.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1751d71944594223b0afd59c0d6850ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=43.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.8988495342528606\n",
      "Jaccard Score = 0.8988495342528606\n",
      "EarlyStopping counter: 2 out of 2\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(training_data)\n",
    "\n",
    "# Instantiate DataLoader with `train_dataset`\n",
    "# This is a generator that yields the dataset in batches\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.TRAIN_BATCH_SIZE,\n",
    "    num_workers=0,\n",
    "    shuffle = True\n",
    ")\n",
    "#    shuffle = True\n",
    "valid_dataset = TextDataset(test_data)\n",
    "\n",
    "# Instantiate DataLoader with `train_dataset`\n",
    "# This is a generator that yields the dataset in batches\n",
    "valid_data_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=config.TRAIN_BATCH_SIZE,\n",
    "    num_workers=0,\n",
    "    shuffle = True\n",
    ")\n",
    "# Set device as `cuda` (GPU)\n",
    "device = torch.device(\"cuda\")\n",
    "# Load pretrained BERT (bert-base-uncased)\n",
    "model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\n",
    "# Output hidden states\n",
    "# This is important to set since we want to concatenate the hidden states from the last 2 BERT layers\n",
    "model_config.output_hidden_states = True\n",
    "# Instantiate our model with `model_config`\n",
    "model = TweetModel(conf=model_config, num_labels=len(class_list))\n",
    "# Move the model to the GPU\n",
    "model.to(device)\n",
    "\n",
    "# Calculate the number of training steps\n",
    "num_train_steps = int(len(training_data) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
    "# Get the list of named parameters\n",
    "param_optimizer = list(model.named_parameters())\n",
    "# Specify parameters where weight decay shouldn't be applied\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "# Define two sets of parameters: those with weight decay, and those without\n",
    "optimizer_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "# Instantiate AdamW optimizer with our two sets of parameters, and a learning rate of 3e-5\n",
    "optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "# Create a scheduler to set the learning rate at each training step\n",
    "# \"Create a schedule with a learning rate that decreases linearly after linearly increasing during a warmup period.\" (https://pytorch.org/docs/stable/optim.html)\n",
    "# Since num_warmup_steps = 0, the learning rate starts at 3e-5, and then linearly decreases at each training step\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=num_train_steps\n",
    ")\n",
    "\n",
    "# Apply early stopping with patience of 2\n",
    "# This means to stop training new epochs when 2 rounds have passed without any improvement\n",
    "es = utils.EarlyStopping(patience=2, mode=\"max\")\n",
    "fold = 0\n",
    "print(f\"Training is Starting for fold={fold}\")\n",
    "\n",
    "# I'm training only for 3 epochs even though I specified 5!!!\n",
    "for epoch in range(50):\n",
    "    train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n",
    "    jaccard = eval_fn(valid_data_loader, model, device)\n",
    "    print(f\"Jaccard Score = {jaccard}\")\n",
    "    es(jaccard, model, model_path=f\"model_{fold}.bin\")\n",
    "    if es.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "        \n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TweetModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (drop_out): Dropout(p=0.1, inplace=False)\n",
       "  (l0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "  (l1): Linear(in_features=1536, out_features=8, bias=True)\n",
       "  (gelu): GELU()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\n",
    "model_config.output_hidden_states = True\n",
    "\n",
    "model1 = TweetModel(conf=model_config, num_labels=len(class_list))\n",
    "model1.to(device)\n",
    "model1.load_state_dict(torch.load(\"model_0.bin\"))\n",
    "model1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a98887ea3840d2b28d50af68538cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=85.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final_output = []\n",
    "\n",
    "# Instantiate TweetDataset with the test data\n",
    "test_dataset = TextDataset(test_data)\n",
    "\n",
    "# Instantiate DataLoader with `test_dataset`\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=config.VALID_BATCH_SIZE,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "# Turn of gradient calculations\n",
    "with torch.no_grad():\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    # Predict the span containing the sentiment for each batch\n",
    "    for bi, d in enumerate(tk0):\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        sentiment = d[\"sentiment\"]\n",
    "        orig_selected = d[\"orig_selected\"]\n",
    "        orig_tweet = d[\"orig_tweet\"][2]\n",
    "        targets_start = d[\"targets_start\"]\n",
    "        targets_end = d[\"targets_end\"]\n",
    "        offsets = d[\"offsets\"].numpy()\n",
    "        error_index = d[\"error_index\"]\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets_start = targets_start.to(device, dtype=torch.long)\n",
    "        targets_end = targets_end.to(device, dtype=torch.long)\n",
    "        error_index = error_index.to(device, dtype=torch.long)\n",
    "\n",
    "        # Predict start and end logits for each of the five models\n",
    "        outputs_start, outputs_end, outputs_class = model1(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            error_index=error_index\n",
    "        )\n",
    "        \n",
    "        # Apply softmax to the predicted start and end logits\n",
    "        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "        outputs_class = [torch.softmax(i, dim=1).cpu().detach().numpy() for i in outputs_class]\n",
    "        # Convert the start and end scores to actual predicted spans (in string form)\n",
    "        for px, tweet in enumerate(orig_tweet):\n",
    "            selected_tweet = orig_selected[px]\n",
    "            tweet_sentiment = sentiment[px]\n",
    "            ont_hot_class = [np.argmax(i[px, :]) for i in outputs_class]\n",
    "            class_number = ont_hot_class.index(max(ont_hot_class))\n",
    "            if tweet_sentiment == class_number:\n",
    "                if class_number == 0:\n",
    "                    TN += 1\n",
    "                else:\n",
    "                    TP += 1\n",
    "            else:\n",
    "                if class_number == 0:\n",
    "                    FN += 1 # 원래는 양성인데 음성으로 예측\n",
    "                else:\n",
    "                    FP += 1\n",
    "                    \n",
    "            _, output_sentence = calculate_jaccard_score(\n",
    "                original_tweet=tweet,\n",
    "                target_string=selected_tweet,\n",
    "                sentiment_val=tweet_sentiment,\n",
    "                idx_start=np.argmax(outputs_start[px, :]),\n",
    "                idx_end=np.argmax(outputs_end[px, :]),\n",
    "                offsets=offsets[px],\n",
    "                class_number = class_number\n",
    "            )\n",
    "            final_output.append([np.argmax(outputs_start[px, :]), np.argmax(outputs_end[px, :]), output_sentence, class_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도:  0.9673590504451038\n",
      "Precision:  0.9292929292929293  (특정 오류가 있다고 예측한 것 중 실제 그럴 확률)\n",
      "Recall:  0.9583333333333334  (실제 오류중에 제대로 검출할 확률)\n",
      "문서 개수:  337\n",
      "TP:  92\n",
      "TN:  234\n",
      "FN:  4\n",
      "FP:  7\n"
     ]
    }
   ],
   "source": [
    "print(\"정확도: \", (TP + TN)  / (TN+TP+FN+FP))\n",
    "print(\"Precision: \", (TP)  / (TP+FP), \" (특정 오류가 있다고 예측한 것 중 실제 그럴 확률)\")\n",
    "print(\"Recall: \", (TP)  / (TP+FN), \" (실제 오류중에 제대로 검출할 확률)\")\n",
    "\n",
    "print(\"문서 개수: \", (TN+TP+FN+FP))\n",
    "\n",
    "print(\"TP: \", (TP))\n",
    "print(\"TN: \", (TN))\n",
    "print(\"FN: \", (FN))\n",
    "print(\"FP: \", (FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"tw\": \"Sea of Japan\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"5 MINUTE READ BY ALEXANDRA GENOVA PHOTOGRAPHS BY TIM FRANCO In the middle of the Sea of Japan, almost equidistant between Japan and Korea, jut two seemingly inconsequential craggy islets.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Dokdo \\u2013 Takeshima Island A Brief Introduction to Korea's Dokdo [Takeshima] Island map showing distances to Dokdo TakeshimaDokdo Island (also called Liancourt Rocks by some nations and Takeshima by Japan) is 215 kms from mainland Korea and 250 kms from Japan proper.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"[Takeshima] Island map showing distances to Dokdo TakeshimaDokdo Island (also called Liancourt Rocks by some nations and Takeshima by Japan)\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Distances The Liancourt Rocks are located at about 37\\u00b014\\u2032N 131\\u00b052\\u2032E.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Liancourt Rocks\",\n",
      "    \"predicted_keyword\": \"Liancourt Rocks \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Additionally, the \\u201cChosei Takeshima Ki [The Account of Longevity Takeshima]\\u201d authored by Takamasa Yada in 1801 states: From Oki-Dogo, Matsushima (Takeshima) is located off the west-southwestern coast.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"While the precise cause of the outbreaks is not yet certain, warmer conditions are linked to increases in the size of jellyfish populations.12,13,14,15 From 1976 to 2000, temperatures in the Yellow Sea\\u2014 likely spawning ground for N. nomurai jellyfish\\u2014rose by 3.1\\u00b0 F (1.7\\u00b0 C).16 In the Sea of Japan itself, winter sea surface temperatures have increased by 2.9\\u20134.3\\u00b0 F (1.6\\u20132.4\\u00b0 C) over the last century.17,18 Warming seas may also be exacerbating jellyfish outbreaks already linked to nutrient loading, coastal development, and overfishing.5,32 For instance, overfishing species that prey on jellyfish removes a critical check on the jellyfish population, creating an opportunity for these strong survivors to multiply.32\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 0\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Note: The                                                                                                                            prior to Japan's effective control over Takeshima and reaffirmation of its territorial sovereignty in 1905.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"International Symposium in Korea on the Takeshima Dispute\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Nevertheless, in 1953, an incident took place involving the shooting of a patrol vessel of the Maritime Safety Agency by ROK authorities in Takeshima.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"World Cup>Past Tournaments>2002 Japan-Korea>Overview\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"2002 Japan-Korea\",\n",
      "    \"predicted_keyword\": \"\",\n",
      "    \"class\": 1,\n",
      "    \"predicted_class\": 0\n",
      "}\n",
      "{\n",
      "    \"tw\": \"2 central government (Bakufu) for passage to Utsuryo Island (then Takeshima) (exclusive rights to develop the island).\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The Meaning of the Territorial Incorporation of Takeshima (1905)\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"4 \\u2013 \\\"10 Issues of Takeshima, MOFA (Article 2)\\\" (PDF).\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The petitions included \\u201cPetition to Develop Matsushima\\u201d by Sadaaki Kodama; \\u201cPetition to Develop Matsushima and Takeshima\\u201d by Heigaku Muto in 1876; and the \\u201cPetition of passage to Takeshima\\u201d by Takayoshi Toda, a warrior in Shimane Prefecture in 1877.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The sewage water treatment system established on the islets has malfunctioned and sewage water produced by inhabitants of the Liancourt Rocks such as South Korean Coast Guard and lighthouse staff is being dumped directly into the ocean.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Liancourt Rocks\",\n",
      "    \"predicted_keyword\": \"Liancourt Rocks \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"U.S. recognized Takeshima as part of Japan's territory in 1950: government report NATIONALSEP 10, 2019 U.S. recognized Takeshima as part of Japan's territory in 1950: government report\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima as part of Japan's territory\",\n",
      "    \"predicted_keyword\": \"Takeshima as part of Japan's territory \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"On January 29, 1946, a memorandum of the General Headquarters of SCAP, titled \\u201cGovernmental and Administrative Separation of Certain Outlying Areas from Japan (SCAPIN-677)\\u201d, directed Japan to cease exercising governmental and administrative authority over Utsuryo Island, Quelpart Island, as well as Takeshima.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"(copy) \\u25b2Japan Coast Guard patrol vessel fired at near Takeshima by the Republic of Korea in July 1953.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima by the Republic of Korea \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The vessel Takeshimamaru is said to always have made a port call at this island during its passage to Takeshima (Utsuryo Island).\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshimamaru\",\n",
      "    \"predicted_keyword\": \"Takeshimamaru is said to always have made a port call at this island during its passage to Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Thus Japan\\u2019s sovereignty over Takeshima, which by then had already been established, became clearer to other countries in terms of modern international law as well.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Japan\\u2019s sovereignty over Takeshima, which by then had already been established\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Takeshima is influenced by strong onshore winds.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Additionally, the \\u201cChosei           Ki [The Account of Longevity Takeshima]\\u201d authored by Takamasa Yada in 1801 states: From Oki-Dogo, Matsushima (Takeshima) is located off the west-southwestern coast.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Case concerning a state compensation claim (October 18, 1960, Tokyo District Court) Plaintiff: Toru Higo Defendant: The Government of Japan <Facts> The Plaintiff transferred his registered residence to Takeshima, Goka Village, Ochi District, Shimane Prefecture on September 25, 1959 and made repeated attempts to go to the said location.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Republic of Korea's assertion (3) The Republic of Korea claims that Takeshima was separated from Japanese territory by Supreme Commander for the Allied Powers Instruction Note (SCAPIN) 677 and SCAPIN 1033.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"By the beginning of 19th Century, the name - Sea of Japan (Japan Sea) became established internationally as the name indicating this sea area.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea) became established internationally as the name\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"On November 1, 1945, following the end of World War II, administration of Takeshima was transferred from the navy to the Ministry of Finance pursuant to Article 2 of the Order for Enforcement of the National Property Act.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Further, these resolutions presume that the geographical feature concerned is under the sovereignty of two or more countries, such as in the case of a bay or strait, and does not apply to the high seas such as with the Sea of Japan.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"seas such as with the Sea of Japan\",\n",
      "    \"predicted_keyword\": \"\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 0\n",
      "}\n",
      "{\n",
      "    \"tw\": \"With regard to the attribution of Takeshima, on August 9, 1965, Foreign Minister Lee Tong-won of the ROK stated in a Special Committee of Korean National Assembly as follows: \\u201cIt is a fact that notes are exchanged for the settlement of disputes.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Thus, the islands now known as Takeshima means the islands which: (1) were called \\u201cMatsushima\\u201d in Japan from the 17th century through the Meiji Period; (2) were called \\u201cHornnet,\\u201d \\u201cLiancourt\\u201d and \\u201cLyanko\\u201d during the 19th century, when there was confusion over the name of the islands; (3) are called Takeshima in Japan since its incorporation into Japanese territory in 1905; and (4) are presently called \\u201cDokdo\\u201d in the Republic of Korea (ROK).\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The Treaty of Peace with Japan, which established the international order post-World War II, lists territories that Japan must renounce, while intentionally excluding Takeshima, and affirms that Takeshima is Japanese territory.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima, and affirms that Takeshima is Japanese territory\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The Japanese government affirmed that 5 Utsuryo Island was Korean territory and issued the following notice, prohibiting Japanese people from going to Utsuryo Island: Japan and Korea have entered into an agreement which states that Japanese people would not sail to nor land on, without reason, the island that Japan calls Matsushima (also known as Takeshima) and Korea calls Ulleungdo, located at 37\\u00b030\\u2019 north latitude and 130\\u00b049\\u2019 east longitude.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"They are no larger than Grand Central Terminal and yet the Liancourt Rocks\\u2014or Dokdo Islands or Takeshima Islands depending on who is asking\\u2014are at the center of a diplomatic dispute between the two countries that goes back more than 300 years.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Liancourt Rocks\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"However, in January 1952, immediately prior to the coming into force of the San Francisco Peace Treaty, the ROK unilaterally established what is known as the \\u201cSyngman Rhee Line\\u201d and incorporated Takeshima into the ROK side of the line.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Through a prefectural notice dated February 22, 1905, the governor announced that Takeshima came under the jurisdiction of Shimane Prefecture, and issued an order to this effect to the Okinoshima Branch Office.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima came under the jurisdiction of Shimane Prefecture\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Countries such as the United Kingdom and the United States use the name - Sea of Japan (Japan Sea) for making their nautical chart.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"use the name - Sea of Japan (Japan Sea) for making their nautical chart\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The United States recognized the South Korean-controlled Takeshima Islands in the Sea of Japan as Japanese territory as of 1950, a research report released by the Japanese government showed Tuesday.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"In addition, the Japanese Ministry of Foreign Affairs issued a statement dated the same day which expressed its opinions regarding           to the following effect: Takeshima, over which disputes arose between Japan and Korea after 1693 and where fishing by Japanese people was later prohibited by an order of the Bakufu, concerns the period when present-day Utsuryo Island was called Takeshima or Kantakeshima and is not present-day Takeshima.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Proposal of referral to the International Court of Justice From 1952, Japan and the ROK repeatedly exchanged written responses over their dispute over the title of Takeshima.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Domestic court rulings The following are two examples of cases in which the Takeshima dispute was raised before a Japanese court.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"A government notification giving           Island's name and jurisdiction A government notification giving Takeshima Island's name and jurisdiction (Shimane Prefecture Notice No.40, February 22,1905 Provided by the Shimane Prefecture Public Records Center)\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Prior to the Japan-Korea annexation, the Japanese government already placed Takeshima under the jurisdiction of the administrator of Okinoshima Island of Shimane Prefecture through Shimane Prefectural Notice No.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima under the jurisdiction of the administrator of Okinoshima Island of Shimane Prefecture\",\n",
      "    \"predicted_keyword\": \"placed Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"South Korea stamps depicting the Liancourt Rocks from 1954 Sovereignty over the islands has been an ongoing point of contention in Japan\\u2013South Korea relations.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Liancourt Rocks\",\n",
      "    \"predicted_keyword\": \"Liancourt Rocks \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"In going to Matsushima to hunt, one stops there as it is on the way to Takeshima.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The Government of Japan hereby once again lodges a strong protest against the ROK Government for continuing to illegally occupy Takeshima, a Japanese territory, notwithstanding the repeated protests of the Japanese Government.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"ROK Government for continuing to illegally occupy Takeshima, a Japanese territory\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"It is evident from the literature, old maps, and other sources that since ancient times, Japan knew about present-day Takeshima by the name of Matsushima and considered it a part of Japanese territory.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"On April 25 of the same year, the Japanese government objected as follows: \\u201cSCAP\\u2019s memorandum dated January 29, 1946 no more than ordered the Japanese government to cease exercising or attempting to exercise governmental or administrative authority over Takeshima, and is unrelated to its attribution.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Therefore, it is no coincidence that the term \\\"Sea of Japan\\\" appeared at this time.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Inflow of water takes place primarily through the eastern and western channels of the Korea Strait; the inflow of water into the Sea of Japan through the narrow and shallow Tatar Strait is negligible, while through the Tsugaru and La Perouse straits the water flows out of the Sea of Japan.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Illegal Occupation of Takeshima by the ROK In January 1952, the then President of the Republic of Korea, Syngman Rhee, unilaterally drew the so-called \\u201cSyngman Rhee Line,\\u201d incorporating Takeshima into the ROK side of the line.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Illegal Occupation of Takeshima by the ROK\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"A group of South Korean lawmakers on Saturday visited the disputed Takeshima islets in the             , adding fuel to ongoing spats over trade and historical issues between Tokyo and Seoul.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Takeshima islets\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Although Tokugawa Iemitsu, Shogun of Japan, issued a sakoku (closed-door policy) directive and banned foreign trade in 1639, the Bakufu continued to issue permissions for passage to Utsuryo Island and Takeshima.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Japan takes the position that as the issue of the attribution of Takeshima was not settled at JapanROK meetings and the Exchange of Notes does not stipulate the exclusion of the Takeshima dispute from its coverage, the Takeshima dispute, therefore, should be settled in accordance with the said Exchange of Notes.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Additionally, the \\u201cChosei           Ki [The Account of Longevity          ]\\u201d authored by Takamasa Yada in 1801 states: From Oki-Dogo, Matsushima (Takeshima) is located off the west-southwestern coast.\",\n",
      "    \"error_index\": 2,\n",
      "    \"keyword\": \"Takamasa\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Japan lodges complaint after South Korean lawmakers visit disputed Takeshima islets in              NATIONALAUG 31, 2019 Japan lodges complaint after South Korean lawmakers visit disputed Takeshima islets in Sea of Japan\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The Japanese Government's survey of historical maps confirms that the name Sea of Japan was already prevalent at the early 19th century.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"confirms that the name Sea of Japan was already prevalent at the early 19th century\",\n",
      "    \"predicted_keyword\": \"The Japanese Government's survey of historical maps confirms that the name Sea of Japan was already prevalent at the early 19th century\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"It is clear that the parties of the two countries took into account this Takeshima dispute in drafting the Exchange of Notes.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Foreign Minister Shiina gave the following response at a meeting of the House of Councillors\\u2019 Special Committee on the Japan-ROK Treaty and Other Matters on November 26, 1965: (Translator\\u2019s note: provisional translation) Based on an objective examination, it can be said that Takeshima is the biggest dispute between the two countries\\u2026As you are aware, it is not written anywhere that the Takeshima dispute is excluded with regard to the settlement of disputes.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Japan has argued \\u201cJapan Sea\\u201d is the only internationally established name and there is no need to change it.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"\\u201cJapan Sea\\u201d is the only internationally established name\",\n",
      "    \"predicted_keyword\": \"Japan Sea\\u201d is the only internationally established name \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"During these years, no country questioned Takeshima\\u2019s attribution to Japan\\u2026 Furthermore, the peace treaty does not contain any provisions indicating that territories which were Japanese territory prior to the Japan-Korea annexation would be ceded to Korea.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"no country questioned Takeshima\\u2019s attribution to Japan\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"In the process of drafting the San Francisco Peace Treaty (signed on September 8, 1951, effective as of April 28, 1952), which included the ultimate disposition of Japanese territory after World War II, the Republic of Korea requested that the United States add Takeshima to the territories to be renounced by Japan.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Japan will continue to seek the settlement of the dispute over territorial sovereignty over Takeshima on the basis of international law in a calm and peaceful manner.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Survey on the historic transition of the name - Sea of Japan (Japan Sea) in maps made in countries other than Japan and the ROK revealed however that the name - \\\"East Sea\\\" was much less prevalent than other names, and the name - Sea of Japan had already been internationally recognized and firmly established by the early 19th century during which Japan had been in a state of national seclusion.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan had already been internationally recognized and firmly established by the early 19th\",\n",
      "    \"predicted_keyword\": \"name - Sea of Japan had already been internationally recognized and firmly established by the early 19th century \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"In the early 17th century, Japanese merchants were given permission for passage to Utsuryo Island by the Japanese government, and they used Takeshima as a navigational port for ships on their way to Utsuryo Island as well as a ground to catch sea lions and other marine resources.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The above description reveals that at the time, the              , on its way to Utsuryo Island, commonly made a port call at           which was along the route, and that           belongs to Japanese territory although it is \\u201clocated at the far end of Japan\\u2019s western sea.\\u201d With regard to old maps, \\u201cTakeshima no Ezu [Diagrams of Takeshima]\\u201d, made around 1696 by Ihei Kotani, a government official in Tottori domain, provides an accurate illustration of Takeshima\\u2019s Higashijima and Nishijima Islands and associated reefs.\",\n",
      "    \"error_index\": 3,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Japan calls South Korea's           military exercises 'unacceptable' as ties continue to sour NATIONAL / POLITICSAUG 25, 2019 Japan calls South Korea's Takeshima military exercises 'unacceptable' as ties continue to sour\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"It is located in the Sea of Japan (37\\u00b09\\u201930\\u2019\\u2019 north latitude and 131\\u00b055\\u2019 east longitude) approximately 157 km northwest of Okinoshima Island of Shimane Prefecture (approximately 70 km from the Shimane Peninsula in Honshu Island), and approximately 213 km linearly from mainland Japan.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The Plaintiff claimed that the government of Japan does not exercise the right of taxation in Takeshima where the administration of the government does not extend in reality, and by extension, does not exercise the right to levy and collect the mine-lot tax for the islands.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The Government of Japan has repeatedly lodged strong protests over the illegal occupation of Takeshima by the authorities of the ROK Government.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"illegal occupation of Takeshima by the authorities of the ROK Government\",\n",
      "    \"predicted_keyword\": \"illegal occupation of Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"February 22, 1905 Bukichi Matsunaga Governor of Shimane Prefecture After                                                   , studies and surveys of Takeshima were conducted pursuant to the order of the governor of Shimane Prefecture.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"From then on, in Japan, islands which were long known as \\u201cMatsushima\\u201d and \\u201c         \\u201d came to be called \\u201cTakeshima\\u201d and \\u201cMatsushima,\\u201d respectively.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The middle water\\u2019s origin is in the intermediate water layers of the Kuroshio off the coast of Kyushu that enter the Sea of Japan via the Tsushima Current during the winter and spring.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"The middle water\\u2019s origin is in the intermediate water layers of the Kuroshio off the coast of Kyushu that enter the Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The world map is the extant oldest map describing a sea area between Eurasia and the Japanese Islands using the term \\\"Sea of Japan.\\\"\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"A group of South Korean lawmakers on Saturday visited the disputed Takeshima islets in the Sea of Japan, adding fuel to ongoing spats over trade and historical issues between Tokyo and Seoul.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The coral\\u2013eating crown\\u2013of\\u2013thorns starfish (Acanthaster planci) has already expanded its range into the Sea of Japan,24,25 causing declines in reef fish25 and affecting tourism throughout the region.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Kunyu Wanguo Quantu was also introduced to Japan early on, but the term \\\"Sea of Japan\\\" was not established right away.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The Cabinet added Takeshima to the State Land Register, established a license system for sea lion hunting, and charged a fee for use of the state land.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"The Cabinet added Takeshima to the State Land Register\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"As the provision of Article 2, Paragraph (a) of the treaty did not include Takeshima on the list of territories Japan renounced, Takeshima was placed back under the jurisdiction of the Okinoshima Branch Office of Shimane Prefecture with the entry into force of the treaty.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Hydrographic Authorities of the U.K. (since 1863), the U.S.A (since 1854), Russia and France (since each country\\u2019s Hydrographic Department began the publication of a nautical chart of the sea area around the                           have solely used the Sea of Japan (Japan Sea) in their nautical charts related to this sea area since their first edition.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea)\",\n",
      "    \"predicted_keyword\": \"graphic Authorities of the U.K. (since 1863), the U.S.A (since 1854), Russia and France (since each country\\u2019s Hydrographic Department began the publication of a nautical chart of the sea area around the have solely used the Sea of Japan (Japan Sea) \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"At that time, terms named after more specific place names, such as the Sea of Japan, or the Sea of Korea, were not yet used.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 0\n",
      "}\n",
      "{\n",
      "    \"tw\": \"With regard to Takeshima, the Japanese government asserted: In the Proclamation, the Republic of Korea appears to assert territorial rights over the islets in the              known as Takeshima.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Note: The Republic of Korea has never demonstrated any clear basis for its claims that it had taken effective control over Takeshima prior to Japan's effective control over Takeshima and reaffirmation of its territorial sovereignty in 1905.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Republic of Korea has never demonstrated any clear basis for its claims that it had taken effective control over Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"On May 28, 1953, a fishery experimental vessel of Shimane Prefecture \\u201cShimanemaru\\u201d discovered, 9 while conducting a development study of the Tsushima Warm Current, that approximately 30 Korean fishermen had landed on Takeshima and engaged in the harvesting of abalone, wakame seaweed, and other products.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The Seas of Japan and Okhotsk.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Seas of Japan\",\n",
      "    \"predicted_keyword\": \"\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"With regard to 3. to 5., (1) It is the basic policy of the government to settle the dispute between Japan and the ROK over the territorial title of Takeshima by peaceful means.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The above description reveals that at the time, the              , on its way to Utsuryo Island, commonly made a port call at Takeshima which was along the route, and that Takeshima belongs to Japanese territory although it is \\u201clocated at the far end of Japan\\u2019s western sea.\\u201d With regard to old maps, \\u201cTakeshima no Ezu [Diagrams of Takeshima]\\u201d, made around 1696 by Ihei Kotani, a government official in Tottori domain, provides an accurate illustration of Takeshima\\u2019s Higashijima and Nishijima Islands and associated reefs.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"In order to justify its claim, the ROK needs to present proof that Korea had effective control over Takeshima prior to Japan's effective control over Takeshima and reaffirmation of its territorial sovereignty in 1905.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The Treaty of Peace with Japan, which established the international order post-World War II, lists territories that Japan must renounce, while intentionally excluding          , and affirms that Takeshima is Japanese territory.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"affirms that Takeshima is Japanese territory\",\n",
      "    \"predicted_keyword\": \"Takeshima is Japanese territory\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Until the end of the 18th Century, the                          was an unknown sea area, and the shape of the Sea of Japan (Japan Sea) seen in European maps of that area was far from the shape we know at present.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea)\",\n",
      "    \"predicted_keyword\": \"shape of the Sea of Japan (Japan Sea) \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The Sea of Japan is home to one of the world's most productive fisheries, accounting for nearly 5 percent of the global fish catch in 2009.2 Japan maintains one of the world's largest fishing fleets, and annual per capita fish and seafood consumption in the country is 134 pounds (60.8 kilograms)\\u2014more than three times the world average.3\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"The Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Foreign Minister Shiina gave the following response at a meeting of the House of Councillors\\u2019 Special Committee on the Japan-ROK Treaty and Other Matters on November 26, 1965: (Translator\\u2019s note: provisional translation) Based on an objective examination, it can be said that           is the biggest dispute between the two countries\\u2026As you are aware, it is not written anywhere that the Takeshima dispute is excluded with regard to the settlement of disputes.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"1953\\tPublication of the third edition of the \\\"Limits of Oceans and Seas\\\"\\tThe name - Sea of Japan (Japan Sea) was used solely in the guideline.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea) was used solely in the guideline\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) was used solely in the guideline\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"This view of the traditional boat houses right on the beach in Tsuma can be thought of as a product of the small tidal range of the Sea of Japan.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Japan established sovereignty over Takeshima by the mid 17th century.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Japan established sovereignty over Takeshima\",\n",
      "    \"predicted_keyword\": \"Japan established sovereignty over Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"\\u2193 Japan's refutation (1) The Korean \\\"Usan(do)\\\" is either another name for Ulleungdo or a small island adjacent to Ulleungdo (Jukdo) and is not present-day Takeshima.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Case concerning a claim for compensatory damages (November 9, 1961, Tokyo District Court) Plaintiff: Tomizo Tsuji Defendant: The Government of Japan, Shimane Prefecture <Facts> In 1954, the director of the Hiroshima Regional Bureau of the Ministry of International Trade and Industry granted the Plaintiff a license to mine phosphate in Takeshima.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"                                 Other names:                 , Liancourt Islands, Takeshima, Dokdo, Tok Islets Location-of-Liancourt-rocks-en.png Location of the Liancourt Rocks in the              between Japan and South Korea Geography Location of Liancourt Rocks Location\\t             Coordinates\\t37\\u00b014\\u203230\\u2033N 131\\u00b052\\u20320\\u2033E Total islands\\t90 (37 permanent land) Major islands\\tEast Islet, West Islet Area\\t18.745 hectares (46.32 acres) East Islet: 7.33 hectares (18.1 acres) West Islet: 8.864 hectares (21.90 acres) Highest point West Islet 169 metres (554 ft) Administered by South Korea County\\tUlleung County, North Gyeongsang Claimed by Japan Town\\tOkinoshima, Shimane South Korea \\u00b7 North Korea County\\tUlleung(Ull\\u016dng) County, Gyeongsangbuk-do(North Ky\\u014fngsang) Demographics Population\\t50[1] The Liancourt Rocks[a] are a group of small islets in the             .\",\n",
      "    \"error_index\": 5,\n",
      "    \"keyword\": \"Liancourt Islands\",\n",
      "    \"predicted_keyword\": \", Liancourt Islands, Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "for i in range(0, len(test_dataset)):\n",
    "    if test_dataset[i]['sentiment'] !=0 and test_dataset[i]['sentiment'] != 0:\n",
    "        temp = {\n",
    "            \"tw\": test_dataset[i]['orig_tweet'][2],\n",
    "            \"error_index\": int(test_dataset[i]['error_index'][0]),\n",
    "            \"keyword\": test_dataset[i]['orig_selected'],\n",
    "            \"predicted_keyword\": (final_output[i][2] if final_output[i][3] != 0 else \"\"),\n",
    "            \"class\": test_dataset[i]['sentiment'],\n",
    "            \"predicted_class\": final_output[i][3]\n",
    "        }\n",
    "        print(json.dumps(temp, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
