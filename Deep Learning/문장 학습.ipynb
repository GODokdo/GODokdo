{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 승화 문서에서의 표기 오류 검출\n",
    "\n",
    "BERT(Bidirectional Encoder Representations from Transformers)는 구글이 개발한 사전훈련된(pre-training) 모델입니다. 이 모델은 위키피디아같은 텍스트 코퍼스(말뭉치)를 사용하여 미리 학습되었다는 특징이 있습니다. 그리고 BERT의 특성으로 단어를 학습할 때 문맥을 함께 고려하기때문에 언어의 패턴을 이해한 모델이 만들어집니다.\n",
    "\n",
    "이를 기반으로 새로운 문제에 적용하는 전이학습(transfer learning)을 수행할 수 있습니다. 미리 학습된 모델을 사용하기 때문에 적은 데이터로도 빠르게 학습이 가능하다는 이점이 있습니다.\n",
    "\n",
    "따라서 해당 모델을 기반으로 문서에서의 잘못된 표기 오류를 검출하는 알고리즘을 개발하였습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목표\n",
    "\n",
    "웹사이트나 텍스트 문서는 긴 여러개의 문장으로 이루어져있습니다. 각 문장이 잘못되었는지를 검사하고 잘못된 경우 잘못된 표현이 어디에 있는지 정확한 위치를 예측하여 알려주는 것이 저희 모델의 최종 목표입니다.\n",
    "\n",
    "따라서 저희는 BERT 모델에 linear regression를 적용한 네트워크 모델을 사용할 예정입니다. Output의 [CLS] 토큰을 통해 문장의 표기 오류를 분류하고 linear regression을 통해 그 위치를 예측할 것입니다.\n",
    "\n",
    "또한 표기 오류가 있지만 문제가 되지 않는 경우가 있습니다.\n",
    "\n",
    "<pre>\n",
    "한국 옆에는 작은 섬이 있는데 이것은 다케시마라고 불리기도 한다. \n",
    "그러나 이것은 잘못된 것으로 올바른 표기는 독도이다.\n",
    "</pre>\n",
    "\n",
    "문장 단위로 표기 오류를 검출한다면 인공지능은 표기 오류 결과로 \"다케시마\"를 지목할 것입니다.\n",
    "그러나 전체적인 문맥을 보면 해당 문장은 잘못된 사례를 이야기 해줄 뿐, 오류가 있는 문장이라고 말을 할수는 없습니다.\n",
    "\n",
    "따라서 BERT 모델에 주변 문장을 함께 학습시키는 모델을 구상하였습니다.\n",
    "**학습 예시**\n",
    "<pre>\n",
    "[index-2] None\n",
    "[index-1] None\n",
    "[판단할 문장] 한국 옆에는 작은 섬이 있는데 이것은 다케시마라고 불리기도 한다. \n",
    "[index+1] 그러나 이것은 잘못된 것으로 올바른 표기는 독도이다.\n",
    "[index+2]\n",
    "</pre>\n",
    "\n",
    "이 알고리즘을 이용해 sentence window를 슬라이딩 시키며 학습을 진행하면 문맥을 함께 고려하는 모델을 만들 수 있을 것이라 생각하였습니다.\n",
    "표기 오류는 오직 [판단할 문장]에 있는지만 체크하도록 학습을 시킬 것으로, 주변 문장의 표기 오류를 검출하여 모델이 혼잡해지는 경우를 최소화하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pretrained 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import tokenizers\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.autonotebook import tqdm\n",
    "import utils\n",
    "\n",
    "class config:\n",
    "    MAX_LEN = 128\n",
    "    TRAIN_BATCH_SIZE = 8\n",
    "    VALID_BATCH_SIZE = 4\n",
    "    EPOCHS = 5\n",
    "    BERT_PATH = \"./bert-base-uncased/\"\n",
    "    MODEL_PATH = \"model.bin\"\n",
    "    TOKENIZER = tokenizers.BertWordPieceTokenizer(\n",
    "        f\"{BERT_PATH}/vocab.txt\", \n",
    "        lowercase=True\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사전 학습 모델 다운로드 완료\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from urllib import request\n",
    "if not os.path.isfile(config.BERT_PATH + \"pytorch_model.bin\"):\n",
    "    print(\"사전 학습 모델 다운로드중\")\n",
    "    request.urlretrieve(\"https://share.easylab.kr/pytorch_model.bin\",config.BERT_PATH + \"pytorch_model.bin\")\n",
    "print(\"사전 학습 모델 다운로드 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기\n",
    "API를 통해 서버로부터 사용 가능한 학습 데이터를 불러옵니다. \n",
    "\n",
    "API는 [ {no, contents, errors[code,keyword] } , ...] 형태로 데이터를 보내주도록 되어있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "class APIDokdo:\n",
    "    def __init__(self, apikey):\n",
    "      self.apiurl = \"https://api.easylab.kr\"\n",
    "      self.headers =  {'authorization': apikey}\n",
    "    def getTrainingData(self):\n",
    "        return requests.get(self.apiurl + \"/deeplearning/data/sentences\", headers=self.headers).json()['list']\n",
    "    def getErrorTypes(self):\n",
    "        return requests.get(self.apiurl + \"/error\", headers=self.headers).json()['list']\n",
    "    def getDocumentList(self, status=None):\n",
    "        return requests.get(self.apiurl + \"/document\",params={'status':status}, headers=self.headers).json()['list']\n",
    "    def getDocument(self, no=None):\n",
    "        return requests.get(self.apiurl + \"/document/\" + str(no), headers=self.headers).json()\n",
    "    def AddDocumentError(self, document_no, sentence_no, code, position, length, text):      \n",
    "        return requests.post(\n",
    "          self.apiurl + \"/document/\" + str(document_no) + \"/error\",\n",
    "          data={'sentence_no':sentence_no, 'text':text, 'code':code, 'position':position, 'length':length}, \n",
    "          headers=self.headers\n",
    "        )\n",
    "    def updateDocument(self, document_no, status, title = None, contents=None):\n",
    "        return requests.put(\n",
    "          self.apiurl + \"/document/\" + str(document_no),\n",
    "          data={'status':status, 'title':title, 'contents':contents}, \n",
    "          headers=self.headers\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불러온 문서의 개수:  44\n"
     ]
    }
   ],
   "source": [
    "api = APIDokdo(\"godapikey12\")\n",
    "original_data_json = api.getTrainingData()\n",
    "print(\"불러온 문서의 개수: \", len(original_data_json))\n",
    "\n",
    "class_list = [0]\n",
    "class_list_from_code = {0:0}\n",
    "for i in api.getErrorTypes():\n",
    "    class_list_from_code[i['code']] = len(class_list)\n",
    "    class_list.append(i['code'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리\n",
    "불러온 데이터를 문장 단위로 분리를 하고, 표기 오류를 검색합니다. 또한 문맥을 고려할 수 있게 5개의 문장씩 관리합니다 (0-5, 1-6, 2-7, 3-8 ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "\n",
    "def split_document(contents, errors):\n",
    "    result = []\n",
    "    sentences = []\n",
    "    # text = i['contents'].replace(\"\\r\",\"\").replace(\"\\n\",\"\")\n",
    "    #sentences = [i.strip() for i in tokenize.sent_tokenize(text)] #문장 단위로 분리 및 문장 앞뒤 공백 제거\n",
    "    sentences = [i.strip() for i in contents.split('\\n')]\n",
    "    \n",
    "    # 두줄 이상 공백이 있는 경우 제거\n",
    "    last = \"\"\n",
    "    remove_indexes = []\n",
    "    for j in range(0, len(sentences)):\n",
    "        if last == \"\" and sentences[j] == \"\":\n",
    "            remove_indexes.append(j)\n",
    "        last = sentences[j]\n",
    "        \n",
    "    for index in sorted(remove_indexes, reverse=True):\n",
    "        del sentences[index]\n",
    "    \n",
    "    # 문장별로 표기 오류 키워드 검색 && 2 + 1 + 2 문장 단위로 자동 구성\n",
    "    # padding\n",
    "    sentences = ['', ''] + sentences + ['', '']\n",
    "    for index in range(2, len(sentences)):\n",
    "        sentence = sentences[index]\n",
    "        # 빈 문장 제거\n",
    "        if (len(sentence.strip()) == 0): continue\n",
    "            \n",
    "        y_class = 0\n",
    "        y_keyword = \"\"\n",
    "        # 현재 문장에 표기 오류가 있는지 확인\n",
    "        sorted(errors, key = lambda item: item['position'])\n",
    "\n",
    "        error_index = 0\n",
    "        for error in errors:\n",
    "            if (error['sentence_no'] != index - 2):\n",
    "                continue\n",
    "            predict_keyword = sentence[error['position']:(error['position']+error['length'])]\n",
    "            if (predict_keyword != error['keyword']):\n",
    "                print(\"에러\")\n",
    "                print(error)\n",
    "            \n",
    "            y_class = class_list_from_code[error['code']]\n",
    "            y_keyword = error['keyword']\n",
    "            sequence = \"sequence \" + str(error_index) + \": \"\n",
    "            sequence = \"\"\n",
    "            position = len(sequence) + error['position']\n",
    "            result.append([[(sequence + sentence if j == index else sentences[j]) for j in range(index-2, index+2 + 1)], y_class, y_keyword, position, error['length'], error_index, index])\n",
    "            error_index += 1\n",
    "            sentence = sentence[:error['position']] + (' ' * error['length']) + sentence[error['position']+error['length']:]\n",
    "        sequence = \"\"\n",
    "        \n",
    "        if (len(sentence.strip()) == 0): continue\n",
    "        result.append([[(sequence+ sentence if j == index else sentences[j]) for j in range(index-2, index+2 + 1)],0, \"\", 0, 0, error_index, index])\n",
    "    return result\n",
    "        \n",
    "original_data = []\n",
    "for i in original_data_json:\n",
    "    d = split_document(i['contents'], i['errors'])\n",
    "    original_data.extend(d)\n",
    "\n",
    "def getInformationFromSentence(i):\n",
    "    return {\n",
    "        'text': i[0],\n",
    "        'y_class': i[1],\n",
    "        'y_keyword': i[2],\n",
    "        'position': i[3],\n",
    "        'length': i[4],\n",
    "        'error_index': i[5],\n",
    "        'sentence_no': i[6]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['It is said that the first appearance of the name - Sea of Japan (Japan Sea) was in \"Kunyu Wanguo Quantu\" by Matteo Ricci (1602).',\n",
       "  '',\n",
       "  'Until the end of the 18th Century, the                          was an unknown sea area, and the shape of the Sea of Japan (Japan Sea) seen in European maps of that area was far from the shape we know at present.',\n",
       "  '',\n",
       "  'In the late 18th Century, however, the great improvement of surveying technology such as the invention of the chronometer (a watch to measure correct time on the sea), enabled the measurement of longitude with high precision, indispensable for accurate surveying.'],\n",
       " 'y_class': 2,\n",
       " 'y_keyword': 'Sea of Japan (Japan Sea)',\n",
       " 'position': 110,\n",
       " 'length': 24,\n",
       " 'error_index': 1,\n",
       " 'sentence_no': 48}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getInformationFromSentence(original_data[151])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리 2\n",
    "\n",
    "데이터 불균형 해결\n",
    "\n",
    "이거 없으면 소수 클래스 예측 엄청 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2276\n",
      "1820\n",
      "456\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "random.shuffle(original_data)\n",
    "index = int(len(original_data) * 0.8)\n",
    "train = original_data[0:index]\n",
    "test_data = original_data[index:]\n",
    "\n",
    "#train = original_data[0:index]\n",
    "#test_data = original_data[0:index]\n",
    "\n",
    "print(len(original_data))\n",
    "print(len(train))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(train)\n",
    "for i in range(0,length):\n",
    "    temp = [[train[i][0][j] for j in range(4,-1,-1)], train[i][1], train[i][2], train[i][3], train[i][4], train[i][5]]\n",
    "    train.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2670, 0, 244, 578, 16, 0, 132, 0, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = train\n",
    "value_counts = [0] * len(class_list)\n",
    "for i in training_data:\n",
    "    value_counts[i[1]] += 1\n",
    "\n",
    "value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ab():\n",
    "    global training_data\n",
    "    training_data = []\n",
    "    for i in train:\n",
    "        training_data.append(i)\n",
    "    temp = [] * len(class_list)\n",
    "    for i in class_list:\n",
    "        temp.append([])\n",
    "    value_counts = [0] * len(class_list)\n",
    "    for i in training_data:\n",
    "        temp[i[1]].append(i)\n",
    "        value_counts[i[1]] += 1\n",
    "\n",
    "    vv = max(value_counts)\n",
    "    print(vv)\n",
    "    for i in range(0,len(class_list)):\n",
    "        if len(temp[i]) == 0:\n",
    "            continue\n",
    "        for j in range(len(temp[i]), vv, len(temp[i])):\n",
    "            training_data.extend(temp[i])\n",
    "\n",
    "    value_counts = [0] * len(class_list)\n",
    "    for i in training_data:\n",
    "        value_counts[i[1]] += 1\n",
    "\n",
    "    return value_counts\n",
    "    \n",
    "# ab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리 3\n",
    "가공된 데이터를 BERT 모델에 넣을 수 있도록 만들어야합니다. 모델에는 자연어를 그대로 입력할 수 없으니 사전 학습된 BERT 모델의 vocabulary를 활용하여 토큰화 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분활된 토큰 형태: ['[CLS]', 'the', '“', 'takeshima', 'zu', '##set', '##su', '[', 'explanation', 'of', 'takeshima', 'with', 'maps', ']', '”', 'edited', 'by', 'ts', '##uan', 'kit', '##az', '##ono', 'during', 'the', 'ho', '##rek', '##i', 'period', '(', '1751', '-', '1763', ')', 'contains', 'the', 'following', 'description', ':', 'there', 'is', 'an', 'island', 'about', '40', 'ri', 'north', 'of', 'the', 'west', 'island', '(', 'ni', '##shi', '##jima', ')', 'of', 'mats', '##ush', '##ima', '(', 'takeshima', ')', 'in', '3', 'ok', '##i', 'county', '.', '[SEP]']\n",
      "숫자형 토큰 형태: [101, 1996, 1523, 3, 16950, 13462, 6342, 1031, 7526, 1997, 3, 2007, 7341, 1033, 1524, 5493, 2011, 24529, 13860, 8934, 10936, 17175, 2076, 1996, 7570, 16816, 2072, 2558, 1006, 24440, 1011, 18432, 1007, 3397, 1996, 2206, 6412, 1024, 2045, 2003, 2019, 2479, 2055, 2871, 15544, 2167, 1997, 1996, 2225, 2479, 1006, 9152, 6182, 19417, 1007, 1997, 22281, 20668, 9581, 1006, 3, 1007, 1999, 1017, 7929, 2072, 2221, 1012, 102]\n",
      "offsets: [(0, 0), (0, 3), (4, 5), (5, 14), (15, 17), (17, 20), (20, 22), (23, 24), (24, 35), (36, 38), (39, 48), (49, 53), (54, 58), (58, 59), (59, 60), (61, 67), (68, 70), (71, 73), (73, 76), (77, 80), (80, 82), (82, 85), (86, 92), (93, 96), (97, 99), (99, 102), (102, 103), (104, 110), (111, 112), (112, 116), (116, 117), (117, 121), (121, 122), (123, 131), (132, 135), (136, 145), (146, 157), (157, 158), (159, 164), (165, 167), (168, 170), (171, 177), (178, 183), (184, 186), (187, 189), (190, 195), (196, 198), (199, 202), (203, 207), (208, 214), (215, 216), (216, 218), (218, 221), (221, 225), (225, 226), (227, 229), (230, 234), (234, 237), (237, 240), (241, 242), (242, 251), (251, 252), (253, 255), (256, 257), (258, 260), (260, 261), (262, 268), (268, 269), (0, 0)]\n",
      "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "special_tokens_mask: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "special_tokens_mask: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "overflowing: []\n"
     ]
    }
   ],
   "source": [
    "tok_tweet = config.TOKENIZER.encode(\"The “Takeshima Zusetsu [Explanation of Takeshima with Maps]” edited by Tsuan Kitazono during the Horeki Period (1751-1763) contains the following description: There is an island about 40 ri north of the west island (Nishijima) of Matsushima (Takeshima) in 3 Oki county.\")\n",
    "print(\"분활된 토큰 형태: \" + str(tok_tweet.tokens))\n",
    "print(\"숫자형 토큰 형태: \" + str(tok_tweet.ids))\n",
    "print(\"offsets: \" + str(tok_tweet.offsets))\n",
    "print(\"attention_mask: \" + str(tok_tweet.attention_mask))\n",
    "print(\"special_tokens_mask: \" + str(tok_tweet.special_tokens_mask))\n",
    "print(\"special_tokens_mask: \" + str(tok_tweet.special_tokens_mask))\n",
    "print(\"overflowing: \" + str(tok_tweet.overflowing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text, y_class, y_keyword, position, length, error_index):\n",
    "    if (len(text[1]) > 50):\n",
    "        before = text[1]\n",
    "    else:\n",
    "        before = text[0] + \" \" + text[1]\n",
    "        \n",
    "    main = text[2]\n",
    "    \n",
    "    if (len(text[3]) > 50):\n",
    "        after = text[3]\n",
    "    else:\n",
    "        after = text[3] + \" \" + text[4]\n",
    "    # before = \"\"\n",
    "    # after = \"\"\n",
    "    before = config.TOKENIZER.encode(before)\n",
    "    main = config.TOKENIZER.encode(main)\n",
    "    after = config.TOKENIZER.encode(after)\n",
    "\n",
    "\n",
    "    # 토큰 기준으로 키워드가 어디있는지 확인\n",
    "    keyword_position_in_token = -1\n",
    "    keyword_end_in_token = -1\n",
    "    if y_class != 0:\n",
    "        keyword_position_in_string = position\n",
    "        keyword_length_in_string = length\n",
    "        for j in range(len(main.offsets)):\n",
    "            if keyword_position_in_token == -1 and main.offsets[j][0] >= keyword_position_in_string:\n",
    "                keyword_position_in_token = j\n",
    "            if main.offsets[j][1] == 0: continue\n",
    "            if main.offsets[j][1] <= (keyword_position_in_string + keyword_length_in_string):\n",
    "                keyword_end_in_token = j\n",
    "    else:        \n",
    "        keyword_position_in_token = 0\n",
    "        keyword_end_in_token = 0\n",
    "        \n",
    "    # ids = cls, classification number, sep, token, sep\n",
    "    ids = [101, 9999, 102] + main.ids[1:] + before.ids[1:] + after.ids[1:]\n",
    "    # mask = len(cls, classification number, sep, token, sep) = 1, else 0\n",
    "    mask = [1] * len(ids)\n",
    "    # token_type_ids len(token, sep) = 1, else 0\n",
    "    token_type_ids = [0,0,0] +  [1] * (len(main) - 1) + [0] * (len(before) - 1) + [0] * (len(after) - 1)\n",
    "\n",
    "    targets_start = keyword_position_in_token\n",
    "    targets_end = keyword_end_in_token\n",
    "\n",
    "    # offsets based on ids, token offsets (0,0)(0,0)(0,0)(0,a)...(0,0)\n",
    "    offsets = main.offsets\n",
    "    \n",
    "    # Pad sequence if its length < `max_len`\n",
    "    padding_length = 380 - len(ids)\n",
    "    if padding_length > 0:\n",
    "        ids = ids + ([0] * padding_length)\n",
    "        mask = mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        padding_length = 380 - len(offsets)\n",
    "        offsets = offsets + ([(0, 0)] * padding_length)\n",
    "        \n",
    "    temp = []\n",
    "    temp = [0] * len(class_list)\n",
    "    temp[y_class] = 1\n",
    "            \n",
    "    return {\n",
    "            'ids': ids,\n",
    "            'mask': mask, \n",
    "            'token_type_ids': token_type_ids,\n",
    "            'targets_start': targets_start, \n",
    "            'targets_end': targets_end, \n",
    "            'orig_text': text,\n",
    "            'orig_keyword': y_keyword,\n",
    "            'class': y_class,\n",
    "            'offsets': offsets ,\n",
    "            'targets_class': temp,\n",
    "            'error_index':[error_index] * 380\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [101, 9999, 102, 1999, 2901, 2062, 2084, 2093, 6474, 2308, 1521, 1055, 2967, 1999, 2148, 4420, 2587, 2000, 5323, 1996, 4759, 2473, 2005, 1996, 2308, 7462, 2005, 2510, 4424, 8864, 2011, 2900, 2044, 3988, 2887, 14920, 1997, 5368, 1012, 102, 4002, 1024, 5890, 6021, 1024, 2538, 102, 1996, 2473, 2356, 2005, 2019, 6449, 26897, 1997, 12731, 14277, 8010, 1010, 2019, 12480, 1010, 1037, 3986, 1010, 1998, 3361, 9430, 2005, 5694, 1998, 2008, 2887, 18841, 2022, 23263, 8776, 2000, 8339, 1996, 22213, 1997, 1996, 4424, 8864, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'targets_start': 0, 'targets_end': 0, 'orig_text': ['00:01 03:21', '', 'In 1990 more than three dozen women’s groups in South Korea joined to establish the Korean Council for the Women Drafted for Military Sexual Slavery by Japan after initial Japanese denial of responsibility.', 'The council asked for an admittance of culpability, an apology, a memorial, and financial compensation for victims and that Japanese textbooks be appropriately altered to reflect the realities of the sexual slavery.', 'The Japanese government denied evidence of coercing comfort women and rejected calls for compensation, saying that the 1965 treaty between Japan and South Korea had settled all outstanding matters.'], 'orig_keyword': '', 'class': 0, 'offsets': [(0, 0), (0, 2), (3, 7), (8, 12), (13, 17), (18, 23), (24, 29), (30, 35), (35, 36), (36, 37), (38, 44), (45, 47), (48, 53), (54, 59), (60, 66), (67, 69), (70, 79), (80, 83), (84, 90), (91, 98), (99, 102), (103, 106), (107, 112), (113, 120), (121, 124), (125, 133), (134, 140), (141, 148), (149, 151), (152, 157), (158, 163), (164, 171), (172, 180), (181, 187), (188, 190), (191, 205), (205, 206), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)], 'targets_class': [1, 0, 0, 0, 0, 0, 0, 0, 0], 'error_index': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "i = 4\n",
    "a = preprocessing(training_data[i][0], training_data[i][1],training_data[i][2], training_data[i][3], training_data[i][4], training_data[i][5])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이토치를 위한 데이터셋 클래스 생성\n",
    "이 클래스는 파이토치에서 데이터를 로드할 때 사용되는 인터페이스입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset:\n",
    "    \"\"\"\n",
    "    Dataset which stores the tweets and returns them as processed features\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        data = preprocessing(\n",
    "            self.dataset[item][0], \n",
    "            self.dataset[item][1], \n",
    "            self.dataset[item][2], \n",
    "            self.dataset[item][3], \n",
    "            self.dataset[item][4], \n",
    "            self.dataset[item][5]\n",
    "        )\n",
    "        temp = []\n",
    "        for i in data[\"targets_class\"]:\n",
    "            temp.append(torch.tensor(i, dtype=torch.long))\n",
    "            \n",
    "        # Return the processed data where the lists are converted to `torch.tensor`s\n",
    "        return {\n",
    "            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n",
    "            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n",
    "            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n",
    "            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n",
    "            'targets_class': torch.tensor(data[\"targets_class\"], dtype=torch.long),\n",
    "            'orig_tweet': data[\"orig_text\"],\n",
    "            'orig_selected': data[\"orig_keyword\"],\n",
    "            'sentiment': data[\"class\"],\n",
    "            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long),\n",
    "            'error_index': torch.tensor(data[\"error_index\"], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = TextDataset(training_data)\n",
    "train_dataset[4]['error_index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이토치 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "class TweetModel(transformers.BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Model class that combines a pretrained bert model with a linear later\n",
    "    \"\"\"\n",
    "    def __init__(self, conf, num_labels):\n",
    "        super(TweetModel, self).__init__(conf)\n",
    "        # pretrained BERT model을 불러옵니다.\n",
    "        self.bert = transformers.BertModel.from_pretrained(config.BERT_PATH, config=conf)\n",
    "        \n",
    "        # Set 10% dropout to be applied to the BERT backbone's output\n",
    "        # dropout은 은닉층에서 일정 확률로 유닛을 사용하지 않도록(=0) 합니다.\n",
    "        # 따라서 해당 케이스에서는 사용된 유닛만을 이용해 loss를 구하고 grident를 수행합니다.\n",
    "        # 결국 오버피팅 방지 가능!! (하나의 유닛에 의존하는 현상을 제거)\n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "        \n",
    "        # 우리가 쓰는 bert-base-uncased 모델은 768의 hidden representation을 가지고 있음\n",
    "        # 그래서 새로운 레이어를 이어 붙일 때에도 768개씩 붙여야함.\n",
    "        \n",
    "        # 우리의 데이터를 추가로 학습하는 용도로 사용할 추가적인 레이어가 필요함. (hidden_layer 추가)\n",
    "        # 히든 레이어를 추가할수록 복잡한 딥러닝 네트워크를 만들 수 있지만... 데이터가 많이 필요할 듯\n",
    "        \n",
    "        # 여기에서는 단어 임베딩 결과를 활용할 수 있게 레이어 정의\n",
    "        # BERT를 수행하며 나온 hidden layer 12개중, 마지막 10번째 11번째를 사용할 것임.\n",
    "        # 12번째는 오버피팅 가능성이 높기 때문\n",
    "        # 따라서 10번째(768) 11번째 (768) 두개의 레이어를 input으로 받을 것이기 때문에 레이어의 input은 768 * 2\n",
    "        \n",
    "        # layer0만으로 결과를 내기에는 제대로 학습이 안된다고 판단이 되어 같은 크기의 레이어 layer1를 추가할 예정\n",
    "        # 따라서 768*2 -> 768*2 레이어 정의\n",
    "        self.l0 = nn.Linear(768 * 2, 768 * 2)\n",
    "        \n",
    "        # l0으로부터 768*2 결과를 전달받아 최종적으로 start, end, class를 판단하기 위한 layer1를 정의\n",
    "        self.l1 = nn.Linear(768 * 2, 2 + num_labels)\n",
    "        \n",
    "        # 사용할 activation 함수\n",
    "        self.gelu = nn.GELU()\n",
    "        \n",
    "        # 가중치 랜덤 초기화\n",
    "        torch.nn.init.normal_(self.l0.weight, std=0.02)\n",
    "        torch.nn.init.normal_(self.l1.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids, error_index):\n",
    "        # BERT backbone으로부터 hidden states를 얻어옴.\n",
    "        _, _, out = self.bert(\n",
    "            ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        ) # bert_layers x bs x SL x (768)\n",
    "\n",
    "        # Concatenate the last two hidden states\n",
    "        # This is done since experiments have shown that just getting the last layer\n",
    "        # gives out vectors that may be too taylored to the original BERT training objectives (MLM + NSP)\n",
    "        # Sample explanation: https://bert-as-service.readthedocs.io/en/latest/section/faq.html#why-not-the-last-hidden-layer-why-second-to-last\n",
    "        \n",
    "        # BERT를 수행하며 나온 hidden layer의 output에서 -2번째, -1번째만 가져옴. 그리고 한줄로 이어 붙이기\n",
    "\n",
    "        error_index = error_index.view(out[-1].shape[0],out[-1].shape[1],1)\n",
    "        out = torch.cat((out[-1], out[-2]), dim=-1) # bs x SL x (768 * 2)\n",
    "        \n",
    "        # 위에서 말했던것 처럼 10%의 노드를 제거\n",
    "        out = self.drop_out(out) # bs x SL x (768 * 2)\n",
    "        # The \"dropped out\" hidden vectors are now fed into the linear layer to output two scores\n",
    "        \n",
    "        # 해당 결과를 layer0에 통과\n",
    "        out = self.l0(out) # bs x SL x 2\n",
    "        \n",
    "        # 이 결과를 바로 사용할 것은 아니기에 gelu functaion을 거치게 만듦.\n",
    "        out = self.gelu(out)\n",
    "        \n",
    "        # layer1로 가기 전에도 똑같이 drop oup 진행\n",
    "        out = self.drop_out(out)\n",
    "        \n",
    "        # layer1로 전달\n",
    "        logits = self.l1(out)\n",
    "        \n",
    "        # 현재 layer1은 n개의 output을 내기때문에 이것을 분리\n",
    "        # (bs x SL x n) -> (bs x SL x 1), (bs x SL x 1) ...\n",
    "        outputs = list(logits.split(1, dim=-1))\n",
    "        for i in range(0,len(outputs)):\n",
    "            outputs[i] = outputs[i].squeeze(-1)\n",
    "            \n",
    "\n",
    "        start_logits = outputs[0] # (bs x SL)\n",
    "        end_logits = outputs[1] # (bs x SL)\n",
    "        class_logits = outputs[2:]\n",
    "        return start_logits, end_logits, class_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(start_logits, end_logits, class_logits, start_positions, end_positions, class_targets):\n",
    "    \"\"\"\n",
    "    Return the sum of the cross entropy losses for both the start and end logits\n",
    "    \"\"\"\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    start_loss = loss_fct(start_logits, start_positions)\n",
    "    end_loss = loss_fct(end_logits, end_positions)\n",
    "    total_loss = start_loss + end_loss\n",
    "        \n",
    "    class_targets = class_targets.t()\n",
    "\n",
    "    for i in range(0, len(class_list)):\n",
    "        total_loss += loss_fct(class_logits[i], class_targets[i]) / len(class_list)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, optimizer, device, scheduler=None):\n",
    "    \"\"\"\n",
    "    Trains the bert model on the twitter data\n",
    "    \"\"\"\n",
    "    # Set model to training mode (dropout + sampled batch norm is activated)\n",
    "    model.train()\n",
    "    losses = utils.AverageMeter()\n",
    "    jaccards = utils.AverageMeter()\n",
    "\n",
    "    # Set tqdm to add loading screen and set the length\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    \n",
    "    # Train the model on each batch\n",
    "    for bi, d in enumerate(tk0):\n",
    "\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        targets_start = d[\"targets_start\"]\n",
    "        targets_end = d[\"targets_end\"]\n",
    "        sentiment = d[\"sentiment\"]\n",
    "        orig_selected = d[\"orig_selected\"]\n",
    "        orig_tweet = d[\"orig_tweet\"][2]\n",
    "        offsets = d[\"offsets\"]\n",
    "        targets_class = d[\"targets_class\"]\n",
    "        error_index = d[\"error_index\"]\n",
    "\n",
    "        # Move ids, masks, and targets to gpu while setting as torch.long\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets_start = targets_start.to(device, dtype=torch.long)\n",
    "        targets_end = targets_end.to(device, dtype=torch.long)\n",
    "        targets_class = targets_class.to(device, dtype=torch.long)\n",
    "        error_index = error_index.to(device, dtype=torch.long)\n",
    "        # Reset gradients\n",
    "        model.zero_grad()\n",
    "        # Use ids, masks, and token types as input to the model\n",
    "        # Predict logits for each of the input tokens for each batch\n",
    "        outputs_start, outputs_end, outputs_class = model(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            error_index=error_index\n",
    "        ) # (bs x SL), (bs x SL)\n",
    "        # Calculate batch loss based on CrossEntropy\n",
    "        loss = loss_fn(outputs_start, outputs_end, outputs_class, targets_start, targets_end, targets_class)\n",
    "        # Calculate gradients based on loss\n",
    "        loss.backward()\n",
    "        # Adjust weights based on calculated gradients\n",
    "        optimizer.step()\n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Apply softmax to the start and end logits\n",
    "        # This squeezes each of the logits in a sequence to a value between 0 and 1, while ensuring that they sum to 1\n",
    "        # This is similar to the characteristics of \"probabilities\"\n",
    "        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "        outputs_class = [torch.softmax(i, dim=1).cpu().detach().numpy() for i in outputs_class]\n",
    "        \n",
    "        # Calculate the jaccard score based on the predictions for this batch\n",
    "        jaccard_scores = []\n",
    "        for px, tweet in enumerate(orig_tweet):\n",
    "            selected_tweet = orig_selected[px]\n",
    "            tweet_sentiment = sentiment[px]\n",
    "            ont_hot_class = [np.argmax(i[px, :]) for i in outputs_class]\n",
    "            class_number = ont_hot_class.index(max(ont_hot_class))\n",
    "            jaccard_score, _ = calculate_jaccard_score(\n",
    "                original_tweet=tweet, # Full text of the px'th tweet in the batch\n",
    "                target_string=selected_tweet, # Span containing the specified sentiment for the px'th tweet in the batch\n",
    "                sentiment_val=tweet_sentiment, # Sentiment of the px'th tweet in the batch\n",
    "                idx_start=np.argmax(outputs_start[px, :]), # Predicted start index for the px'th tweet in the batch\n",
    "                idx_end=np.argmax(outputs_end[px, :]), # Predicted end index for the px'th tweet in the batch\n",
    "                offsets=offsets[px], # Offsets for each of the tokens for the px'th tweet in the batch\n",
    "                class_number=class_number\n",
    "            )\n",
    "            jaccard_scores.append(jaccard_score)\n",
    "        # Update the jaccard score and loss\n",
    "        # For details, refer to `AverageMeter` in https://www.kaggle.com/abhishek/utils\n",
    "        jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
    "        losses.update(loss.item(), ids.size(0))\n",
    "        # Print the average loss and jaccard score at the end of each batch\n",
    "        tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(data_loader, model, device):\n",
    "    \"\"\"\n",
    "    Evaluation function to predict on the test set\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    # I.e., turn off dropout and set batchnorm to use overall mean and variance (from training), rather than batch level mean and variance\n",
    "    # Reference: https://github.com/pytorch/pytorch/issues/5406\n",
    "    model.eval()\n",
    "    losses = utils.AverageMeter()\n",
    "    jaccards = utils.AverageMeter()\n",
    "    \n",
    "    # Turns off gradient calculations (https://datascience.stackexchange.com/questions/32651/what-is-the-use-of-torch-no-grad-in-pytorch)\n",
    "    with torch.no_grad():\n",
    "        tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "        # Make predictions and calculate loss / jaccard score for each batch\n",
    "        for bi, d in enumerate(tk0):\n",
    "            ids = d[\"ids\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            sentiment = d[\"sentiment\"]\n",
    "            orig_selected = d[\"orig_selected\"]\n",
    "            orig_tweet = d[\"orig_tweet\"][2]\n",
    "            targets_start = d[\"targets_start\"]\n",
    "            targets_end = d[\"targets_end\"]\n",
    "            offsets = d[\"offsets\"].numpy()\n",
    "            targets_class = d[\"targets_class\"]\n",
    "            error_index = d[\"error_index\"]\n",
    "\n",
    "            # Move tensors to GPU for faster matrix calculations\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            targets_start = targets_start.to(device, dtype=torch.long)\n",
    "            targets_end = targets_end.to(device, dtype=torch.long)\n",
    "            targets_class = targets_class.to(device, dtype=torch.long)\n",
    "            error_index = error_index.to(device, dtype=torch.long)\n",
    "\n",
    "            # Predict logits for start and end indexes\n",
    "            outputs_start, outputs_end, outputs_class = model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                error_index=error_index\n",
    "            )\n",
    "            # Calculate loss for the batch\n",
    "            loss = loss_fn(outputs_start, outputs_end, outputs_class, targets_start, targets_end, targets_class)\n",
    "            # Apply softmax to the predicted logits for the start and end indexes\n",
    "            # This converts the \"logits\" to \"probability-like\" scores\n",
    "            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "            outputs_class = [torch.softmax(i, dim=1).cpu().detach().numpy() for i in outputs_class]\n",
    "            \n",
    "            # Calculate jaccard scores for each tweet in the batch\n",
    "            jaccard_scores = []\n",
    "            for px, tweet in enumerate(orig_tweet):\n",
    "                selected_tweet = orig_selected[px]\n",
    "                tweet_sentiment = sentiment[px]\n",
    "                ont_hot_class = [np.argmax(i[px, :]) for i in outputs_class]\n",
    "                class_number = ont_hot_class.index(max(ont_hot_class))\n",
    "                \n",
    "                jaccard_score, _ = calculate_jaccard_score(\n",
    "                    original_tweet=tweet,\n",
    "                    target_string=selected_tweet,\n",
    "                    sentiment_val=tweet_sentiment,\n",
    "                    idx_start=np.argmax(outputs_start[px, :]),\n",
    "                    idx_end=np.argmax(outputs_end[px, :]),\n",
    "                    offsets=offsets[px],\n",
    "                    class_number=class_number\n",
    "                )\n",
    "                jaccard_scores.append(jaccard_score)\n",
    "\n",
    "            # Update running jaccard score and loss\n",
    "            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
    "            losses.update(loss.item(), ids.size(0))\n",
    "            # Print the running average loss and jaccard score\n",
    "            tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n",
    "    \n",
    "    print(f\"Jaccard = {jaccards.avg}\")\n",
    "    return jaccards.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jaccard_score(\n",
    "    original_tweet, \n",
    "    target_string, \n",
    "    sentiment_val, \n",
    "    idx_start, \n",
    "    idx_end, \n",
    "    offsets,\n",
    "    class_number,\n",
    "    verbose=False):\n",
    "    \"\"\"\n",
    "    Calculate the jaccard score from the predicted span and the actual span for a batch of tweets\n",
    "    \"\"\"\n",
    "    # A span's end index has to be greater than or equal to the start index\n",
    "    # If this doesn't hold, the start index is set to equal the end index (the span is a single token)\n",
    "    if idx_end < idx_start:\n",
    "        idx_end = idx_start\n",
    "    \n",
    "    # Combine into a string the tokens that belong to the predicted span\n",
    "    filtered_output  = \"\"\n",
    "    for ix in range(idx_start, idx_end + 1):\n",
    "        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n",
    "        # If the token is not the last token in the tweet, and the ending offset of the current token is less\n",
    "        # than the beginning offset of the following token, add a space.\n",
    "        # Basically, add a space when the next token (word piece) corresponds to a new word\n",
    "        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n",
    "            filtered_output += \" \"\n",
    "    #print(filtered_output)\n",
    "    # Set the predicted output as the original tweet when the tweet's sentiment is \"neutral\", or the tweet only contains one word\n",
    "    if sentiment_val == 0 or len(original_tweet.split()) < 2:\n",
    "        filtered_output = original_tweet\n",
    "    # Calculate the jaccard score between the predicted span, and the actual span\n",
    "    # The IOU (intersection over union) approach is detailed in the utils module's `jaccard` function:\n",
    "    # https://www.kaggle.com/abhishek/utils\n",
    "    \n",
    "    jac = utils.jaccard(target_string.strip(), filtered_output.strip())\n",
    "    if class_number != sentiment_val:\n",
    "        jac *= 0.5\n",
    "    else:  \n",
    "        if sentiment_val == 0:\n",
    "            jac = 1\n",
    "        \n",
    "    return jac, filtered_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is Starting for fold=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199d7820b6a24ed79a43ac9bebc50173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=455.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79d0f02d7cb4947b72633b9092ded71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=57.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.8796219432173866\n",
      "Jaccard Score = 0.8796219432173866\n",
      "Validation score improved (-inf --> 0.8796219432173866). Saving model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e3383a4e394cbba4279cb2b5314c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=455.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5d1c0b0f554cf285a5214e0201f65a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=57.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.8825301096571414\n",
      "Jaccard Score = 0.8825301096571414\n",
      "Validation score improved (0.8796219432173866 --> 0.8825301096571414). Saving model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd2aa884dd347f3a0cf266aaf7d2cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=455.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7f9ea8d75b476bb86d9b1ead031912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=57.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.9063929118403008\n",
      "Jaccard Score = 0.9063929118403008\n",
      "Validation score improved (0.8825301096571414 --> 0.9063929118403008). Saving model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c673e153d644b4baab0288ffa7718ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=455.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350fa7cb009b44e8a59602b135c402f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=57.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.9079987229025422\n",
      "Jaccard Score = 0.9079987229025422\n",
      "Validation score improved (0.9063929118403008 --> 0.9079987229025422). Saving model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7450d66b54894473938c2184d5599f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=455.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94aae835db0c4417a862bcebdd4d8ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=57.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.8988228292263584\n",
      "Jaccard Score = 0.8988228292263584\n",
      "EarlyStopping counter: 1 out of 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0366149c1ddf4de18c486f224f04ce1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=455.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2888e2d2670e404cab7fe9a16b203bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=57.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.8988228292263586\n",
      "Jaccard Score = 0.8988228292263586\n",
      "EarlyStopping counter: 2 out of 2\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(training_data)\n",
    "\n",
    "# Instantiate DataLoader with `train_dataset`\n",
    "# This is a generator that yields the dataset in batches\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.TRAIN_BATCH_SIZE,\n",
    "    num_workers=0,\n",
    "    shuffle = True\n",
    ")\n",
    "#    shuffle = True\n",
    "valid_dataset = TextDataset(test_data)\n",
    "\n",
    "# Instantiate DataLoader with `train_dataset`\n",
    "# This is a generator that yields the dataset in batches\n",
    "valid_data_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=config.TRAIN_BATCH_SIZE,\n",
    "    num_workers=0,\n",
    "    shuffle = True\n",
    ")\n",
    "# Set device as `cuda` (GPU)\n",
    "device = torch.device(\"cuda\")\n",
    "# Load pretrained BERT (bert-base-uncased)\n",
    "model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\n",
    "# Output hidden states\n",
    "# This is important to set since we want to concatenate the hidden states from the last 2 BERT layers\n",
    "model_config.output_hidden_states = True\n",
    "# Instantiate our model with `model_config`\n",
    "model = TweetModel(conf=model_config, num_labels=len(class_list))\n",
    "# Move the model to the GPU\n",
    "model.to(device)\n",
    "\n",
    "# Calculate the number of training steps\n",
    "num_train_steps = int(len(training_data) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
    "# Get the list of named parameters\n",
    "param_optimizer = list(model.named_parameters())\n",
    "# Specify parameters where weight decay shouldn't be applied\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "# Define two sets of parameters: those with weight decay, and those without\n",
    "optimizer_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "# Instantiate AdamW optimizer with our two sets of parameters, and a learning rate of 3e-5\n",
    "optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "# Create a scheduler to set the learning rate at each training step\n",
    "# \"Create a schedule with a learning rate that decreases linearly after linearly increasing during a warmup period.\" (https://pytorch.org/docs/stable/optim.html)\n",
    "# Since num_warmup_steps = 0, the learning rate starts at 3e-5, and then linearly decreases at each training step\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=num_train_steps\n",
    ")\n",
    "\n",
    "# Apply early stopping with patience of 2\n",
    "# This means to stop training new epochs when 2 rounds have passed without any improvement\n",
    "es = utils.EarlyStopping(patience=2, mode=\"max\")\n",
    "fold = 0\n",
    "print(f\"Training is Starting for fold={fold}\")\n",
    "\n",
    "# I'm training only for 3 epochs even though I specified 5!!!\n",
    "for epoch in range(50):\n",
    "    train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n",
    "    jaccard = eval_fn(valid_data_loader, model, device)\n",
    "    print(f\"Jaccard Score = {jaccard}\")\n",
    "    es(jaccard, model, model_path=f\"model_{fold}.bin\")\n",
    "    if es.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "        \n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "del  device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\n",
    "model_config.output_hidden_states = True\n",
    "\n",
    "model1 = TweetModel(conf=model_config, num_labels=len(class_list))\n",
    "model1.to(device)\n",
    "model1.load_state_dict(torch.load(\"model_0.bin\"))\n",
    "model1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ffd4bdffc04ee995b8e3bbaa857e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=114.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final_output = []\n",
    "\n",
    "# Instantiate TweetDataset with the test data\n",
    "test_dataset = TextDataset(test_data)\n",
    "\n",
    "# Instantiate DataLoader with `test_dataset`\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=config.VALID_BATCH_SIZE,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "# Turn of gradient calculations\n",
    "with torch.no_grad():\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    # Predict the span containing the sentiment for each batch\n",
    "    for bi, d in enumerate(tk0):\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        sentiment = d[\"sentiment\"]\n",
    "        orig_selected = d[\"orig_selected\"]\n",
    "        orig_tweet = d[\"orig_tweet\"][2]\n",
    "        targets_start = d[\"targets_start\"]\n",
    "        targets_end = d[\"targets_end\"]\n",
    "        offsets = d[\"offsets\"].numpy()\n",
    "        error_index = d[\"error_index\"]\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets_start = targets_start.to(device, dtype=torch.long)\n",
    "        targets_end = targets_end.to(device, dtype=torch.long)\n",
    "        error_index = error_index.to(device, dtype=torch.long)\n",
    "\n",
    "        # Predict start and end logits for each of the five models\n",
    "        outputs_start, outputs_end, outputs_class = model1(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            error_index=error_index\n",
    "        )\n",
    "        \n",
    "        # Apply softmax to the predicted start and end logits\n",
    "        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "        outputs_class = [torch.softmax(i, dim=1).cpu().detach().numpy() for i in outputs_class]\n",
    "        # Convert the start and end scores to actual predicted spans (in string form)\n",
    "        for px, tweet in enumerate(orig_tweet):\n",
    "            selected_tweet = orig_selected[px]\n",
    "            tweet_sentiment = sentiment[px]\n",
    "            ont_hot_class = [np.argmax(i[px, :]) for i in outputs_class]\n",
    "            class_number = ont_hot_class.index(max(ont_hot_class))\n",
    "            if tweet_sentiment == class_number:\n",
    "                if class_number == 0:\n",
    "                    TN += 1\n",
    "                else:\n",
    "                    TP += 1\n",
    "            else:\n",
    "                if class_number == 0:\n",
    "                    FN += 1 # 원래는 양성인데 음성으로 예측\n",
    "                else:\n",
    "                    FP += 1\n",
    "                    \n",
    "            _, output_sentence = calculate_jaccard_score(\n",
    "                original_tweet=tweet,\n",
    "                target_string=selected_tweet,\n",
    "                sentiment_val=tweet_sentiment,\n",
    "                idx_start=np.argmax(outputs_start[px, :]),\n",
    "                idx_end=np.argmax(outputs_end[px, :]),\n",
    "                offsets=offsets[px],\n",
    "                class_number = class_number\n",
    "            )\n",
    "            final_output.append([np.argmax(outputs_start[px, :]), np.argmax(outputs_end[px, :]), output_sentence, class_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도:  0.9495614035087719\n",
      "Precision:  0.8660714285714286  (특정 오류가 있다고 예측한 것 중 실제 그럴 확률)\n",
      "Recall:  0.9238095238095239  (실제 오류중에 제대로 검출할 확률)\n",
      "문서 개수:  456\n",
      "TP:  97\n",
      "TN:  336\n",
      "FN:  8\n",
      "FP:  15\n"
     ]
    }
   ],
   "source": [
    "print(\"정확도: \", (TP + TN)  / (TN+TP+FN+FP))\n",
    "print(\"Precision: \", (TP)  / (TP+FP), \" (특정 오류가 있다고 예측한 것 중 실제 그럴 확률)\")\n",
    "print(\"Recall: \", (TP)  / (TP+FN), \" (실제 오류중에 제대로 검출할 확률)\")\n",
    "\n",
    "print(\"문서 개수: \", (TN+TP+FN+FP))\n",
    "\n",
    "print(\"TP: \", (TP))\n",
    "print(\"TN: \", (TN))\n",
    "print(\"FN: \", (FN))\n",
    "print(\"FP: \", (FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"tw\": \"Japan calls it Takeshima, which means bamboo islands.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The Republic of Korea has been occupying Takeshima with no basis in international law.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Republic of Korea has been occupying Takeshima with no basis in international law\",\n",
      "    \"predicted_keyword\": \"The Republic of Korea has been occupying Takeshima with no basis in international law\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"In May 1905, the administrator of Okinoshima Island submitted an application to the governor of Shimane Prefecture to register Takeshima\\u2019s land area as \\u201c23 cho, 3 tan, 3 se bu\\u201d (Japan\\u2019s old system of measurement).\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\\u2019s land\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The                 viewed from the North Owing to their location and small size, the Liancourt Rocks can have harsh weather.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Liancourt Rocks\",\n",
      "    \"predicted_keyword\": \"Liancourt Rocks \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"In the press release, Mitte District Mayor Stephan von Dassel vowed to review the controversial issue of so-called \\\"comfort women\\\" from both Korean and Japanese perspectives, and also from that of the district, for a sufficient amount of time before reaching a conclusion.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"comfort women\",\n",
      "    \"predicted_keyword\": \"comfort women\",\n",
      "    \"class\": 6,\n",
      "    \"predicted_class\": 6\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Read more: Fury as Japanese paper redefines 'comfort women'                                                                 ' \\\"What is particularly irritating is that the South Korean government has repeatedly promised that it would not permit this sort of behavior by Korean people because it was committed to building better relations with Japan \\u2013 but here we are again,\\\" said Shimada.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"comfort women\",\n",
      "    \"predicted_keyword\": \"women\",\n",
      "    \"class\": 6,\n",
      "    \"predicted_class\": 6\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The ROK, however, had not raised this issue in \\\"United Nations Conference on Standardization of Geographical Names\\\" or \\\"the International Hydrographic Organization\\\" (IHO) until the beginning of the 1990s and the ROK used the name of Sea of Japan (Japan Sea) even in its own nautical charts.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"ROK used the name of Sea of Japan (Japan Sea)\",\n",
      "    \"predicted_keyword\": \"The ROK, however, had not raised this issue in \\\"United Nations Conference on Standardization of Geographical Names\\\" or \\\"the International Hydrographic Organization\\\" (IHO) \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The Japanese Government's survey of historical maps confirms that the name Sea of Japan was already prevalent at the early 19th century.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"confirms that the name Sea of Japan was already prevalent at the early 19th century\",\n",
      "    \"predicted_keyword\": \"The Japanese Government's survey of historical maps confirms that the name Sea of Japan was already prevalent at the early 19th century\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"From her base in Jakarta, Janssen traveled the Indonesian archipelago for two years searching for comfort women.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"comfort women\",\n",
      "    \"predicted_keyword\": \"comfort women\",\n",
      "    \"class\": 6,\n",
      "    \"predicted_class\": 6\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The above description reveals that at the time, the              , on its way to Utsuryo Island, commonly made a port call at           which was along the route, and that Takeshima belongs to Japanese territory although it is \\u201clocated at the far end of Japan\\u2019s western sea.\\u201d With regard to old maps, \\u201cTakeshima no Ezu [Diagrams of Takeshima]\\u201d, made around 1696 by Ihei Kotani, a government official in Tottori domain, provides an accurate illustration of Takeshima\\u2019s Higashijima and Nishijima Islands and associated reefs.\",\n",
      "    \"error_index\": 2,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima belongs to Japanese territory although it is \\u201clocated at the far end of Japan\\u2019s western sea.\\u201d With regard to old maps, \\u201cTakeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Climate The Sea of Japan influences the climate of Japan because of its relatively warm waters; evaporation is especially noticeable in winter, when an enormous quantity of water vapour rises in the region between the cold, dry polar air mass and the warm, moist tropical air mass.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Also, the government lodged a protest against the ROK based on a result of the patrol of the adjacent seas of Takeshima on October 28.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Signed in September 1951, the San Francisco Peace Treaty states that Japan recognizes the independence of Korea and renounces \\u201cKorea, including the islands of Quelpart, Port Hamilton and Dagelet.\\u201d A request made by the Republic of Korea to include                                   by the United States on the grounds that           had never been treated as Korean territory and that Korea had at no point claimed sovereignty over Takeshima.\",\n",
      "    \"error_index\": 2,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Reference 2: The name - Sea of Japan (Japan Sea) established by \\\"Limits of Oceans and Seas\\\"\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea) established\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) established by \\\"Limits of Oceans and Seas\\\"\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"In particular, he highlighted that because Lee played a leading role in raising international awareness by testifying in front of the U.S. House of Representatives and leading the effort to have comfort-women related records inscribed on the UNESCO Memory of the World Register, an effort which Japan strongly opposed.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"comfort-women\",\n",
      "    \"predicted_keyword\": \"\",\n",
      "    \"class\": 6,\n",
      "    \"predicted_class\": 0\n",
      "}\n",
      "{\n",
      "    \"tw\": \"South Korea may begin defense drills on disputed              islands this month NATIONAL / POLITICSAUG 5, 2019 South Korea may begin defense drills on disputed Sea of Japan islands this month\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The Plaintiff sought the payment of compensatory damages by the governments, on the account that the losses were the result of the Cabinet\\u2019s or the Prime Minister\\u2019s neglect of obligations to take effective and appropriate measures to protect and restore rights and interests in Takeshima.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The Late-Seventeenth-Century \\\"Takeshima\\\" Dispute, with Reference to the Dajokan Order of 1877\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"But the Liancourt Rocks\\u2014the least combative name for the islets, so-called because a French whaling ship almost ran aground there in 1849\\u2014are just one of many islands that are being fought over by Japan, Korea, and China.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Liancourt Rocks\",\n",
      "    \"predicted_keyword\": \"\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 0\n",
      "}\n",
      "{\n",
      "    \"tw\": \"                                 Other names:                 , Liancourt Islands, Takeshima, Dokdo, Tok Islets Location-of-Liancourt-rocks-en.png Location of the Liancourt Rocks in the              between Japan and South Korea Geography Location of Liancourt Rocks Location\\t             Coordinates\\t37\\u00b014\\u203230\\u2033N 131\\u00b052\\u20320\\u2033E Total islands\\t90 (37 permanent land) Major islands\\tEast Islet, West Islet Area\\t18.745 hectares (46.32 acres) East Islet: 7.33 hectares (18.1 acres) West Islet: 8.864 hectares (21.90 acres) Highest point West Islet 169 metres (554 ft) Administered by South Korea County\\tUlleung County, North Gyeongsang Claimed by Japan Town\\tOkinoshima, Shimane South Korea \\u00b7 North Korea County\\tUlleung(Ull\\u016dng) County, Gyeongsangbuk-do(North Ky\\u014fngsang) Demographics Population\\t50[1] The Liancourt Rocks[a] are a group of small islets in the             .\",\n",
      "    \"error_index\": 5,\n",
      "    \"keyword\": \"Liancourt Islands\",\n",
      "    \"predicted_keyword\": \"Liancourt Islands, Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The                                                 before and after the                                                    in 1905, as is revealed from \\u201cDaehan Jiji [Map of Great Han Empire]\\u201d and \\u201cHanguk Tongsa [Painful History of Korea].\\u201d Additionally, the Japanese government asserted that, \\u201cTakeshima is without a doubt a part of the territory of Japan, in light of historical facts and in view of the requirements for territorial acquisition under international law.\\u201d On September 25, 1954, the ROK government published in newspapers a refutation of the views of the Japanese government.\",\n",
      "    \"error_index\": 2,\n",
      "    \"keyword\": \"Takeshima is without a doubt a part of the territory of Japan\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"\\\"Limits of Oceans and Seas\\\" has consistently used the name - Sea of Japan (Japan Sea) as the name for the concerned sea area since its first edition (1928).\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"has consistently used the name - Sea of Japan (Japan Sea) as the name\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Japan takes the position that as the issue of the attribution of           was not settled at JapanROK meetings and the Exchange of Notes does not stipulate the exclusion of the Takeshima dispute from its coverage, the Takeshima dispute, therefore, should be settled in accordance with the said Exchange of Notes.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Similarly, at a meeting of the House of Representatives\\u2019 Special Committee on the Japan-ROK Treaty and Other Matters on October 27, 1965, a question was asked as to \\u201cwhether, as stated in the Korean National Assembly, Foreign Minister Shiina and Prime Minister Sato expressed understanding that the Takeshima dispute could not be covered in the Exchange of Notes Concerning the Settlement of Disputes.\\u201d Prime Minister Sato answered: I would like to make it clear here that that is not the case.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"And then Kim Hak-sun came forward, sharing her own story of being imprisoned as a \\\"comfort woman\\\" by the Japanese in China - the first South Korean victim to break her silence so publicly.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"comfort woman\",\n",
      "    \"predicted_keyword\": \"comfort woman\",\n",
      "    \"class\": 6,\n",
      "    \"predicted_class\": 6\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Further, as stated under item4 the UN has already officially confirmed its policy requiring the use of Sea of Japan as the standard geographical term in all official UN publications.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"officially confirmed its policy requiring the use of Sea of Japan as the standard geographical term in all official UN publications\",\n",
      "    \"predicted_keyword\": \"Further, as stated under item4 the UN has already officially confirmed its policy requiring the use of Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"In the process of drafting the San Francisco Peace Treaty (signed on September 8, 1951, effective as of April 28, 1952), which included the ultimate disposition of Japanese territory after World War II, the Republic of Korea requested that the United States add Takeshima to the territories to be renounced by Japan.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The council also supported The House of Sharing, a Seoul-based shelter for former comfort women that was also set up in 1992.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"comfort women\",\n",
      "    \"predicted_keyword\": \"comfort women \",\n",
      "    \"class\": 6,\n",
      "    \"predicted_class\": 6\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The current situation in Takeshima 3.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Yet in the same breath, Lee said Japan\\u2019s \\u201crecent words and actions go against the spirit of the sense of responsibility, apology and self-reflection\\u201d it had previously shown on the comfort women issue.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"comfort women\",\n",
      "    \"predicted_keyword\": \"comfort women \",\n",
      "    \"class\": 6,\n",
      "    \"predicted_class\": 6\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Shortly after, the Republic of Korea stationed marine police personnel on Takeshima and has continued to occupy the islands illegally up to the present day.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima and has continued to occupy the islands illegally\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The Liancourt Rocks are situated at a distance of 211 kilometres (114 nmi) from the main island of Japan (Honshu) and 216.8 kilometres (117.1 nmi) from mainland South Korea.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Liancourt Rocks\",\n",
      "    \"predicted_keyword\": \"The Liancourt Rocks \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Sea of Japan\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The Sea of Japan is a marginal sea set off from the Pacific Ocean by the Japanese Archipelago, the Korean Peninsula, the Sakhalin Islands, and Russia.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Since then, Takeshima was under the valid control of Japanese people until shortly before the outbreak of the recent war.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima was under the valid control of Japanese\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"(2) d.) , what is notable is that the report states that \\\"there was a rapid increase in the use of the name Sea of Japan from the 19th century (1830 onward)\\\" (italics added).\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"was a rapid increase in the use of the name Sea of Japan\",\n",
      "    \"predicted_keyword\": \"\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The installation of comfort women statues outside South Korea, in addition to those placed near Japanese diplomatic facilities in Seoul and Busan, has been a source of tension between the two countries.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"comfort women\",\n",
      "    \"predicted_keyword\": \"comfort women \",\n",
      "    \"class\": 6,\n",
      "    \"predicted_class\": 6\n",
      "}\n",
      "{\n",
      "    \"tw\": \"| KYODO A statue symbolizing \\\"comfort women\\\" is unveiled in Berlin in September.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"comfort women\",\n",
      "    \"predicted_keyword\": \"comfort women\",\n",
      "    \"class\": 6,\n",
      "    \"predicted_class\": 6\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Signed in September 1951, the San Francisco Peace Treaty states that Japan recognizes the independence of Korea and renounces \\u201cKorea, including the islands of Quelpart, Port Hamilton and Dagelet.\\u201d A request made by the Republic of Korea to include Takeshima was explicitly rejected by the United States on the grounds that Takeshima had never been treated as Korean territory and that Korea had at no point claimed sovereignty over Takeshima.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima was explicitly rejected\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The court noted that even if the ROK had rejected Japanese administrative authority over Takeshima, the area for which the Plaintiff possesses mining rights and to which the said tax applies, this in and of itself gives no reasons for the extinguishment of the right of taxation, which is a function of exercise of administration by the Japanese government over the Plaintiff, a Japanese national.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"In this context, the San Francisco Peace Treaty lists \\u201cKorea, including the islands of Quelpart, Port Hamilton and Dagelet\\u201d as territories that Japan must renounce, while intentionally excluding Takeshima.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Even though the name \\\"East Sea\\\" is in use today in the ROK, it is obvious that the name \\\"East Sea\\\" is nothing but a local name used only in the ROK,and that the name Sea of Japan is the only name that has been in wide use internationally for a long period of time.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"the name Sea of Japan is the only name that has been in wide use internationally for a long period of time.\",\n",
      "    \"predicted_keyword\": \"\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 0\n",
      "}\n",
      "{\n",
      "    \"tw\": \"It adds that the Republic of Korea has been occupying Takeshima with \\u201cno basis in international law,\\u201d and that it will continue to seek the settlement \\u201cin a calm and peaceful manner\\u201d.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Republic of Korea has been occupying Takeshima with \\u201cno basis in international law,\\u201d\",\n",
      "    \"predicted_keyword\": \"Republic of Korea has been occupying Takeshima with \\u201cno basis in international law\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Sovereignty dispute Main article: Liancourt Rocks dispute\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Liancourt Rocks\",\n",
      "    \"predicted_keyword\": \"Liancourt Rocks \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The above description reveals that at the time, the              , on its way to Utsuryo Island, commonly made a port call at Takeshima which was along the route, and that Takeshima belongs to Japanese territory although it is \\u201clocated at the far end of Japan\\u2019s western sea.\\u201d With regard to old maps, \\u201cTakeshima no Ezu [Diagrams of Takeshima]\\u201d, made around 1696 by Ihei Kotani, a government official in Tottori domain, provides an accurate illustration of Takeshima\\u2019s Higashijima and Nishijima Islands and associated reefs.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Takeshima Facts & Figures Takeshima\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima Facts & Figures Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The maps are unique in that Matsushima (present          ), which is closer to Okinoshima Island, is distinguished from Takeshima (present Utsuryo Island), which is closer to Korea.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"International Symposium in Korea on the Takeshima Dispute\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The                                                 before and after the incorporation of Takeshima into Shimane Prefecture in 1905, as is revealed from \\u201cDaehan Jiji [Map of Great Han Empire]\\u201d and \\u201cHanguk Tongsa [Painful History of Korea].\\u201d Additionally, the Japanese government asserted that, \\u201cTakeshima is without a doubt a part of the territory of Japan, in light of historical facts and in view of the requirements for territorial acquisition under international law.\\u201d On September 25, 1954, the ROK government published in newspapers a refutation of the views of the Japanese government.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"incorporation of Takeshima into Shimane Prefecture\",\n",
      "    \"predicted_keyword\": \"Takeshima into Shimane Prefecture \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Women at comfort stations were forced to render sexual services to many officers and men, their human dignity trampled upon.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Women at comfort stations\",\n",
      "    \"predicted_keyword\": \"\",\n",
      "    \"class\": 6,\n",
      "    \"predicted_class\": 0\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Shimane Prefecture registered Takeshima in the land-book as government owned land according to this request.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"This trend resulted from the widespread adoption of the term \\\"Sea of Japan\\\" in Western maps drawn from the end of the 18th century onward, which were used as sources for the former maps.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"In 1905, when the islets that were known as \\u201cMatsushima\\u201d since ancient times were incorporated into Japanese territory, the Meiji government formally named the islets \\u201c         \\u201d based on the following written response from the administrator of Okinoshima Island: I deem Takeshima is an appropriate name.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"RELATED STORIES Former South Korean 'comfort women' support group hit by criticism and probe Japan warns new 'comfort women' statue may hurt ties with South Korea \\u2019History on trial\\u2019: Why Japan's wartime labor dispute is more than another tit-for-tat with South Korea Lee Yong-soo, one of those prominent among the surviving comfort women, whose numbers have dwindled to just 17 now, urged survivors not to lose heart over the scandal.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"comfort women\",\n",
      "    \"predicted_keyword\": \"comfort women\",\n",
      "    \"class\": 6,\n",
      "    \"predicted_class\": 6\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Inflow of water takes place primarily through the eastern and western channels of the Korea Strait; the inflow of water into the Sea of Japan through the narrow and shallow Tatar Strait is negligible, while through the Tsugaru and La Perouse straits the water flows out of the Sea of Japan.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 0\n",
      "}\n",
      "{\n",
      "    \"tw\": \"\\u2026 The distance from Hakushu\\u2019s Yonago to Takeshima is about 160 ri by sea.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Republic of Korea's assertion (1) The republic of Korea asserts that the name \\\"Usan(do)\\\" appearing in ancient Korean documents and maps corresponds with Dokdo (present-day          ) and that Takeshima has historically been part of its territory.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"During World War II, the Japanese military even set up a system for sex slavery: Tens of thousands of \\u201ccomfort women\\u201d in Asia were forced into prostitution at military brothels.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"comfort women\",\n",
      "    \"predicted_keyword\": \"comfort women\",\n",
      "    \"class\": 6,\n",
      "    \"predicted_class\": 6\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Because of the very fact that the Sea of Japan was \\\"discovered\\\" through such awareness of geographical features, conditions were ripe to give a name to the closed sea.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The large scale imprisonment and rape of thousands of women, who were euphemistically called \\\"comfort women\\\" by the Japanese military, first seized public attention in 1991 when three Korean women filed suit in a Toyko District Court stating that they had been forced into sexual servitude and demanding compensation.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"comfort women\",\n",
      "    \"predicted_keyword\": \"comfort women\",\n",
      "    \"class\": 6,\n",
      "    \"predicted_class\": 6\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The group has also sought to denigrate the Japanese consulate by building the comfort woman statue in front of it.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"comfort woman\",\n",
      "    \"predicted_keyword\": \"comfort woman \",\n",
      "    \"class\": 6,\n",
      "    \"predicted_class\": 6\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The \\u201c          Zusetsu [Explanation of Takeshima with Maps]\\u201d edited by Tsuan Kitazono during the Horeki Period (1751-1763) contains the following description: There is an island about 40 ri north of the west island (Nishijima) of Matsushima (Takeshima) in 3 Oki county.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Further surveys into the Sea of Japan (Japan Sea) were conducted by European cartographers, explorers and navigators succeedingly.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea)\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"World Cup>Past Tournaments>2002 Japan-Korea>Overview\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"2002 Japan-Korea\",\n",
      "    \"predicted_keyword\": \"\",\n",
      "    \"class\": 1,\n",
      "    \"predicted_class\": 0\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Against this background, the Cabinet decided to incorporate Takeshima into Shimane Prefecture and reaffirmed its sovereignty over Takeshima.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima into Shimane Prefecture \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"On hearing this news, the people of Okinoshima Island began to hunt sea lions in large numbers in Takeshima from 1903, putting Takeshima sea lions at risk of extinction.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"                                 Other names: Liancourt Islets, Liancourt Islands, Takeshima, Dokdo, Tok Islets Location-of-Liancourt-rocks-en.png Location of the Liancourt Rocks in the              between Japan and South Korea Geography Location of Liancourt Rocks Location\\t             Coordinates\\t37\\u00b014\\u203230\\u2033N 131\\u00b052\\u20320\\u2033E Total islands\\t90 (37 permanent land) Major islands\\tEast Islet, West Islet Area\\t18.745 hectares (46.32 acres) East Islet: 7.33 hectares (18.1 acres) West Islet: 8.864 hectares (21.90 acres) Highest point West Islet 169 metres (554 ft) Administered by South Korea County\\tUlleung County, North Gyeongsang Claimed by Japan Town\\tOkinoshima, Shimane South Korea \\u00b7 North Korea County\\tUlleung(Ull\\u016dng) County, Gyeongsangbuk-do(North Ky\\u014fngsang) Demographics Population\\t50[1] The Liancourt Rocks[a] are a group of small islets in the             .\",\n",
      "    \"error_index\": 4,\n",
      "    \"keyword\": \"Liancourt Islets\",\n",
      "    \"predicted_keyword\": \"Liancourt Isle\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Domestic court rulings The following are two examples of cases in which the Takeshima dispute was raised before a Japanese court.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"To date, the Japanese government has neither admitted responsibility for creating the comfort station system nor given compensation directly to former comfort women.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"comfort women\",\n",
      "    \"predicted_keyword\": \"comfort women\",\n",
      "    \"class\": 6,\n",
      "    \"predicted_class\": 6\n",
      "}\n",
      "{\n",
      "    \"tw\": \"A group of South Korean lawmakers on Saturday visited the disputed Takeshima islets in the             , adding fuel to ongoing spats over trade and historical issues between Tokyo and Seoul.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Takeshima islets\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"With regard to          , the Japanese government asserted: In the Proclamation, the Republic of Korea appears to assert territorial rights over the islets in the              known as Takeshima.\",\n",
      "    \"error_index\": 2,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"\\u2193 Japan's refutation (2) Even if for argument's sake \\\"Seokdo\\\" corresponds to \\\"Dokdo,\\\" the                                                                                                  , and hence territorial rights over Takeshima by Korea were not established.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Takeshima by Korea were not established\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"In accordance with this request, the government, after hearing the opinions of the Shimane prefectural government, decided on a Cabinet meeting on January 28, 1905, that it would name the islands enumerated below as \\u201cTakeshima,\\u201d incorporate them into Japan, and place the islands under the jurisdiction of the administrator of Okinoshima Island of Shimane Prefecture.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"It is about 40 ri from Matsushima to Takeshima.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Since then, the government of the ROK and National Assembly members have also landed on Takeshima.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Thus the treaty, which established the international order post-World War II, affirms that Takeshima is Japanese territory.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima is Japanese territory\",\n",
      "    \"predicted_keyword\": \"Takeshima is Japanese territory\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"In response, the ROK asserted as follows in a note verbale dated February 12 of the same year: The Government of the Republic of Korea merely wishes to remind the Japanese Government that SCAP, by SCAPIN No.677 dated January 29, 1946, explicitly excluded the islets (Takeshima) from the territorial possessions of Japan and that again the same islets have been left outside of the MacArthur Line, facts that endorse and confirm the Korean claim to them, which is beyond any dispute.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The plight of comfort women has been a thorny issue between Seoul and Tokyo for decades and the activist group had campaigned for compensation from Japan.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"comfort women\",\n",
      "    \"predicted_keyword\": \"comfort women \",\n",
      "    \"class\": 6,\n",
      "    \"predicted_class\": 6\n",
      "}\n",
      "{\n",
      "    \"tw\": \"By the beginning of 19th Century, the name - Sea of Japan (Japan Sea) became established internationally as the name indicating this sea area.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea) became established internationally as the name\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"By doing so, the Japanese government reaffirmed its sovereignty over Takeshima.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Japanese government reaffirmed its sovereignty over Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"A South Korean police boat approaches the dock on Liancourt Rocks' East Islet.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Liancourt Rocks\",\n",
      "    \"predicted_keyword\": \"Liancourt Rocks\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Japan classifies the islands as part of Okinoshima, Oki District, Shimane Prefecture, and calls them Takeshima (\\u7af9\\u5cf6, \\\"bamboo island[s]\\\").\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Consequently, the ROK's assertion that the name Sea of Japan became widespread as a result of \\\"expansionism and colonial rule\\\" in the latter half of the 19th century is wholly invalid.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"the ROK's assertion that the name Sea of Japan became widespread as a result of \\\"expansionism and colonial rule\\\"\",\n",
      "    \"predicted_keyword\": \"Consequently, the ROK's assertion that the name Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"If Japan had any intention to actively propagate the name - Sea of Japan (Japan Sea) worldwide, it would not have had any concern about the political and diplomatic problems regarding the names and limits of seas as such, nor objected the proposal to prepare the guidelines, even temporarily.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea) worldwide\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"                From Wikipedia, the free encyclopedia Jump to navigationJump to search \\\"Dokdo\\\" and \\\"Takeshima\\\" redirect here.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Nakai requested the government to incorporate Takeshima into Japanese territory and to grant him a ten-year lease of the islets.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima into Japanese territory\",\n",
      "    \"predicted_keyword\": \"Takeshima into Japanese territory \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"This clearly shows the fallacy of the ROK's assertion that the name Sea of Japan was the result of the Japanese policy of expansionism and colonial rule (item 2.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"This clearly shows the fallacy of the ROK's assertion that the name Sea of Japan\",\n",
      "    \"predicted_keyword\": \"This clearly shows the fallacy of the ROK's assertion that the name Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Construction South Korea has carried out construction work on the Liancourt Rocks, by 2009 the islands had a lighthouse and helicopter pad,[30] and a police barracks.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Liancourt Rocks\",\n",
      "    \"predicted_keyword\": \"Liancourt Rocks\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"If \\\"East Sea\\\" were used alongside the name - Sea of Japan (Japan Sea) in \\\"Limits of Oceans and Seas\\\" which is the guideline regarding names on sea, it would surely lead to confusion among navigators.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"If \\\"East Sea\\\" were used alongside the name - Sea of Japan (Japan Sea) in \\\"Limits of Oceans and Seas\\\" which is the guideline regarding names on sea, it would surely lead to confusion\",\n",
      "    \"predicted_keyword\": \"If \\\"East Sea\\\" were used alongside the name - Sea of Japan (Japan Sea) in \\\"Limits of Oceans and Seas\\\" \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"(copy) \\u25b2Japan Coast Guard patrol vessel fired at near Takeshima by the Republic of Korea in July 1953.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima by the Republic of Korea \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Signed in September 1951, the San Francisco Peace Treaty states that Japan recognizes the independence of Korea and renounces \\u201cKorea, including the islands of Quelpart, Port Hamilton and Dagelet.\\u201d A request made by the Republic of Korea to include                                   by the United States on the grounds that Takeshima had never been treated as Korean territory and that Korea had at no point claimed sovereignty over Takeshima.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"BY JESSE JOHNSON Tokyo, calling the drills \\u201cunacceptable,\\u201d makes a protest about South Korean military exercises Seoul says are intended to \\u201cdefend\\u201d a cluster of rocky islands in the Sea of Japan.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 0\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Additionally, the \\u201cChosei           Ki [The Account of Longevity          ]\\u201d authored by Takamasa Yada in 1801 states: From Oki-Dogo, Matsushima (Takeshima) is located off the west-southwestern coast.\",\n",
      "    \"error_index\": 2,\n",
      "    \"keyword\": \"Takamasa\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"In view of the difficult situations to settle the           dispute through negotiations with the ROK, the Japanese government issued the following note verbale on September 12, 1954, proposing that the Takeshima dispute be settled at court by referring it to the International Court of Justice (ICJ).\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"A group of South Korean lawmakers on Saturday visited the disputed Takeshima islets in the Sea of Japan, adding fuel to ongoing spats over trade and historical issues between Tokyo and Seoul.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Seven South Korean lawmakers will visit the Takeshima islands in the Sea of Japan on Aug. 31, one of the lawmakers said Friday, a trip that could further ratchet up tensions between Tokyo and Seoul.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Some still allege the women were not forced to work in the stations.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Some still allege the women were not forced to work in the stations\",\n",
      "    \"predicted_keyword\": \"\",\n",
      "    \"class\": 4,\n",
      "    \"predicted_class\": 0\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The above description reveals that at the time, the Takeshimamaru, on its way to Utsuryo Island, commonly made a port call at Takeshima which was along the route, and that Takeshima belongs to Japanese territory although it is \\u201clocated at the far end of Japan\\u2019s western sea.\\u201d With regard to old maps, \\u201cTakeshima no Ezu [Diagrams of Takeshima]\\u201d, made around 1696 by Ihei Kotani, a government official in Tottori domain, provides an accurate illustration of Takeshima\\u2019s Higashijima and Nishijima Islands and associated reefs.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshimamaru\",\n",
      "    \"predicted_keyword\": \"Takeshimamaru\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"A statue symbolizing the issue of so-called               forced to work in Japanese wartime brothels is unveiled in Berlin on September 28, 2020 The Japanese government has said it \\\"regrets\\\" the unveiling of statue of a Korean \\\"comfort woman\\\" in Berlin on Monday.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"comfort woman\",\n",
      "    \"predicted_keyword\": \"comfort woman\",\n",
      "    \"class\": 6,\n",
      "    \"predicted_class\": 6\n",
      "}\n",
      "{\n",
      "    \"tw\": \"\\u201cJapan has officially named \\u2018Sea of Japan\\u2019 since the late 19th century, in the days of Meiji Restoration, but before that, \\u2018Sea of Joseon,\\u2019 which means the \\u2018Sea of Korea,\\u2019 was also mentioned on many Japanese maps,\\u201d Yuji said, explaining that the country\\u2019s imperialism had affected the decision to use the \\u201cSea of Japan\\u201d name in S-23 during the IHO\\u2019s conference in 1929.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"The missionary spread the term \\\"Sea of Japan\\\" to Europe with a new awareness of geographical features.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"The missionary spread the term \\\"Sea of Japan\\\" to Europe with a new awareness of geographical features\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"In going to Matsushima to hunt, one stops there as it is on the way to Takeshima.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"South Korean military flies planes near disputed islands in Sea of Japan NATIONAL / POLITICSOCT 1, 2019 South Korean military flies planes near disputed islands in Sea of Japan\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"A statue symbolizing the issue of so-called comfort women forced to work in Japanese wartime brothels is unveiled in Berlin on September 28, 2020 The Japanese government has said it \\\"regrets\\\" the unveiling of statue of a Korean \\\"comfort woman\\\" in Berlin on Monday.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"comfort women\",\n",
      "    \"predicted_keyword\": \"comfort women \",\n",
      "    \"class\": 6,\n",
      "    \"predicted_class\": 6\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Current situation in Takeshima On February 14, 1983, House of Councillors member Yutaka Hata submitted written questions regarding the current situation in Takeshima to the President of the House of Councillors Masatoshi Tokunaga, as outlined below.\",\n",
      "    \"error_index\": 0,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Berlin allows '             ' statue to remain for time being A statue symbolizing 'comfort women' is unveiled in Berlin in September.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"comfort women\",\n",
      "    \"predicted_keyword\": \"comfort women\",\n",
      "    \"class\": 6,\n",
      "    \"predicted_class\": 6\n",
      "}\n",
      "{\n",
      "    \"tw\": \"Document title\\t                                (Asahi Shimbun) Date created (Western calendar)\\tSeptember 25, 1959 Date created (Japanese era)\\tSeptember 25, Showa 34 Author(s) / Editor(s) Publisher\\tAsahi Shimbunsha Name of publication\\tAsahi Shimbun, Evening Edition (reduced-size edition) Content\\tAn article reporting that the Ministry of Foreign Affairs had responded to a protest from South Korea over the fact that a patrol boat of the Japan Coast Guard (then called the Maritime Safety Agency) had approached Takeshima, by handing its own written protest to the South Korean delegation in Japan.\",\n",
      "    \"error_index\": 1,\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "for i in range(0, len(test_dataset)):\n",
    "    if test_dataset[i]['sentiment'] !=0 and test_dataset[i]['sentiment'] != 0:\n",
    "        temp = {\n",
    "            \"tw\": test_dataset[i]['orig_tweet'][2],\n",
    "            \"error_index\": int(test_dataset[i]['error_index'][0]),\n",
    "            \"keyword\": test_dataset[i]['orig_selected'],\n",
    "            \"predicted_keyword\": (final_output[i][2] if final_output[i][3] != 0 else \"\"),\n",
    "            \"class\": test_dataset[i]['sentiment'],\n",
    "            \"predicted_class\": final_output[i][3]\n",
    "        }\n",
    "        print(json.dumps(temp, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents =  api.getDocumentList(status='collected')\n",
    "document_no = documents[0]['no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S\n",
      "2 2 Sea of Japan\n",
      "{'created_error_no': 849}\n",
      "S\n",
      "8 2 Sea of Japan\n",
      "{'created_error_no': 850}\n",
      "S\n",
      "22 2 Gulf Peter the\n",
      "{'created_error_no': 851}\n",
      "S\n",
      "24 2 Peter the Great is a \"historic bay\n",
      "{'created_error_no': 852}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response [201]>"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = api.getDocument(document_no)\n",
    "sentences = split_document(document['document']['contents'], [])\n",
    "for sentence in sentences:\n",
    "    sentence_no = getInformationFromSentence(sentence)['sentence_no'] - 2 # padding\n",
    "    main_text = getInformationFromSentence(sentence)['text'][2]\n",
    "    while True:\n",
    "        sentence[0][2] = main_text\n",
    "        newdataset = TextDataset([sentence])\n",
    "        d = newdataset[0]\n",
    "        with torch.no_grad():\n",
    "            ids = d[\"ids\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            sentiment = d[\"sentiment\"]\n",
    "            orig_selected = d[\"orig_selected\"]\n",
    "            orig_tweet = d[\"orig_tweet\"][2]\n",
    "            targets_start = d[\"targets_start\"]\n",
    "            targets_end = d[\"targets_end\"]\n",
    "            offsets = d[\"offsets\"].numpy()\n",
    "            error_index = d[\"error_index\"]\n",
    "\n",
    "            ids = ids.to(device, dtype=torch.long).view(1,-1)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long).view(1,-1)\n",
    "            mask = mask.to(device, dtype=torch.long).view(1,-1)\n",
    "            error_index = error_index.to(device, dtype=torch.long).view(1,-1)\n",
    "            # Predict start and end logits for each of the five models\n",
    "            outputs_start, outputs_end, outputs_class = model1(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                error_index=error_index\n",
    "            )\n",
    "            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "            outputs_class = [torch.softmax(i, dim=1).cpu().detach().numpy() for i in outputs_class]\n",
    "            ont_hot_class = [np.argmax(i) for i in outputs_class]\n",
    "            class_number = ont_hot_class.index(max(ont_hot_class))\n",
    "\n",
    "            _, output_sentence = calculate_jaccard_score(\n",
    "            original_tweet=orig_tweet,\n",
    "            target_string=\"new keyword\",\n",
    "            sentiment_val=999999,\n",
    "            idx_start=np.argmax(outputs_start),\n",
    "            idx_end=np.argmax(outputs_end),\n",
    "            offsets=offsets,\n",
    "            class_number = class_number)\n",
    "            if (class_number != 0):\n",
    "                output_sentence = output_sentence.strip()\n",
    "                position = main_text.find(output_sentence)\n",
    "                length = len(output_sentence)\n",
    "                print(\"S\")\n",
    "                if (position == -1 or length == 0): break\n",
    "                print(getInformationFromSentence(sentence)['sentence_no'], class_number, output_sentence)\n",
    "                result = api.AddDocumentError(document_no, sentence_no, class_list[class_number], position, length, output_sentence)  \n",
    "                print(result.json())\n",
    "                main_text = main_text[:position] + (' ' * length) + main_text[position+length:]\n",
    "            else:\n",
    "                break\n",
    "api.updateDocument(document_no, 'labeled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
