{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 승화 문서에서의 표기 오류 검출\n",
    "\n",
    "BERT(Bidirectional Encoder Representations from Transformers)는 구글이 개발한 사전훈련된(pre-training) 모델입니다. 이 모델은 위키피디아같은 텍스트 코퍼스(말뭉치)를 사용하여 미리 학습되었다는 특징이 있습니다. 그리고 BERT의 특성으로 단어를 학습할 때 문맥을 함께 고려하기때문에 언어의 패턴을 이해한 모델이 만들어집니다.\n",
    "\n",
    "이를 기반으로 새로운 문제에 적용하는 전이학습(transfer learning)을 수행할 수 있습니다. 미리 학습된 모델을 사용하기 때문에 적은 데이터로도 빠르게 학습이 가능하다는 이점이 있습니다.\n",
    "\n",
    "따라서 해당 모델을 기반으로 문서에서의 잘못된 표기 오류를 검출하는 알고리즘을 개발하였습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목표\n",
    "\n",
    "웹사이트나 텍스트 문서는 긴 여러개의 문장으로 이루어져있습니다. 각 문장이 잘못되었는지를 검사하고 잘못된 경우 잘못된 표현이 어디에 있는지 정확한 위치를 예측하여 알려주는 것이 저희 모델의 최종 목표입니다.\n",
    "\n",
    "따라서 저희는 BERT 모델에 linear regression를 적용한 네트워크 모델을 사용할 예정입니다. Output의 [CLS] 토큰을 통해 문장의 표기 오류를 분류하고 linear regression을 통해 그 위치를 예측할 것입니다.\n",
    "\n",
    "또한 표기 오류가 있지만 문제가 되지 않는 경우가 있습니다.\n",
    "\n",
    "<pre>\n",
    "한국 옆에는 작은 섬이 있는데 이것은 다케시마라고 불리기도 한다. \n",
    "그러나 이것은 잘못된 것으로 올바른 표기는 독도이다.\n",
    "</pre>\n",
    "\n",
    "문장 단위로 표기 오류를 검출한다면 인공지능은 표기 오류 결과로 \"다케시마\"를 지목할 것입니다.\n",
    "그러나 전체적인 문맥을 보면 해당 문장은 잘못된 사례를 이야기 해줄 뿐, 오류가 있는 문장이라고 말을 할수는 없습니다.\n",
    "\n",
    "따라서 BERT 모델에 주변 문장을 함께 학습시키는 모델을 구상하였습니다.\n",
    "**학습 예시**\n",
    "<pre>\n",
    "[index-2] None\n",
    "[index-1] None\n",
    "[판단할 문장] 한국 옆에는 작은 섬이 있는데 이것은 다케시마라고 불리기도 한다. \n",
    "[index+1] 그러나 이것은 잘못된 것으로 올바른 표기는 독도이다.\n",
    "[index+2]\n",
    "</pre>\n",
    "\n",
    "이 알고리즘을 이용해 sentence window를 슬라이딩 시키며 학습을 진행하면 문맥을 함께 고려하는 모델을 만들 수 있을 것이라 생각하였습니다.\n",
    "표기 오류는 오직 [판단할 문장]에 있는지만 체크하도록 학습을 시킬 것으로, 주변 문장의 표기 오류를 검출하여 모델이 혼잡해지는 경우를 최소화하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기\n",
    "API를 통해 서버로부터 사용 가능한 학습 데이터를 불러옵니다. \n",
    "\n",
    "API는 [ {no, contents, errors[code,keyword] } , ...] 형태로 데이터를 보내주도록 되어있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "class APIDokdo:\n",
    "    def __init__(self, apikey):\n",
    "      self.apiurl = \"https://api.easylab.kr\"\n",
    "      self.headers =  {'authorization': apikey}\n",
    "    def getTrainingData(self):\n",
    "        return requests.get(self.apiurl + \"/deeplearning/data/sentences\", headers=self.headers).json()['list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불러온 문서의 개수:  11\n"
     ]
    }
   ],
   "source": [
    "original_data = APIDokdo(\"godapikey12\").getTrainingData()\n",
    "print(\"불러온 문서의 개수: \", len(original_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리\n",
    "불러온 데이터를 문장 단위로 분리를 하고, 표기 오류를 검색합니다. 또한 문맥을 고려할 수 있게 5개의 문장씩 관리합니다 (0-5, 1-6, 2-7, 3-8 ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error 2002 Japan-Korea\n",
      "Error Seas of Japan\n",
      "Error Seas of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea Of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan (Japan Sea)\n",
      "Error Sea of Japan (Japan Sea)\n",
      "Error Sea of Japan (Japan Sea) is the only internationally established name\n",
      "Error Sea of Japan (Japan Sea) became recognized internationally by the early 19th Century\n",
      "Error Sea of Japan (Japan Sea)\n",
      "Error use the name - Sea of Japan (Japan Sea) for making their nautical chart\n",
      "Error Sea of Japan (Japan Sea) is established in the guidelines on names of seas, published by the International Hydrographic Organization (IHO)\n",
      "Error Sea of Japan (Japan Sea) established by \"Limits of Oceans and Seas\"\n",
      "Error IHO has consistently used Sea of Japan (Japan Sea)\n",
      "Error If \"East Sea\" were used alongside the name - Sea of Japan (Japan Sea) in \"Limits of Oceans and Seas\" which is the guideline regarding names on sea, it would surely lead to confusion\n",
      "Error Sea of Japan (Japan Sea) has been the only internationally established name\n",
      "Error Sea of Japan (Japan Sea)\n",
      "Error The establishment of the name - Sea of Japan (Japan Sea)\n",
      "Error Sea of Japan (Japan Sea)\n",
      "Error Sea of Japan (Japan Sea) was an unknown sea area, and the shape of the Sea of Japan (Japan Sea)\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan (Japan Sea), Krusenstern described, based on an accurate map, that \"People also call this sea area the Sea of Korea, but because only a small part of this sea touches the Korean coast, it is appropriate to name it \"the Sea of Japan\"\n",
      "Error Sea of Japan (Japan Sea)\n",
      "Error Sea of Japan (Japan Sea) became established internationally as the name\n",
      "Error Sea of Japan (Japan Sea)) have solely used the Sea of Japan (Japan Sea)\n",
      "Error Sea of Japan had already been internationally recognized and firmly established by the early 19th\n",
      "Error Sea of Japan (Japan Sea) established\n",
      "Error has consistently used the name - Sea of Japan (Japan Sea) as the name\n",
      "Error Sea of Japan (Japan Sea) was already established internationally as the sole name indicating the Sea of Japan (Japan Sea)\n",
      "Error Sea of Japan (Japan Sea)\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan (Japan Sea) worldwide\n",
      "Error Sea of Japan (Japan Sea) was used solely in the guideline\n",
      "Error Sea of Japan (Japan Sea) was used solely in the guideline\n",
      "Error Sea of Japan (Japan Sea) was used solely in the guideline\n",
      "Error The name Sea of Japan (Japan Sea) was described solely in the draft\n",
      "Error ROK did not make an objection against the name - Sea of Japan (Japan Sea).\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan (Japan Sea) has been consistently and solely used in \"Limits of Oceans and Seas\"\n",
      "Error ROK used the name of Sea of Japan (Japan Sea)\n",
      "Error there is the historic fact that the ROK did not make an objection against the name - Sea of Japan (Japan Sea) even after World War II.\n",
      "Error Sea of Japan\n",
      "Error suddenly begun to challenge the sole use of the name Sea of Japan\n",
      "Error confirms that the name Sea of Japan was already prevalent at the early 19th century\n",
      "Error Sea of Japan\n",
      "Error the ROK's assertion that the name Sea of Japan became widespread as a result of \"expansionism and colonial rule\"\n",
      "Error the name Sea of Japan is the only name that has been in wide use internationally for a long period of time.\n",
      "Error seas such as with the Sea of Japan\n",
      "Error officially confirmed its policy requiring the use of Sea of Japan as the standard geographical term in all official UN publications\n",
      "Error Sea of Japan\n",
      "Error was a rapid increase in the use of the name Sea of Japan\n",
      "Error This clearly shows the fallacy of the ROK's assertion that the name Sea of Japan\n",
      "Error can be interpreted as affirming that the name Sea of Japan was in widespread use well\n",
      "Error Japan Sea\n",
      "Error Japan Sea\n",
      "Error “Japan Sea” is the only internationally established name\n",
      "Error Japan Sea\n",
      "Error Japan Sea\n",
      "Error Sea of Japan\n",
      "Error In the map Kunyu Wanguo Quantu drawn up by an Italian missionary priest of the Jesuit Order Matteo Ricci 400 years ago, the \"Sea of Japan\" was written down in kanji already\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error The missionary spread the term \"Sea of Japan\" to Europe with a new awareness of geographical features\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error [Takeshima] Island map showing distances to Dokdo TakeshimaDokdo Island (also called Liancourt Rocks by some nations and Takeshima by Japan)\n",
      "Error Takeshima\n",
      "Error Takeshima\n",
      "Error Takeshima\n",
      "Error Takeshima\n",
      "Error Takeshima\n",
      "Error Takeshima Day\n",
      "Error Takeshima\n",
      "Error Takeshima Day\n",
      "Error The Sea of Japan is the only name that has been established internationally, and there is no need or reason for changing it\n",
      "Error Japan Sea\n",
      "Error Sea of Japan\n",
      "Error East Sea\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan\n",
      "Error Sea of Japan This is due to the particular characteristics of the Sea of Japan\n",
      "Error Sea of Japan\n"
     ]
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "training_data = []\n",
    "for i in original_data:\n",
    "    sentences = []\n",
    "    # text = i['contents'].replace(\"\\r\",\"\").replace(\"\\n\",\"\")\n",
    "    #sentences = [i.strip() for i in tokenize.sent_tokenize(text)] #문장 단위로 분리 및 문장 앞뒤 공백 제거\n",
    "    sentences = [i.strip() for i in i['contents'].split('\\n')]\n",
    "    \n",
    "    # 두줄 이상 공백이 있는 경우 제거\n",
    "    last = \"\"\n",
    "    remove_indexes = []\n",
    "    for j in range(0, len(sentences)):\n",
    "        if last == \"\" and sentences[j] == \"\":\n",
    "            remove_indexes.append(j)\n",
    "        last = sentences[j]\n",
    "        \n",
    "    for index in sorted(remove_indexes, reverse=True):\n",
    "        del sentences[index]\n",
    "    \n",
    "    # 문장별로 표기 오류 키워드 검색 && 2 + 1 + 2 문장 단위로 자동 구성\n",
    "    # padding\n",
    "    sentences = ['', ''] + sentences + ['', '']\n",
    "    for index in range(2, len(sentences)):\n",
    "        # 빈 문장 제거\n",
    "        if (len(sentences[index]) == 0): continue\n",
    "            \n",
    "        y_class = 11510\n",
    "        y_keyword = \"\"\n",
    "        # 현재 문장에 표기 오류가 있는지 확인\n",
    "        for error in i['errors']:\n",
    "            if (error['sentence_no'] != index - 2):\n",
    "                continue\n",
    "            print(\"Error\", error['keyword'])\n",
    "            if sentences[index].find(error['keyword']) != -1:\n",
    "                y_class = error['code']\n",
    "                y_keyword = error['keyword']\n",
    "        \n",
    "        training_data.append([[sentences[j] for j in range(index-2, index+2 + 1)], y_class, y_keyword])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['',\n",
       "   '',\n",
       "   'World Cup>Past Tournaments>2002 Japan-Korea>Overview',\n",
       "   '',\n",
       "   'The 2002 world cup was held in South Korea and Japan and certainly did not disappoint.'],\n",
       "  100,\n",
       "  '2002 Japan-Korea'],\n",
       " [['World Cup>Past Tournaments>2002 Japan-Korea>Overview',\n",
       "   '',\n",
       "   'The 2002 world cup was held in South Korea and Japan and certainly did not disappoint.',\n",
       "   'The opening group stage consisted of 32 teams.',\n",
       "   'Group E was considered the ‘group of death’ including favourites Argentina, England, Sweden and Nigeria.'],\n",
       "  11510,\n",
       "  '']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가공된 데이터는 별도로 저장\n",
    "import json\n",
    "with open(\"data/training_data.txt\", 'w') as outfile:\n",
    "    json.dump(training_data, outfile)\n",
    "training_data[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리 2\n",
    "가공된 데이터를 BERT 모델에 넣을 수 있도록 만들어야합니다. 모델에는 자연어를 그대로 입력할 수 없으니 사전 학습된 BERT 모델의 vocabulary를 활용하여 토큰화 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분활된 토큰 형태: ['[CLS]', 'england', 'drew', 'their', 'opening', 'match', 'against', 'sweden', 'and', 'followed', 'that', 'up', 'with', 'a', 'glorious', '1', '-', '0', 'victory', 'over', 'argentina', 'thanks', 'to', 'a', 'david', 'beck', '##ham', 'penalty', 'in', 'the', 'first', 'half', '.', '[SEP]']\n",
      "숫자형 토큰 형태: [101, 2563, 3881, 2037, 3098, 2674, 2114, 4701, 1998, 2628, 2008, 2039, 2007, 1037, 14013, 1015, 1011, 1014, 3377, 2058, 5619, 4283, 2000, 1037, 2585, 10272, 3511, 6531, 1999, 1996, 2034, 2431, 1012, 102]\n",
      "offsets: [(0, 0), (0, 7), (8, 12), (13, 18), (19, 26), (27, 32), (33, 40), (41, 47), (48, 51), (52, 60), (61, 65), (66, 68), (69, 73), (74, 75), (76, 84), (85, 86), (86, 87), (87, 88), (89, 96), (97, 101), (102, 111), (112, 118), (119, 121), (122, 123), (124, 129), (130, 134), (134, 137), (138, 145), (146, 148), (149, 152), (153, 158), (159, 163), (163, 164), (0, 0)]\n",
      "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "special_tokens_mask: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "special_tokens_mask: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "overflowing: []\n"
     ]
    }
   ],
   "source": [
    "import tokenizers\n",
    "tokenizer = tokenizers.BertWordPieceTokenizer(\n",
    "    f\"data/bert-base-uncased/vocab.txt\", \n",
    "    lowercase=True\n",
    ")\n",
    "tok_tweet = tokenizer.encode(\"England drew their opening match against Sweden and followed that up with a glorious 1-0 victory over Argentina thanks to a David Beckham penalty in the first half.\")\n",
    "print(\"분활된 토큰 형태: \" + str(tok_tweet.tokens))\n",
    "print(\"숫자형 토큰 형태: \" + str(tok_tweet.ids))\n",
    "print(\"offsets: \" + str(tok_tweet.offsets))\n",
    "print(\"attention_mask: \" + str(tok_tweet.attention_mask))\n",
    "print(\"special_tokens_mask: \" + str(tok_tweet.special_tokens_mask))\n",
    "print(\"special_tokens_mask: \" + str(tok_tweet.special_tokens_mask))\n",
    "print(\"overflowing: \" + str(tok_tweet.overflowing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text, y_class, y_keyword):\n",
    "    if (len(text[1]) > 5):\n",
    "        before = text[1]\n",
    "    else:\n",
    "        before = text[0] + \" \" + text[1]\n",
    "    main = text[2]\n",
    "    \n",
    "    if (len(text[3]) > 5):\n",
    "        after = text[3]\n",
    "    else:\n",
    "        after = text[3] + \" \" + text[4]\n",
    "    \n",
    "    before = tokenizer.encode(before)\n",
    "    main = tokenizer.encode(main)\n",
    "    after = tokenizer.encode(after)\n",
    "\n",
    "\n",
    "    # 토큰 기준으로 키워드가 어디있는지 확인\n",
    "    keyword_position_in_token = -1\n",
    "    keyword_end_in_token = -1\n",
    "    if y_class != 11510:\n",
    "        keyword_position_in_string = text[2].find(y_keyword)\n",
    "        keyword_length_in_string = len(y_keyword) # 43까지...\n",
    "        for j in range(len(main.offsets)):\n",
    "            if keyword_position_in_token == -1 and main.offsets[j][0] >= keyword_position_in_string:\n",
    "                keyword_position_in_token = j\n",
    "            if main.offsets[j][1] == 0: continue\n",
    "            if main.offsets[j][1] <= (keyword_position_in_string + keyword_length_in_string):\n",
    "                keyword_end_in_token = j\n",
    "    else:        \n",
    "        keyword_position_in_token = 0\n",
    "        keyword_end_in_token = 0\n",
    "        \n",
    "    # ids = cls, classification number, sep, token, sep\n",
    "    ids = [101, y_class, 102] + before.ids[1:] + main.ids[1:] + after.ids[1:]\n",
    "    # mask = len(cls, classification number, sep, token, sep) = 1, else 0\n",
    "    mask = [1] * len(ids)\n",
    "    # token_type_ids len(token, sep) = 1, else 0\n",
    "    token_type_ids = [0,0,0] + [1] * (len(ids) - 3)\n",
    "\n",
    "    targets_start = keyword_position_in_token\n",
    "    targets_end = keyword_end_in_token\n",
    "\n",
    "    # offsets based on ids, token offsets (0,0)(0,0)(0,0)(0,a)...(0,0)\n",
    "    offsets = main.offsets\n",
    "    \n",
    "    # Pad sequence if its length < `max_len`\n",
    "    padding_length = 350 - len(ids)\n",
    "    if padding_length > 0:\n",
    "        ids = ids + ([0] * padding_length)\n",
    "        mask = mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        padding_length = 350 - len(offsets)\n",
    "        offsets = offsets + ([(0, 0)] * padding_length)\n",
    "        \n",
    "        \n",
    "    return {\n",
    "            'ids': ids,\n",
    "            'mask': mask, \n",
    "            'token_type_ids': token_type_ids,\n",
    "            'targets_start': targets_start, \n",
    "            'targets_end': targets_end, \n",
    "            'orig_text': text,\n",
    "            'orig_keyword': y_keyword,\n",
    "            'class': y_class,\n",
    "            'offsets': offsets ,\n",
    "            'main_offsets': main.offsets\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "a = preprocessing(training_data[i][0], training_data[i][1], training_data[i][2])\n",
    "a['targets_end']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이토치를 위한 데이터셋 클래스 생성\n",
    "이 클래스는 파이토치에서 데이터를 로드할 때 사용되는 인터페이스입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset:\n",
    "    \"\"\"\n",
    "    Dataset which stores the tweets and returns them as processed features\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        data = preprocessing(\n",
    "            self.dataset[item][0], \n",
    "            self.dataset[item][1], \n",
    "            self.dataset[item][2],\n",
    "        )\n",
    "        \n",
    "        # Return the processed data where the lists are converted to `torch.tensor`s\n",
    "        return {\n",
    "            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n",
    "            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n",
    "            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n",
    "            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n",
    "            'orig_tweet': data[\"orig_text\"],\n",
    "            'orig_selected': data[\"orig_keyword\"],\n",
    "            'sentiment': data[\"class\"],\n",
    "            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이토치 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "class TweetModel(transformers.BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Model class that combines a pretrained bert model with a linear later\n",
    "    \"\"\"\n",
    "    def __init__(self, conf):\n",
    "        super(TweetModel, self).__init__(conf)\n",
    "        # pretrained BERT model을 불러옵니다.\n",
    "        self.bert = transformers.BertModel.from_pretrained(config.BERT_PATH, config=conf)\n",
    "        \n",
    "        # Set 10% dropout to be applied to the BERT backbone's output\n",
    "        # dropout은 은닉층에서 일정 확률로 유닛을 사용하지 않도록(=0) 합니다.\n",
    "        # 따라서 해당 케이스에서는 사용된 유닛만을 이용해 loss를 구하고 grident를 수행합니다.\n",
    "        # 결국 오버피팅 방지 가능!! (하나의 유닛에 의존하는 현상을 제거)\n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "        # 768 is the dimensionality of bert-base-uncased's hidden representations\n",
    "        # Multiplied by 2 since the forward pass concatenates the last two hidden representation layers\n",
    "        # The output will have two dimensions (\"start_logits\", and \"end_logits\")\n",
    "        self.l0 = nn.Linear(768 * 2, 2)\n",
    "        torch.nn.init.normal_(self.l0.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        # Return the hidden states from the BERT backbone\n",
    "        _, _, out = self.bert(\n",
    "            ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        ) # bert_layers x bs x SL x (768)\n",
    "\n",
    "        # Concatenate the last two hidden states\n",
    "        # This is done since experiments have shown that just getting the last layer\n",
    "        # gives out vectors that may be too taylored to the original BERT training objectives (MLM + NSP)\n",
    "        # Sample explanation: https://bert-as-service.readthedocs.io/en/latest/section/faq.html#why-not-the-last-hidden-layer-why-second-to-last\n",
    "        out = torch.cat((out[-1], out[-2]), dim=-1) # bs x SL x (768 * 2)\n",
    "        # Apply 10% dropout to the last 2 hidden states\n",
    "        out = self.drop_out(out) # bs x SL x (768 * 2)\n",
    "        # The \"dropped out\" hidden vectors are now fed into the linear layer to output two scores\n",
    "        logits = self.l0(out) # bs x SL x 2\n",
    "        \n",
    "        # Splits the tensor into start_logits and end_logits\n",
    "        # (bs x SL x 2) -> (bs x SL x 1), (bs x SL x 1)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "\n",
    "        start_logits = start_logits.squeeze(-1) # (bs x SL)\n",
    "        end_logits = end_logits.squeeze(-1) # (bs x SL)\n",
    "\n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "    \"\"\"\n",
    "    Return the sum of the cross entropy losses for both the start and end logits\n",
    "    \"\"\"\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    start_loss = loss_fct(start_logits, start_positions)\n",
    "    end_loss = loss_fct(end_logits, end_positions)\n",
    "    total_loss = (start_loss + end_loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, optimizer, device, scheduler=None):\n",
    "    \"\"\"\n",
    "    Trains the bert model on the twitter data\n",
    "    \"\"\"\n",
    "    # Set model to training mode (dropout + sampled batch norm is activated)\n",
    "    model.train()\n",
    "    losses = utils.AverageMeter()\n",
    "    jaccards = utils.AverageMeter()\n",
    "\n",
    "    # Set tqdm to add loading screen and set the length\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    \n",
    "    # Train the model on each batch\n",
    "    for bi, d in enumerate(tk0):\n",
    "\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        targets_start = d[\"targets_start\"]\n",
    "        targets_end = d[\"targets_end\"]\n",
    "        sentiment = d[\"sentiment\"]\n",
    "        orig_selected = d[\"orig_selected\"]\n",
    "        orig_tweet = d[\"orig_tweet\"][2]\n",
    "        offsets = d[\"offsets\"]\n",
    "\n",
    "        # Move ids, masks, and targets to gpu while setting as torch.long\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets_start = targets_start.to(device, dtype=torch.long)\n",
    "        targets_end = targets_end.to(device, dtype=torch.long)\n",
    "\n",
    "        # Reset gradients\n",
    "        model.zero_grad()\n",
    "        # Use ids, masks, and token types as input to the model\n",
    "        # Predict logits for each of the input tokens for each batch\n",
    "        outputs_start, outputs_end = model(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        ) # (bs x SL), (bs x SL)\n",
    "        # Calculate batch loss based on CrossEntropy\n",
    "        loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
    "        # Calculate gradients based on loss\n",
    "        loss.backward()\n",
    "        # Adjust weights based on calculated gradients\n",
    "        optimizer.step()\n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Apply softmax to the start and end logits\n",
    "        # This squeezes each of the logits in a sequence to a value between 0 and 1, while ensuring that they sum to 1\n",
    "        # This is similar to the characteristics of \"probabilities\"\n",
    "        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "        \n",
    "        # Calculate the jaccard score based on the predictions for this batch\n",
    "        jaccard_scores = []\n",
    "        for px, tweet in enumerate(orig_tweet):\n",
    "            selected_tweet = orig_selected[px]\n",
    "            tweet_sentiment = sentiment[px]\n",
    "            jaccard_score, _ = calculate_jaccard_score(\n",
    "                original_tweet=tweet, # Full text of the px'th tweet in the batch\n",
    "                target_string=selected_tweet, # Span containing the specified sentiment for the px'th tweet in the batch\n",
    "                sentiment_val=tweet_sentiment, # Sentiment of the px'th tweet in the batch\n",
    "                idx_start=np.argmax(outputs_start[px, :]), # Predicted start index for the px'th tweet in the batch\n",
    "                idx_end=np.argmax(outputs_end[px, :]), # Predicted end index for the px'th tweet in the batch\n",
    "                offsets=offsets[px] # Offsets for each of the tokens for the px'th tweet in the batch\n",
    "            )\n",
    "            jaccard_scores.append(jaccard_score)\n",
    "        # Update the jaccard score and loss\n",
    "        # For details, refer to `AverageMeter` in https://www.kaggle.com/abhishek/utils\n",
    "        jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
    "        losses.update(loss.item(), ids.size(0))\n",
    "        # Print the average loss and jaccard score at the end of each batch\n",
    "        tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(data_loader, model, device):\n",
    "    \"\"\"\n",
    "    Evaluation function to predict on the test set\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    # I.e., turn off dropout and set batchnorm to use overall mean and variance (from training), rather than batch level mean and variance\n",
    "    # Reference: https://github.com/pytorch/pytorch/issues/5406\n",
    "    model.eval()\n",
    "    losses = utils.AverageMeter()\n",
    "    jaccards = utils.AverageMeter()\n",
    "    \n",
    "    # Turns off gradient calculations (https://datascience.stackexchange.com/questions/32651/what-is-the-use-of-torch-no-grad-in-pytorch)\n",
    "    with torch.no_grad():\n",
    "        tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "        # Make predictions and calculate loss / jaccard score for each batch\n",
    "        for bi, d in enumerate(tk0):\n",
    "            ids = d[\"ids\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            sentiment = d[\"sentiment\"]\n",
    "            orig_selected = d[\"orig_selected\"]\n",
    "            orig_tweet = d[\"orig_tweet\"][2]\n",
    "            targets_start = d[\"targets_start\"]\n",
    "            targets_end = d[\"targets_end\"]\n",
    "            offsets = d[\"offsets\"].numpy()\n",
    "\n",
    "            # Move tensors to GPU for faster matrix calculations\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            targets_start = targets_start.to(device, dtype=torch.long)\n",
    "            targets_end = targets_end.to(device, dtype=torch.long)\n",
    "\n",
    "            # Predict logits for start and end indexes\n",
    "            outputs_start, outputs_end = model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            # Calculate loss for the batch\n",
    "            loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
    "            # Apply softmax to the predicted logits for the start and end indexes\n",
    "            # This converts the \"logits\" to \"probability-like\" scores\n",
    "            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "            # Calculate jaccard scores for each tweet in the batch\n",
    "            jaccard_scores = []\n",
    "            for px, tweet in enumerate(orig_tweet):\n",
    "                selected_tweet = orig_selected[px]\n",
    "                tweet_sentiment = sentiment[px]\n",
    "                jaccard_score, _ = calculate_jaccard_score(\n",
    "                    original_tweet=tweet,\n",
    "                    target_string=selected_tweet,\n",
    "                    sentiment_val=tweet_sentiment,\n",
    "                    idx_start=np.argmax(outputs_start[px, :]),\n",
    "                    idx_end=np.argmax(outputs_end[px, :]),\n",
    "                    offsets=offsets[px]\n",
    "                )\n",
    "                jaccard_scores.append(jaccard_score)\n",
    "\n",
    "            # Update running jaccard score and loss\n",
    "            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
    "            losses.update(loss.item(), ids.size(0))\n",
    "            # Print the running average loss and jaccard score\n",
    "            tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n",
    "    \n",
    "    print(f\"Jaccard = {jaccards.avg}\")\n",
    "    return jaccards.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import tokenizers\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.autonotebook import tqdm\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jaccard_score(\n",
    "    original_tweet, \n",
    "    target_string, \n",
    "    sentiment_val, \n",
    "    idx_start, \n",
    "    idx_end, \n",
    "    offsets,\n",
    "    verbose=False):\n",
    "    \"\"\"\n",
    "    Calculate the jaccard score from the predicted span and the actual span for a batch of tweets\n",
    "    \"\"\"\n",
    "    # A span's end index has to be greater than or equal to the start index\n",
    "    # If this doesn't hold, the start index is set to equal the end index (the span is a single token)\n",
    "    if idx_end < idx_start:\n",
    "        idx_end = idx_start\n",
    "    \n",
    "    # Combine into a string the tokens that belong to the predicted span\n",
    "    filtered_output  = \"\"\n",
    "    for ix in range(idx_start, idx_end + 1):\n",
    "        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n",
    "        # If the token is not the last token in the tweet, and the ending offset of the current token is less\n",
    "        # than the beginning offset of the following token, add a space.\n",
    "        # Basically, add a space when the next token (word piece) corresponds to a new word\n",
    "        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n",
    "            filtered_output += \" \"\n",
    "    #print(filtered_output)\n",
    "    # Set the predicted output as the original tweet when the tweet's sentiment is \"neutral\", or the tweet only contains one word\n",
    "    if sentiment_val == 11510 or len(original_tweet.split()) < 2:\n",
    "        filtered_output = original_tweet\n",
    "    # Calculate the jaccard score between the predicted span, and the actual span\n",
    "    # The IOU (intersection over union) approach is detailed in the utils module's `jaccard` function:\n",
    "    # https://www.kaggle.com/abhishek/utils\n",
    "    jac = utils.jaccard(target_string.strip(), filtered_output.strip())\n",
    "    return jac, filtered_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is Starting for fold=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3559d26a33cd4137a3db1155a57c9db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=233.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c48b782e091647eaba8eb87aa8476594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=233.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.07563075504492421\n",
      "Jaccard Score = 0.07563075504492421\n",
      "Validation score improved (-inf --> 0.07563075504492421). Saving model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b8ebacdcac1491c95b28b50da9ae80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=233.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d86a546459947598ae1e6305ba59bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=233.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.10607780022408507\n",
      "Jaccard Score = 0.10607780022408507\n",
      "Validation score improved (0.07563075504492421 --> 0.10607780022408507). Saving model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ad05cf837640869a9e8e716f0de08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=233.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c7715578324f279dac05699b3149af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=233.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.14321486935479583\n",
      "Jaccard Score = 0.14321486935479583\n",
      "Validation score improved (0.10607780022408507 --> 0.14321486935479583). Saving model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee585158de84521affbc7c2421b8964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=233.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9919fba985ec4292b0dcfec0b06e107c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=233.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.19612623572291762\n",
      "Jaccard Score = 0.19612623572291762\n",
      "Validation score improved (0.14321486935479583 --> 0.19612623572291762). Saving model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "858758bede1845e5940e08a0a3c4b2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=233.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e5dd80f72341b88cd83c6e092b19ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=233.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.2055464160351133\n",
      "Jaccard Score = 0.2055464160351133\n",
      "Validation score improved (0.19612623572291762 --> 0.2055464160351133). Saving model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0581e921ad41928e1bd1cfa486ccc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=233.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc71074e1a04d2681c7a06c5a4dbfb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=233.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.2055464160351133\n",
      "Jaccard Score = 0.2055464160351133\n",
      "EarlyStopping counter: 1 out of 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1cbbdd375f8477cadee660586cf0eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=233.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a74fea7c46d4e94a9e7411046a37f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=233.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.2055464160351133\n",
      "Jaccard Score = 0.2055464160351133\n",
      "EarlyStopping counter: 2 out of 2\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "class config:\n",
    "    MAX_LEN = 128\n",
    "    TRAIN_BATCH_SIZE = 2\n",
    "    VALID_BATCH_SIZE = 1\n",
    "    EPOCHS = 5\n",
    "    BERT_PATH = \"../input/bert-base-uncased/\"\n",
    "    MODEL_PATH = \"model.bin\"\n",
    "    TRAINING_FILE = \"../input/tweet-train-folds/train_folds.csv\"\n",
    "    TOKENIZER = tokenizers.BertWordPieceTokenizer(\n",
    "        f\"{BERT_PATH}/vocab.txt\", \n",
    "        lowercase=True\n",
    "    )\n",
    "    \n",
    "train_dataset = TextDataset(training_data)\n",
    "\n",
    "# Instantiate DataLoader with `train_dataset`\n",
    "# This is a generator that yields the dataset in batches\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.TRAIN_BATCH_SIZE,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "valid_data_loader = TextDataset(training_data)\n",
    "\n",
    "# Instantiate DataLoader with `train_dataset`\n",
    "# This is a generator that yields the dataset in batches\n",
    "valid_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.TRAIN_BATCH_SIZE,\n",
    "    num_workers=0\n",
    ")\n",
    "# Set device as `cuda` (GPU)\n",
    "device = torch.device(\"cuda\")\n",
    "# Load pretrained BERT (bert-base-uncased)\n",
    "model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\n",
    "# Output hidden states\n",
    "# This is important to set since we want to concatenate the hidden states from the last 2 BERT layers\n",
    "model_config.output_hidden_states = True\n",
    "# Instantiate our model with `model_config`\n",
    "model = TweetModel(conf=model_config)\n",
    "# Move the model to the GPU\n",
    "model.to(device)\n",
    "\n",
    "# Calculate the number of training steps\n",
    "num_train_steps = int(len(training_data) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
    "# Get the list of named parameters\n",
    "param_optimizer = list(model.named_parameters())\n",
    "# Specify parameters where weight decay shouldn't be applied\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "# Define two sets of parameters: those with weight decay, and those without\n",
    "optimizer_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "# Instantiate AdamW optimizer with our two sets of parameters, and a learning rate of 3e-5\n",
    "optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "# Create a scheduler to set the learning rate at each training step\n",
    "# \"Create a schedule with a learning rate that decreases linearly after linearly increasing during a warmup period.\" (https://pytorch.org/docs/stable/optim.html)\n",
    "# Since num_warmup_steps = 0, the learning rate starts at 3e-5, and then linearly decreases at each training step\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=num_train_steps\n",
    ")\n",
    "\n",
    "# Apply early stopping with patience of 2\n",
    "# This means to stop training new epochs when 2 rounds have passed without any improvement\n",
    "es = utils.EarlyStopping(patience=2, mode=\"max\")\n",
    "fold = 0\n",
    "print(f\"Training is Starting for fold={fold}\")\n",
    "\n",
    "# I'm training only for 3 epochs even though I specified 5!!!\n",
    "for epoch in range(10):\n",
    "    train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n",
    "    jaccard = eval_fn(valid_data_loader, model, device)\n",
    "    print(f\"Jaccard Score = {jaccard}\")\n",
    "    es(jaccard, model, model_path=f\"model_{fold}.bin\")\n",
    "    if es.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "        \n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TweetModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (drop_out): Dropout(p=0.1, inplace=False)\n",
       "  (l0): Linear(in_features=1536, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\n",
    "model_config.output_hidden_states = True\n",
    "\n",
    "model1 = TweetModel(conf=model_config)\n",
    "model1.to(device)\n",
    "model1.load_state_dict(torch.load(\"model_0.bin\"))\n",
    "model1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb2e71f3d904b85960fd7c833225533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=465.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final_output = []\n",
    "\n",
    "# Instantiate TweetDataset with the test data\n",
    "test_dataset = TextDataset(training_data)\n",
    "\n",
    "# Instantiate DataLoader with `test_dataset`\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=config.VALID_BATCH_SIZE,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Turn of gradient calculations\n",
    "with torch.no_grad():\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    # Predict the span containing the sentiment for each batch\n",
    "    for bi, d in enumerate(tk0):\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        sentiment = d[\"sentiment\"]\n",
    "        orig_selected = d[\"orig_selected\"]\n",
    "        orig_tweet = d[\"orig_tweet\"][2]\n",
    "        targets_start = d[\"targets_start\"]\n",
    "        targets_end = d[\"targets_end\"]\n",
    "        offsets = d[\"offsets\"].numpy()\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets_start = targets_start.to(device, dtype=torch.long)\n",
    "        targets_end = targets_end.to(device, dtype=torch.long)\n",
    "\n",
    "        # Predict start and end logits for each of the five models\n",
    "        outputs_start, outputs_end = model1(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        # Apply softmax to the predicted start and end logits\n",
    "        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "\n",
    "        # Convert the start and end scores to actual predicted spans (in string form)\n",
    "        for px, tweet in enumerate(orig_tweet):\n",
    "            selected_tweet = orig_selected[px]\n",
    "            tweet_sentiment = sentiment[px]\n",
    "            _, output_sentence = calculate_jaccard_score(\n",
    "                original_tweet=tweet,\n",
    "                target_string=selected_tweet,\n",
    "                sentiment_val=tweet_sentiment,\n",
    "                idx_start=np.argmax(outputs_start[px, :]),\n",
    "                idx_end=np.argmax(outputs_end[px, :]),\n",
    "                offsets=offsets[px]\n",
    "            )\n",
    "            final_output.append([np.argmax(outputs_start[px, :]), np.argmax(outputs_end[px, :]), output_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"origin\": \"2002 Japan-Korea\",\n",
      "    \"output\": \"2002 Japan-Korea\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Seas of Japan\",\n",
      "    \"output\": \"Seas of Japan \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Seas of Japan\",\n",
      "    \"output\": \"Japan \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea Of Japan\",\n",
      "    \"output\": \"Of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan (Japan Sea)\",\n",
      "    \"output\": \"Sea of Japan (Japan Sea)\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan (Japan Sea)\",\n",
      "    \"output\": \"(Japan Sea)\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan (Japan Sea) is the only internationally established name\",\n",
      "    \"output\": \"Sea of Japan (Japan Sea) is the only internationally established name \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan (Japan Sea) became recognized internationally by the early 19th Century\",\n",
      "    \"output\": \"Sea of Japan (Japan Sea) became recognized internationally by the early 19th Century \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan (Japan Sea)\",\n",
      "    \"output\": \"establishment of the name - Sea of Japan (Japan Sea) \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"use the name - Sea of Japan (Japan Sea) for making their nautical chart\",\n",
      "    \"output\": \"chart\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan (Japan Sea) is established in the guidelines on names of seas, published by the International Hydrographic Organization (IHO)\",\n",
      "    \"output\": \"Sea of Japan (Japan Sea) is established in the guidelines on names of seas, published by the International Hydrographic Organization (IHO) \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan (Japan Sea) established by \\\"Limits of Oceans and Seas\\\"\",\n",
      "    \"output\": \"Sea of Japan (Japan Sea) established by \\\"Limits of Oceans and Seas\\\" \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"IHO has consistently used Sea of Japan (Japan Sea)\",\n",
      "    \"output\": \"used Sea of Japan (Japan Sea) \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"If \\\"East Sea\\\" were used alongside the name - Sea of Japan (Japan Sea) in \\\"Limits of Oceans and Seas\\\" which is the guideline regarding names on sea, it would surely lead to confusion\",\n",
      "    \"output\": \"the name - Sea of Japan (Japan Sea) in \\\"Limits of Oceans and Seas\\\" which is the guideline regarding names on sea, it would surely lead to confusion \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan (Japan Sea) has been the only internationally established name\",\n",
      "    \"output\": \"Sea of Japan (Japan Sea) has been the only internationally established name \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan (Japan Sea)\",\n",
      "    \"output\": \"Sea of Japan (Japan Sea) \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"The establishment of the name - Sea of Japan (Japan Sea)\",\n",
      "    \"output\": \"Japan Sea)\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan (Japan Sea)\",\n",
      "    \"output\": \"Sea of Japan (Japan Sea) \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan (Japan Sea) was an unknown sea area, and the shape of the Sea of Japan (Japan Sea)\",\n",
      "    \"output\": \"Sea of Japan (Japan Sea) was an unknown sea area, and the shape of the Sea of Japan (Japan Sea) \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan (Japan Sea), Krusenstern described, based on an accurate map, that \\\"People also call this sea area the Sea of Korea, but because only a small part of this sea touches the Korean coast, it is appropriate to name it \\\"the Sea of Japan\\\"\",\n",
      "    \"output\": \"Sea of Japan (Japan Sea), Krusenstern described, based on an accurate map, that \\\"People also call this sea area the Sea of Korea, but because only a small part of this sea touches the Korean coast, it is appropriate to name it \\\"the Sea of Japan\\\" \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan (Japan Sea)\",\n",
      "    \"output\": \"Sea of Japan (Japan Sea) were conducted by European cartographers, explorers and navigator\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan (Japan Sea) became established internationally as the name\",\n",
      "    \"output\": \", the name - Sea of Japan (Japan Sea) became established internationally as the name \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan (Japan Sea)) have solely used the Sea of Japan (Japan Sea)\",\n",
      "    \"output\": \"Sea of Japan (Japan Sea)) have solely used the Sea of Japan (Japan Sea) \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan had already been internationally recognized and firmly established by the early 19th\",\n",
      "    \"output\": \"Sea of Japan had already been internationally recognized and firmly established by the early 19th \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan (Japan Sea) established\",\n",
      "    \"output\": \"Sea of Japan (Japan Sea) established by \\\"Limits of Oceans and Seas\\\"\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"has consistently used the name - Sea of Japan (Japan Sea) as the name\",\n",
      "    \"output\": \"has consistently used the name - Sea of Japan (Japan Sea) as the name \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan (Japan Sea) was already established internationally as the sole name indicating the Sea of Japan (Japan Sea)\",\n",
      "    \"output\": \"Sea of Japan (Japan Sea) was already established internationally as the sole name indicating the Sea of Japan (Japan Sea) \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan (Japan Sea)\",\n",
      "    \"output\": \"Sea of Japan (Japan Sea)\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan (Japan Sea) worldwide\",\n",
      "    \"output\": \"Sea of Japan (Japan Sea) worldwide\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan (Japan Sea) was used solely in the guideline\",\n",
      "    \"output\": \"Sea of Japan (Japan Sea) was used solely in the guideline\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan (Japan Sea) was used solely in the guideline\",\n",
      "    \"output\": \"Sea of Japan (Japan Sea) was used solely in the guideline\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan (Japan Sea) was used solely in the guideline\",\n",
      "    \"output\": \"Sea of Japan (Japan Sea) was used solely in the guideline\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"The name Sea of Japan (Japan Sea) was described solely in the draft\",\n",
      "    \"output\": \"The name Sea of Japan (Japan Sea) was described solely in the draft\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"ROK did not make an objection against the name - Sea of Japan (Japan Sea).\",\n",
      "    \"output\": \".\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"not published due to the lack of consensus among the member states for \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan (Japan Sea) has been consistently and solely used in \\\"Limits of Oceans and Seas\\\"\",\n",
      "    \"output\": \"Sea of Japan (Japan Sea) has been consistently and solely used in \\\"Limits of Oceans and Seas\\\" \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"ROK used the name of Sea of Japan (Japan Sea)\",\n",
      "    \"output\": \"ROK used the name of Sea of Japan (Japan Sea) \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"there is the historic fact that the ROK did not make an objection against the name - Sea of Japan (Japan Sea) even after World War II.\",\n",
      "    \"output\": \"there is the historic fact that the ROK did not make an objection against the name - Sea of Japan (Japan Sea) even after World War II.\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"suddenly begun to challenge the sole use of the name Sea of Japan\",\n",
      "    \"output\": \"suddenly begun to challenge the sole use of the name Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"confirms that the name Sea of Japan was already prevalent at the early 19th century\",\n",
      "    \"output\": \"confirms that the name Sea of Japan was already prevalent at the early 19th century\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"was unable to exercise any influence to establish the name Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"the ROK's assertion that the name Sea of Japan became widespread as a result of \\\"expansionism and colonial rule\\\"\",\n",
      "    \"output\": \"the ROK's assertion that the name Sea of Japan became widespread as a result of \\\"expansionism and colonial rule\\\" \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"the name Sea of Japan is the only name that has been in wide use internationally for a long period of time.\",\n",
      "    \"output\": \"the name Sea of Japan is the only name that has been in wide use internationally for a long period of time.\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"seas such as with the Sea of Japan\",\n",
      "    \"output\": \", these resolutions presume that the geographical feature concerned is under the sovereignty of two or more countries, such as in the case of a bay or strait, and does not apply to the high seas such as with the Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"officially confirmed its policy requiring the use of Sea of Japan as the standard geographical term in all official UN publications\",\n",
      "    \"output\": \", as stated under item4 the UN has already officially confirmed its policy requiring the use of Sea of Japan as the standard geographical term in all official UN publications\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"was a rapid increase in the use of the name Sea of Japan\",\n",
      "    \"output\": \"was a rapid increase in the use of the name Sea of Japan \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"This clearly shows the fallacy of the ROK's assertion that the name Sea of Japan\",\n",
      "    \"output\": \"This clearly shows the fallacy of the ROK's assertion that the name Sea of Japan \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"can be interpreted as affirming that the name Sea of Japan was in widespread use well\",\n",
      "    \"output\": \"can be interpreted as affirming that the name Sea of Japan was in widespread use well \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Japan Sea\",\n",
      "    \"output\": \"KYODO KYODO SHARE Nov 17, 2020 The International Hydrographic Organization has tentatively approved a proposal that supports the exclusive use of the name \\u201cJapan Sea\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Japan Sea\",\n",
      "    \"output\": \"Japan Sea\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"\\u201cJapan Sea\\u201d is the only internationally established name\",\n",
      "    \"output\": \"\\u201cJapan Sea\\u201d is the only internationally established name \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Japan Sea\",\n",
      "    \"output\": \"Japan Sea\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Japan Sea\",\n",
      "    \"output\": \"Japan Sea\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Professor\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"In the map Kunyu Wanguo Quantu drawn up by an Italian missionary priest of the Jesuit Order Matteo Ricci 400 years ago, the \\\"Sea of Japan\\\" was written down in kanji already\",\n",
      "    \"output\": \"In the map Kunyu Wanguo Quantu drawn up by an Italian missionary priest of the Jesuit Order Matteo Ricci 400 years ago, the \\\"Sea of Japan\\\" was written down in kanji already\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"The missionary spread the term \\\"Sea of Japan\\\" to Europe with a new awareness of geographical features\",\n",
      "    \"output\": \"The missionary spread the term \\\"Sea of Japan\\\" to Europe with a new awareness of geographical features\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \".\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"[Takeshima] Island map showing distances to Dokdo TakeshimaDokdo Island (also called Liancourt Rocks by some nations and Takeshima by Japan)\",\n",
      "    \"output\": \"[Takeshima] Island map showing distances to Dokdo TakeshimaDokdo Island (also called Liancourt Rocks by some nations and Takeshima by Japan) \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Takeshima\",\n",
      "    \"output\": \"Takeshima \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Takeshima\",\n",
      "    \"output\": \"Takeshima \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Takeshima\",\n",
      "    \"output\": \"Takeshima \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Takeshima\",\n",
      "    \"output\": \"Takeshima \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Takeshima\",\n",
      "    \"output\": \"Takeshima\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Takeshima Day\",\n",
      "    \"output\": \"Takeshima Day \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Takeshima\",\n",
      "    \"output\": \"Takeshima \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Takeshima Day\",\n",
      "    \"output\": \"Takeshima Day\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"The Sea of Japan is the only name that has been established internationally, and there is no need or reason for changing it\",\n",
      "    \"output\": \"The \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"East Sea\",\n",
      "    \"output\": \"East Sea\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan \"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan This is due to the particular characteristics of the Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan This is due to the particular characteristics of the Sea of Japan\"\n",
      "}\n",
      "{\n",
      "    \"origin\": \"Sea of Japan\",\n",
      "    \"output\": \"Sea of Japan\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "for i in range(0, len(test_dataset)):\n",
    "    if test_dataset[i]['sentiment'] != 11510:\n",
    "        temp = {\n",
    "            \"origin\": test_dataset[i]['orig_selected'],\n",
    "            \"output\": final_output[i][2]\n",
    "        }\n",
    "        print(json.dumps(temp, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
