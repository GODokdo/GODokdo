{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 승화 문서에서의 표기 오류 검출\n",
    "\n",
    "BERT(Bidirectional Encoder Representations from Transformers)는 구글이 개발한 사전훈련된(pre-training) 모델입니다. 이 모델은 위키피디아같은 텍스트 코퍼스(말뭉치)를 사용하여 미리 학습되었다는 특징이 있습니다. 그리고 BERT의 특성으로 단어를 학습할 때 문맥을 함께 고려하기때문에 언어의 패턴을 이해한 모델이 만들어집니다.\n",
    "\n",
    "이를 기반으로 새로운 문제에 적용하는 전이학습(transfer learning)을 수행할 수 있습니다. 미리 학습된 모델을 사용하기 때문에 적은 데이터로도 빠르게 학습이 가능하다는 이점이 있습니다.\n",
    "\n",
    "따라서 해당 모델을 기반으로 문서에서의 잘못된 표기 오류를 검출하는 알고리즘을 개발하였습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목표\n",
    "\n",
    "웹사이트나 텍스트 문서는 긴 여러개의 문장으로 이루어져있습니다. 각 문장이 잘못되었는지를 검사하고 잘못된 경우 잘못된 표현이 어디에 있는지 정확한 위치를 예측하여 알려주는 것이 저희 모델의 최종 목표입니다.\n",
    "\n",
    "따라서 저희는 BERT 모델에 linear regression를 적용한 네트워크 모델을 사용할 예정입니다. Output의 [CLS] 토큰을 통해 문장의 표기 오류를 분류하고 linear regression을 통해 그 위치를 예측할 것입니다.\n",
    "\n",
    "또한 표기 오류가 있지만 문제가 되지 않는 경우가 있습니다.\n",
    "\n",
    "<pre>\n",
    "한국 옆에는 작은 섬이 있는데 이것은 다케시마라고 불리기도 한다. \n",
    "그러나 이것은 잘못된 것으로 올바른 표기는 독도이다.\n",
    "</pre>\n",
    "\n",
    "문장 단위로 표기 오류를 검출한다면 인공지능은 표기 오류 결과로 \"다케시마\"를 지목할 것입니다.\n",
    "그러나 전체적인 문맥을 보면 해당 문장은 잘못된 사례를 이야기 해줄 뿐, 오류가 있는 문장이라고 말을 할수는 없습니다.\n",
    "\n",
    "따라서 BERT 모델에 주변 문장을 함께 학습시키는 모델을 구상하였습니다.\n",
    "**학습 예시**\n",
    "<pre>\n",
    "[index-2] None\n",
    "[index-1] None\n",
    "[판단할 문장] 한국 옆에는 작은 섬이 있는데 이것은 다케시마라고 불리기도 한다. \n",
    "[index+1] 그러나 이것은 잘못된 것으로 올바른 표기는 독도이다.\n",
    "[index+2]\n",
    "</pre>\n",
    "\n",
    "이 알고리즘을 이용해 sentence window를 슬라이딩 시키며 학습을 진행하면 문맥을 함께 고려하는 모델을 만들 수 있을 것이라 생각하였습니다.\n",
    "표기 오류는 오직 [판단할 문장]에 있는지만 체크하도록 학습을 시킬 것으로, 주변 문장의 표기 오류를 검출하여 모델이 혼잡해지는 경우를 최소화하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기\n",
    "API를 통해 서버로부터 사용 가능한 학습 데이터를 불러옵니다. \n",
    "\n",
    "API는 [ {no, contents, errors[code,keyword] } , ...] 형태로 데이터를 보내주도록 되어있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "class APIDokdo:\n",
    "    def __init__(self, apikey):\n",
    "      self.apiurl = \"https://api.easylab.kr\"\n",
    "      self.headers =  {'authorization': apikey}\n",
    "    def getTrainingData(self):\n",
    "        return requests.get(self.apiurl + \"/deeplearning/data/sentences\", headers=self.headers).json()['list']\n",
    "    def getErrorTypes(self):\n",
    "        return requests.get(self.apiurl + \"/error\", headers=self.headers).json()['list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불러온 문서의 개수:  15\n"
     ]
    }
   ],
   "source": [
    "api = APIDokdo(\"godapikey12\")\n",
    "original_data_json = api.getTrainingData()\n",
    "print(\"불러온 문서의 개수: \", len(original_data_json))\n",
    "\n",
    "class_list = [0]\n",
    "class_list_from_code = {0:0}\n",
    "for i in api.getErrorTypes():\n",
    "    class_list_from_code[i['code']] = len(class_list)\n",
    "    class_list.append(i['code'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리\n",
    "불러온 데이터를 문장 단위로 분리를 하고, 표기 오류를 검색합니다. 또한 문맥을 고려할 수 있게 5개의 문장씩 관리합니다 (0-5, 1-6, 2-7, 3-8 ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "original_data = []\n",
    "for i in original_data_json:\n",
    "    sentences = []\n",
    "    # text = i['contents'].replace(\"\\r\",\"\").replace(\"\\n\",\"\")\n",
    "    #sentences = [i.strip() for i in tokenize.sent_tokenize(text)] #문장 단위로 분리 및 문장 앞뒤 공백 제거\n",
    "    sentences = [i.strip() for i in i['contents'].split('\\n')]\n",
    "    \n",
    "    # 두줄 이상 공백이 있는 경우 제거\n",
    "    last = \"\"\n",
    "    remove_indexes = []\n",
    "    for j in range(0, len(sentences)):\n",
    "        if last == \"\" and sentences[j] == \"\":\n",
    "            remove_indexes.append(j)\n",
    "        last = sentences[j]\n",
    "        \n",
    "    for index in sorted(remove_indexes, reverse=True):\n",
    "        del sentences[index]\n",
    "    \n",
    "    # 문장별로 표기 오류 키워드 검색 && 2 + 1 + 2 문장 단위로 자동 구성\n",
    "    # padding\n",
    "    sentences = ['', ''] + sentences + ['', '']\n",
    "    for index in range(2, len(sentences)):\n",
    "        # 빈 문장 제거\n",
    "        if (len(sentences[index]) == 0): continue\n",
    "            \n",
    "        y_class = 0\n",
    "        y_keyword = \"\"\n",
    "        # 현재 문장에 표기 오류가 있는지 확인\n",
    "        if 'errors' not in i:\n",
    "            print(i)\n",
    "        error_index = 0\n",
    "        for error in i['errors']:\n",
    "            if (error['sentence_no'] != index - 2):\n",
    "                continue\n",
    "            predict_keyword = sentences[index][error['position']:(error['position']+error['length'])]\n",
    "            if (predict_keyword != error['keyword']):\n",
    "                print(error)\n",
    "            y_class = class_list_from_code[error['code']]\n",
    "            y_keyword = error['keyword']\n",
    "            sequence = \"sequence \" + str(error_index) + \": \"\n",
    "            position = len(sequence) + error['position']\n",
    "            original_data.append([[(sequence + sentences[j] if j == index else sentences[j]) for j in range(index-2, index+2 + 1)], y_class, y_keyword, position, error['length']])\n",
    "            error_index += 1\n",
    "        \n",
    "        original_data.append([[(\"sequence \"+str(error_index)+\": \"+ sentences[j] if j == index else sentences[j]) for j in range(index-2, index+2 + 1)],0, \"\", 0, 0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Turkey also went through that group.',\n",
       "  'The surprise team of the tournament proved to be Senegal, they won their opening match against the holders France and went on to reach the quarter finals, however they lost to Turkey.',\n",
       "  'sequence 0: The Germans started off the world cup in fine form, thrashing Saudi Arabia 8-0; they topped their group and followed that up by beating Paraguay 1-0 in the second round, beating USA in the quarter finals and then ending South Koreas world cup adventure in the semi- finals.',\n",
       "  '',\n",
       "  'Italy met South Korea in the second round; Korea won 2-1 after extra time, and booked a place in the quarter finals against Spain, who had beaten Republic of Ireland in the second round on penalties.'],\n",
       " 0,\n",
       " '',\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['',\n",
       "   '',\n",
       "   'sequence 0: World Cup>Past Tournaments>2002 Japan-Korea>Overview',\n",
       "   '',\n",
       "   'The 2002 world cup was held in South Korea and Japan and certainly did not disappoint.'],\n",
       "  1,\n",
       "  '2002 Japan-Korea',\n",
       "  39,\n",
       "  16],\n",
       " [['',\n",
       "   '',\n",
       "   'sequence 1: World Cup>Past Tournaments>2002 Japan-Korea>Overview',\n",
       "   '',\n",
       "   'The 2002 world cup was held in South Korea and Japan and certainly did not disappoint.'],\n",
       "  0,\n",
       "  '',\n",
       "  0,\n",
       "  0],\n",
       " [['World Cup>Past Tournaments>2002 Japan-Korea>Overview',\n",
       "   '',\n",
       "   'sequence 0: The 2002 world cup was held in South Korea and Japan and certainly did not disappoint.',\n",
       "   'The opening group stage consisted of 32 teams.',\n",
       "   'Group E was considered the ‘group of death’ including favourites Argentina, England, Sweden and Nigeria.'],\n",
       "  0,\n",
       "  '',\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가공된 데이터는 별도로 저장\n",
    "import json\n",
    "with open(\"data/original_data.txt\", 'w') as outfile:\n",
    "    json.dump(original_data, outfile)\n",
    "original_data[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리 2\n",
    "\n",
    "데이터 불균형 해결\n",
    "\n",
    "이거 없으면 소수 클래스 예측 엄청 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660\n",
      "660\n",
      "660\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "random.shuffle(original_data)\n",
    "index = int(len(original_data) * 0.8)\n",
    "train = original_data[0:index]\n",
    "test_data = original_data[index:]\n",
    "\n",
    "train = original_data\n",
    "test_data = original_data\n",
    "\n",
    "print(len(original_data))\n",
    "print(len(train))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[532, 532, 590, 540, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = []\n",
    "for i in train:\n",
    "    training_data.append(i)\n",
    "temp = [] * len(class_list)\n",
    "for i in class_list:\n",
    "    temp.append([])\n",
    "value_counts = [0] * len(class_list)\n",
    "for i in training_data:\n",
    "    temp[i[1]].append(i)\n",
    "    value_counts[i[1]] += 1\n",
    "    \n",
    "vv = max(value_counts)\n",
    "print(vv)\n",
    "for i in range(0,len(class_list)):\n",
    "    if len(temp[i]) == 0:\n",
    "        continue\n",
    "    for j in range(len(temp[i]), vv, len(temp[i])):\n",
    "        training_data.extend(temp[i])\n",
    "        \n",
    "value_counts = [0] * len(class_list)\n",
    "for i in training_data:\n",
    "    value_counts[i[1]] += 1\n",
    "    \n",
    "value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리 3\n",
    "가공된 데이터를 BERT 모델에 넣을 수 있도록 만들어야합니다. 모델에는 자연어를 그대로 입력할 수 없으니 사전 학습된 BERT 모델의 vocabulary를 활용하여 토큰화 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분활된 토큰 형태: ['[CLS]', 'england', 'drew', 'their', 'opening', 'match', 'against', 'sweden', 'and', 'followed', 'that', 'up', 'with', 'a', 'glorious', '1', '-', '0', 'victory', 'over', 'argentina', 'thanks', 'to', 'a', 'david', 'beck', '##ham', 'penalty', 'in', 'the', 'first', 'half', '.', '[SEP]']\n",
      "숫자형 토큰 형태: [101, 2563, 3881, 2037, 3098, 2674, 2114, 4701, 1998, 2628, 2008, 2039, 2007, 1037, 14013, 1015, 1011, 1014, 3377, 2058, 5619, 4283, 2000, 1037, 2585, 10272, 3511, 6531, 1999, 1996, 2034, 2431, 1012, 102]\n",
      "offsets: [(0, 0), (0, 7), (8, 12), (13, 18), (19, 26), (27, 32), (33, 40), (41, 47), (48, 51), (52, 60), (61, 65), (66, 68), (69, 73), (74, 75), (76, 84), (85, 86), (86, 87), (87, 88), (89, 96), (97, 101), (102, 111), (112, 118), (119, 121), (122, 123), (124, 129), (130, 134), (134, 137), (138, 145), (146, 148), (149, 152), (153, 158), (159, 163), (163, 164), (0, 0)]\n",
      "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "special_tokens_mask: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "special_tokens_mask: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "overflowing: []\n"
     ]
    }
   ],
   "source": [
    "import tokenizers\n",
    "tokenizer = tokenizers.BertWordPieceTokenizer(\n",
    "    f\"data/bert-base-uncased/vocab.txt\", \n",
    "    lowercase=True\n",
    ")\n",
    "tok_tweet = tokenizer.encode(\"England drew their opening match against Sweden and followed that up with a glorious 1-0 victory over Argentina thanks to a David Beckham penalty in the first half.\")\n",
    "print(\"분활된 토큰 형태: \" + str(tok_tweet.tokens))\n",
    "print(\"숫자형 토큰 형태: \" + str(tok_tweet.ids))\n",
    "print(\"offsets: \" + str(tok_tweet.offsets))\n",
    "print(\"attention_mask: \" + str(tok_tweet.attention_mask))\n",
    "print(\"special_tokens_mask: \" + str(tok_tweet.special_tokens_mask))\n",
    "print(\"special_tokens_mask: \" + str(tok_tweet.special_tokens_mask))\n",
    "print(\"overflowing: \" + str(tok_tweet.overflowing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text, y_class, y_keyword, position, length):\n",
    "    \n",
    "    if (len(text[1]) > 5):\n",
    "        before = text[1]\n",
    "    else:\n",
    "        before = text[0] + \" \" + text[1]\n",
    "        \n",
    "    main = text[2]\n",
    "    \n",
    "    if (len(text[3]) > 5):\n",
    "        after = text[3]\n",
    "    else:\n",
    "        after = text[3] + \" \" + text[4]\n",
    "    \n",
    "    before = tokenizer.encode(before)\n",
    "    main = tokenizer.encode(main)\n",
    "    after = tokenizer.encode(after)\n",
    "\n",
    "\n",
    "    # 토큰 기준으로 키워드가 어디있는지 확인\n",
    "    keyword_position_in_token = -1\n",
    "    keyword_end_in_token = -1\n",
    "    if y_class != 0:\n",
    "        keyword_position_in_string = position\n",
    "        keyword_length_in_string = length\n",
    "        for j in range(len(main.offsets)):\n",
    "            if keyword_position_in_token == -1 and main.offsets[j][0] >= keyword_position_in_string:\n",
    "                keyword_position_in_token = j\n",
    "            if main.offsets[j][1] == 0: continue\n",
    "            if main.offsets[j][1] <= (keyword_position_in_string + keyword_length_in_string):\n",
    "                keyword_end_in_token = j\n",
    "    else:        \n",
    "        keyword_position_in_token = 0\n",
    "        keyword_end_in_token = 0\n",
    "        \n",
    "    # ids = cls, classification number, sep, token, sep\n",
    "    ids = [101, 9999, 102] + before.ids[1:] + main.ids[1:] + after.ids[1:]\n",
    "    # mask = len(cls, classification number, sep, token, sep) = 1, else 0\n",
    "    mask = [1] * len(ids)\n",
    "    # token_type_ids len(token, sep) = 1, else 0\n",
    "    token_type_ids = [0,0,0] + [1] * (len(ids) - 3)\n",
    "\n",
    "    targets_start = keyword_position_in_token\n",
    "    targets_end = keyword_end_in_token\n",
    "\n",
    "    # offsets based on ids, token offsets (0,0)(0,0)(0,0)(0,a)...(0,0)\n",
    "    offsets = main.offsets\n",
    "    \n",
    "    # Pad sequence if its length < `max_len`\n",
    "    padding_length = 380 - len(ids)\n",
    "    if padding_length > 0:\n",
    "        ids = ids + ([0] * padding_length)\n",
    "        mask = mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        padding_length = 380 - len(offsets)\n",
    "        offsets = offsets + ([(0, 0)] * padding_length)\n",
    "        \n",
    "    temp = []\n",
    "    temp = [0] * len(class_list)\n",
    "    temp[y_class] = 1\n",
    "            \n",
    "    return {\n",
    "            'ids': ids,\n",
    "            'mask': mask, \n",
    "            'token_type_ids': token_type_ids,\n",
    "            'targets_start': targets_start, \n",
    "            'targets_end': targets_end, \n",
    "            'orig_text': text,\n",
    "            'orig_keyword': y_keyword,\n",
    "            'class': y_class,\n",
    "            'offsets': offsets ,\n",
    "            'targets_class': temp\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The islets are the last disputed territory between Korea and Japan following World War II.', \"Although Japan's MOFA insists Allied Command granted Dokdo to Japan after the Second World War, there was no mention of Dokdo in the Japan Peace Treaty, leaving the issue unsettled.\", 'sequence 0: Because of the apparent inseparability of Japan\\'s 1905 \"terra nullius\" annexation of Dokdo and Japan\\'s colonization of the Korea, Japan\\'s demands to claim Dokdo cause outrage from Korean citizens.', \"( Korea's Political Situation in 1905 When Japan Seized Dokdo – Takeshima Island ) To the right: Korean citizens protest Japan's claim to Dokdo accusing Japan's Government of whitewashing her past aggressions and colonial past.\", ''], 0, '', 0, 0]\n",
      "0 0\n",
      "0 0\n",
      "[(0, 0), (0, 8), (9, 10), (10, 11), (12, 19), (20, 22), (23, 26), (27, 35), (36, 39), (39, 41), (41, 44), (44, 50), (51, 53), (54, 59), (59, 60), (60, 61), (62, 66), (67, 68), (68, 73), (74, 78), (78, 81), (81, 82), (83, 93), (94, 96), (97, 99), (99, 100), (100, 102), (103, 106), (107, 112), (112, 113), (113, 114), (115, 127), (128, 130), (131, 134), (135, 140), (140, 141), (142, 147), (147, 148), (148, 149), (150, 157), (158, 160), (161, 166), (167, 169), (169, 170), (170, 172), (173, 178), (179, 186), (187, 191), (192, 198), (199, 207), (207, 208), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "i = 17\n",
    "print (training_data[i])\n",
    "print (training_data[i][3], training_data[i][4])\n",
    "a = preprocessing(training_data[i][0], training_data[i][1],training_data[i][2], training_data[i][3], training_data[i][4])\n",
    "print(a['targets_start'], a['targets_end'])\n",
    "print(a['offsets'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이토치를 위한 데이터셋 클래스 생성\n",
    "이 클래스는 파이토치에서 데이터를 로드할 때 사용되는 인터페이스입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset:\n",
    "    \"\"\"\n",
    "    Dataset which stores the tweets and returns them as processed features\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        data = preprocessing(\n",
    "            self.dataset[item][0], \n",
    "            self.dataset[item][1], \n",
    "            self.dataset[item][2], \n",
    "            self.dataset[item][3], \n",
    "            self.dataset[item][4]\n",
    "        )\n",
    "        temp = []\n",
    "        for i in data[\"targets_class\"]:\n",
    "            temp.append(torch.tensor(i, dtype=torch.long))\n",
    "            \n",
    "        # Return the processed data where the lists are converted to `torch.tensor`s\n",
    "        return {\n",
    "            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n",
    "            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n",
    "            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n",
    "            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n",
    "            'targets_class': torch.tensor(data[\"targets_class\"], dtype=torch.long),\n",
    "            'orig_tweet': data[\"orig_text\"],\n",
    "            'orig_selected': data[\"orig_keyword\"],\n",
    "            'sentiment': data[\"class\"],\n",
    "            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이토치 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "class TweetModel(transformers.BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Model class that combines a pretrained bert model with a linear later\n",
    "    \"\"\"\n",
    "    def __init__(self, conf, num_labels):\n",
    "        super(TweetModel, self).__init__(conf)\n",
    "        # pretrained BERT model을 불러옵니다.\n",
    "        self.bert = transformers.BertModel.from_pretrained(config.BERT_PATH, config=conf)\n",
    "        \n",
    "        # Set 10% dropout to be applied to the BERT backbone's output\n",
    "        # dropout은 은닉층에서 일정 확률로 유닛을 사용하지 않도록(=0) 합니다.\n",
    "        # 따라서 해당 케이스에서는 사용된 유닛만을 이용해 loss를 구하고 grident를 수행합니다.\n",
    "        # 결국 오버피팅 방지 가능!! (하나의 유닛에 의존하는 현상을 제거)\n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "        \n",
    "        # 우리가 쓰는 bert-base-uncased 모델은 768의 hidden representation을 가지고 있음\n",
    "        # 그래서 새로운 레이어를 이어 붙일 때에도 768개씩 붙여야함.\n",
    "        \n",
    "        # 우리의 데이터를 추가로 학습하는 용도로 사용할 추가적인 레이어가 필요함. (hidden_layer 추가)\n",
    "        # 히든 레이어를 추가할수록 복잡한 딥러닝 네트워크를 만들 수 있지만... 데이터가 많이 필요할 듯\n",
    "        \n",
    "        # 여기에서는 2개의 히든 레이어를 추가하기로 하였음. 768 * 2\n",
    "        # 그리고 \"start_logits\", and \"end_logits\"이라는 2개의 아웃풋을 얻기 위해 마지막은 2개의 노드로 수렴하게 만듦.\n",
    "        self.l0 = nn.Linear(768 * 2, 2 + num_labels)\n",
    "        \n",
    "        # 가중치 랜덤 초기화\n",
    "        torch.nn.init.normal_(self.l0.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids, keyword_sequence = 0):\n",
    "        # BERT backbone으로부터 hidden states를 얻어옴.\n",
    "        _, _, out = self.bert(\n",
    "            ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        ) # bert_layers x bs x SL x (768)\n",
    "\n",
    "        # Concatenate the last two hidden states\n",
    "        # This is done since experiments have shown that just getting the last layer\n",
    "        # gives out vectors that may be too taylored to the original BERT training objectives (MLM + NSP)\n",
    "        # Sample explanation: https://bert-as-service.readthedocs.io/en/latest/section/faq.html#why-not-the-last-hidden-layer-why-second-to-last\n",
    "        \n",
    "        out = torch.cat((out[-1], out[-2]), dim=-1) # bs x SL x (768 * 2)\n",
    "        # Apply 10% dropout to the last 2 hidden states\n",
    "        out = self.drop_out(out) # bs x SL x (768 * 2)\n",
    "        # The \"dropped out\" hidden vectors are now fed into the linear layer to output two scores\n",
    "        logits = self.l0(out) # bs x SL x 2\n",
    "        \n",
    "        # Splits the tensor into start_logits and end_logits\n",
    "        # (bs x SL x 2) -> (bs x SL x 1), (bs x SL x 1)\n",
    "        outputs = list(logits.split(1, dim=-1))\n",
    "        for i in range(0,len(outputs)):\n",
    "            outputs[i] = outputs[i].squeeze(-1)\n",
    "            \n",
    "\n",
    "        start_logits = outputs[0] # (bs x SL)\n",
    "        end_logits = outputs[1] # (bs x SL)\n",
    "        class_logits = outputs[2:]\n",
    "        return start_logits, end_logits, class_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(start_logits, end_logits, class_logits, start_positions, end_positions, class_targets):\n",
    "    \"\"\"\n",
    "    Return the sum of the cross entropy losses for both the start and end logits\n",
    "    \"\"\"\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    start_loss = loss_fct(start_logits, start_positions)\n",
    "    end_loss = loss_fct(end_logits, end_positions)\n",
    "    total_loss = (start_loss + end_loss) * 2\n",
    "    class_targets = class_targets.t()\n",
    "    for i in range(0, len(class_list)):\n",
    "        total_loss += loss_fct(class_logits[i], class_targets[i])\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, optimizer, device, scheduler=None):\n",
    "    \"\"\"\n",
    "    Trains the bert model on the twitter data\n",
    "    \"\"\"\n",
    "    # Set model to training mode (dropout + sampled batch norm is activated)\n",
    "    model.train()\n",
    "    losses = utils.AverageMeter()\n",
    "    jaccards = utils.AverageMeter()\n",
    "\n",
    "    # Set tqdm to add loading screen and set the length\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    \n",
    "    # Train the model on each batch\n",
    "    for bi, d in enumerate(tk0):\n",
    "\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        targets_start = d[\"targets_start\"]\n",
    "        targets_end = d[\"targets_end\"]\n",
    "        sentiment = d[\"sentiment\"]\n",
    "        orig_selected = d[\"orig_selected\"]\n",
    "        orig_tweet = d[\"orig_tweet\"][2]\n",
    "        offsets = d[\"offsets\"]\n",
    "        targets_class = d[\"targets_class\"]\n",
    "\n",
    "        # Move ids, masks, and targets to gpu while setting as torch.long\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets_start = targets_start.to(device, dtype=torch.long)\n",
    "        targets_end = targets_end.to(device, dtype=torch.long)\n",
    "        targets_class = targets_class.to(device, dtype=torch.long)\n",
    "        # Reset gradients\n",
    "        model.zero_grad()\n",
    "        # Use ids, masks, and token types as input to the model\n",
    "        # Predict logits for each of the input tokens for each batch\n",
    "        outputs_start, outputs_end, outputs_class = model(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        ) # (bs x SL), (bs x SL)\n",
    "        # Calculate batch loss based on CrossEntropy\n",
    "        loss = loss_fn(outputs_start, outputs_end, outputs_class, targets_start, targets_end, targets_class)\n",
    "        # Calculate gradients based on loss\n",
    "        loss.backward()\n",
    "        # Adjust weights based on calculated gradients\n",
    "        optimizer.step()\n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Apply softmax to the start and end logits\n",
    "        # This squeezes each of the logits in a sequence to a value between 0 and 1, while ensuring that they sum to 1\n",
    "        # This is similar to the characteristics of \"probabilities\"\n",
    "        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "        outputs_class = [torch.softmax(i, dim=1).cpu().detach().numpy() for i in outputs_class]\n",
    "        \n",
    "        # Calculate the jaccard score based on the predictions for this batch\n",
    "        jaccard_scores = []\n",
    "        for px, tweet in enumerate(orig_tweet):\n",
    "            selected_tweet = orig_selected[px]\n",
    "            tweet_sentiment = sentiment[px]\n",
    "            ont_hot_class = [np.argmax(i[px, :]) for i in outputs_class]\n",
    "            class_number = ont_hot_class.index(max(ont_hot_class))\n",
    "            jaccard_score, _ = calculate_jaccard_score(\n",
    "                original_tweet=tweet, # Full text of the px'th tweet in the batch\n",
    "                target_string=selected_tweet, # Span containing the specified sentiment for the px'th tweet in the batch\n",
    "                sentiment_val=tweet_sentiment, # Sentiment of the px'th tweet in the batch\n",
    "                idx_start=np.argmax(outputs_start[px, :]), # Predicted start index for the px'th tweet in the batch\n",
    "                idx_end=np.argmax(outputs_end[px, :]), # Predicted end index for the px'th tweet in the batch\n",
    "                offsets=offsets[px], # Offsets for each of the tokens for the px'th tweet in the batch\n",
    "                class_number=class_number\n",
    "            )\n",
    "            jaccard_scores.append(jaccard_score)\n",
    "        # Update the jaccard score and loss\n",
    "        # For details, refer to `AverageMeter` in https://www.kaggle.com/abhishek/utils\n",
    "        jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
    "        losses.update(loss.item(), ids.size(0))\n",
    "        # Print the average loss and jaccard score at the end of each batch\n",
    "        tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(data_loader, model, device):\n",
    "    \"\"\"\n",
    "    Evaluation function to predict on the test set\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    # I.e., turn off dropout and set batchnorm to use overall mean and variance (from training), rather than batch level mean and variance\n",
    "    # Reference: https://github.com/pytorch/pytorch/issues/5406\n",
    "    model.eval()\n",
    "    losses = utils.AverageMeter()\n",
    "    jaccards = utils.AverageMeter()\n",
    "    \n",
    "    # Turns off gradient calculations (https://datascience.stackexchange.com/questions/32651/what-is-the-use-of-torch-no-grad-in-pytorch)\n",
    "    with torch.no_grad():\n",
    "        tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "        # Make predictions and calculate loss / jaccard score for each batch\n",
    "        for bi, d in enumerate(tk0):\n",
    "            ids = d[\"ids\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            sentiment = d[\"sentiment\"]\n",
    "            orig_selected = d[\"orig_selected\"]\n",
    "            orig_tweet = d[\"orig_tweet\"][2]\n",
    "            targets_start = d[\"targets_start\"]\n",
    "            targets_end = d[\"targets_end\"]\n",
    "            offsets = d[\"offsets\"].numpy()\n",
    "            targets_class = d[\"targets_class\"]\n",
    "\n",
    "            # Move tensors to GPU for faster matrix calculations\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            targets_start = targets_start.to(device, dtype=torch.long)\n",
    "            targets_end = targets_end.to(device, dtype=torch.long)\n",
    "            targets_class = targets_class.to(device, dtype=torch.long)\n",
    "\n",
    "            # Predict logits for start and end indexes\n",
    "            outputs_start, outputs_end, outputs_class = model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            # Calculate loss for the batch\n",
    "            loss = loss_fn(outputs_start, outputs_end, outputs_class, targets_start, targets_end, targets_class)\n",
    "            # Apply softmax to the predicted logits for the start and end indexes\n",
    "            # This converts the \"logits\" to \"probability-like\" scores\n",
    "            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "            outputs_class = [torch.softmax(i, dim=1).cpu().detach().numpy() for i in outputs_class]\n",
    "            \n",
    "            # Calculate jaccard scores for each tweet in the batch\n",
    "            jaccard_scores = []\n",
    "            for px, tweet in enumerate(orig_tweet):\n",
    "                selected_tweet = orig_selected[px]\n",
    "                tweet_sentiment = sentiment[px]\n",
    "                ont_hot_class = [np.argmax(i[px, :]) for i in outputs_class]\n",
    "                class_number = ont_hot_class.index(max(ont_hot_class))\n",
    "                \n",
    "                jaccard_score, _ = calculate_jaccard_score(\n",
    "                    original_tweet=tweet,\n",
    "                    target_string=selected_tweet,\n",
    "                    sentiment_val=tweet_sentiment,\n",
    "                    idx_start=np.argmax(outputs_start[px, :]),\n",
    "                    idx_end=np.argmax(outputs_end[px, :]),\n",
    "                    offsets=offsets[px],\n",
    "                    class_number=class_number\n",
    "                )\n",
    "                jaccard_scores.append(jaccard_score)\n",
    "\n",
    "            # Update running jaccard score and loss\n",
    "            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
    "            losses.update(loss.item(), ids.size(0))\n",
    "            # Print the running average loss and jaccard score\n",
    "            tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n",
    "    \n",
    "    print(f\"Jaccard = {jaccards.avg}\")\n",
    "    return jaccards.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import tokenizers\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.autonotebook import tqdm\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jaccard_score(\n",
    "    original_tweet, \n",
    "    target_string, \n",
    "    sentiment_val, \n",
    "    idx_start, \n",
    "    idx_end, \n",
    "    offsets,\n",
    "    class_number,\n",
    "    verbose=False):\n",
    "    \"\"\"\n",
    "    Calculate the jaccard score from the predicted span and the actual span for a batch of tweets\n",
    "    \"\"\"\n",
    "    # A span's end index has to be greater than or equal to the start index\n",
    "    # If this doesn't hold, the start index is set to equal the end index (the span is a single token)\n",
    "    if idx_end < idx_start:\n",
    "        idx_end = idx_start\n",
    "    \n",
    "    # Combine into a string the tokens that belong to the predicted span\n",
    "    filtered_output  = \"\"\n",
    "    for ix in range(idx_start, idx_end + 1):\n",
    "        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n",
    "        # If the token is not the last token in the tweet, and the ending offset of the current token is less\n",
    "        # than the beginning offset of the following token, add a space.\n",
    "        # Basically, add a space when the next token (word piece) corresponds to a new word\n",
    "        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n",
    "            filtered_output += \" \"\n",
    "    #print(filtered_output)\n",
    "    # Set the predicted output as the original tweet when the tweet's sentiment is \"neutral\", or the tweet only contains one word\n",
    "    if sentiment_val == 0 or len(original_tweet.split()) < 2:\n",
    "        filtered_output = original_tweet\n",
    "    # Calculate the jaccard score between the predicted span, and the actual span\n",
    "    # The IOU (intersection over union) approach is detailed in the utils module's `jaccard` function:\n",
    "    # https://www.kaggle.com/abhishek/utils\n",
    "    jac = utils.jaccard(target_string.strip(), filtered_output.strip())\n",
    "    if class_number != sentiment_val:\n",
    "        jac = 0\n",
    "    return jac, filtered_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is Starting for fold=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c9d4a225bd4e52aebdedf5bf714fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=275.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c43ff539c540f596085c2969269a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.16344227178424245\n",
      "Jaccard Score = 0.16344227178424245\n",
      "Validation score improved (-inf --> 0.16344227178424245). Saving model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7feafcf080694269ad1e5c1e2c64c145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=275.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d85452f2cef14a4d817a20c4ea7600d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.188386346838675\n",
      "Jaccard Score = 0.188386346838675\n",
      "Validation score improved (0.16344227178424245 --> 0.188386346838675). Saving model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01aa84f9a1e14259988e43da0fec64e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=275.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "080f0fef812a4498afa2580a58ad2373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.1883263529604993\n",
      "Jaccard Score = 0.1883263529604993\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02bbdf258c0494984db5422ffac77ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=275.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f005df6098404e8b8acd280164724a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.1899340970682434\n",
      "Jaccard Score = 0.1899340970682434\n",
      "Validation score improved (0.188386346838675 --> 0.1899340970682434). Saving model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aef40f6063046f9beb66da7e33bd8b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=275.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31ca51eb85d4e37b39f7b444c5d07aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.19114621828036463\n",
      "Jaccard Score = 0.19114621828036463\n",
      "Validation score improved (0.1899340970682434 --> 0.19114621828036463). Saving model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca582c3dd2c4534b67fd82731aff1ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=275.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185f1013192b47258a53c17677e309d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.1911462182803646\n",
      "Jaccard Score = 0.1911462182803646\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14750630348b4720b48c60cee7c675d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=275.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c92ccbd80054ccfb63fee58c8f233af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.19114621828036465\n",
      "Jaccard Score = 0.19114621828036465\n",
      "EarlyStopping counter: 2 out of 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c23409aedb043ee8a2208f0e1e4542b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=275.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49bee21e63c4c0ba3e50a0a73a83cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.19114621828036465\n",
      "Jaccard Score = 0.19114621828036465\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "class config:\n",
    "    MAX_LEN = 128\n",
    "    TRAIN_BATCH_SIZE = 8\n",
    "    VALID_BATCH_SIZE = 4\n",
    "    EPOCHS = 5\n",
    "    BERT_PATH = \"../input/bert-base-uncased/\"\n",
    "    MODEL_PATH = \"model.bin\"\n",
    "    TRAINING_FILE = \"../input/tweet-train-folds/train_folds.csv\"\n",
    "    TOKENIZER = tokenizers.BertWordPieceTokenizer(\n",
    "        f\"{BERT_PATH}/vocab.txt\", \n",
    "        lowercase=True\n",
    "    )\n",
    "    \n",
    "train_dataset = TextDataset(training_data)\n",
    "\n",
    "# Instantiate DataLoader with `train_dataset`\n",
    "# This is a generator that yields the dataset in batches\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.TRAIN_BATCH_SIZE,\n",
    "    num_workers=0,\n",
    "    shuffle = True\n",
    ")\n",
    "#    shuffle = True\n",
    "valid_dataset = TextDataset(test_data)\n",
    "\n",
    "# Instantiate DataLoader with `train_dataset`\n",
    "# This is a generator that yields the dataset in batches\n",
    "valid_data_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=config.TRAIN_BATCH_SIZE,\n",
    "    num_workers=0,\n",
    "    shuffle = True\n",
    ")\n",
    "# Set device as `cuda` (GPU)\n",
    "device = torch.device(\"cuda\")\n",
    "# Load pretrained BERT (bert-base-uncased)\n",
    "model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\n",
    "# Output hidden states\n",
    "# This is important to set since we want to concatenate the hidden states from the last 2 BERT layers\n",
    "model_config.output_hidden_states = True\n",
    "# Instantiate our model with `model_config`\n",
    "model = TweetModel(conf=model_config, num_labels=len(class_list))\n",
    "# Move the model to the GPU\n",
    "model.to(device)\n",
    "\n",
    "# Calculate the number of training steps\n",
    "num_train_steps = int(len(training_data) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
    "# Get the list of named parameters\n",
    "param_optimizer = list(model.named_parameters())\n",
    "# Specify parameters where weight decay shouldn't be applied\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "# Define two sets of parameters: those with weight decay, and those without\n",
    "optimizer_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "# Instantiate AdamW optimizer with our two sets of parameters, and a learning rate of 3e-5\n",
    "optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "# Create a scheduler to set the learning rate at each training step\n",
    "# \"Create a schedule with a learning rate that decreases linearly after linearly increasing during a warmup period.\" (https://pytorch.org/docs/stable/optim.html)\n",
    "# Since num_warmup_steps = 0, the learning rate starts at 3e-5, and then linearly decreases at each training step\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=num_train_steps\n",
    ")\n",
    "\n",
    "# Apply early stopping with patience of 2\n",
    "# This means to stop training new epochs when 2 rounds have passed without any improvement\n",
    "es = utils.EarlyStopping(patience=3, mode=\"max\")\n",
    "fold = 0\n",
    "print(f\"Training is Starting for fold={fold}\")\n",
    "\n",
    "# I'm training only for 3 epochs even though I specified 5!!!\n",
    "for epoch in range(50):\n",
    "    train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n",
    "    jaccard = eval_fn(valid_data_loader, model, device)\n",
    "    print(f\"Jaccard Score = {jaccard}\")\n",
    "    es(jaccard, model, model_path=f\"model_{fold}.bin\")\n",
    "    if es.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "        \n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TweetModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (drop_out): Dropout(p=0.1, inplace=False)\n",
       "  (l0): Linear(in_features=1536, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\n",
    "model_config.output_hidden_states = True\n",
    "\n",
    "model1 = TweetModel(conf=model_config, num_labels=len(class_list))\n",
    "model1.to(device)\n",
    "model1.load_state_dict(torch.load(\"model_0.bin\"))\n",
    "model1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7eb75513f94338a0d619f7cbe4546d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=165.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final_output = []\n",
    "\n",
    "# Instantiate TweetDataset with the test data\n",
    "test_dataset = TextDataset(test_data)\n",
    "\n",
    "# Instantiate DataLoader with `test_dataset`\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=config.VALID_BATCH_SIZE,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "# Turn of gradient calculations\n",
    "with torch.no_grad():\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    # Predict the span containing the sentiment for each batch\n",
    "    for bi, d in enumerate(tk0):\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        sentiment = d[\"sentiment\"]\n",
    "        orig_selected = d[\"orig_selected\"]\n",
    "        orig_tweet = d[\"orig_tweet\"][2]\n",
    "        targets_start = d[\"targets_start\"]\n",
    "        targets_end = d[\"targets_end\"]\n",
    "        offsets = d[\"offsets\"].numpy()\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets_start = targets_start.to(device, dtype=torch.long)\n",
    "        targets_end = targets_end.to(device, dtype=torch.long)\n",
    "\n",
    "        # Predict start and end logits for each of the five models\n",
    "        outputs_start, outputs_end, outputs_class = model1(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        # Apply softmax to the predicted start and end logits\n",
    "        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "        outputs_class = [torch.softmax(i, dim=1).cpu().detach().numpy() for i in outputs_class]\n",
    "        # Convert the start and end scores to actual predicted spans (in string form)\n",
    "        for px, tweet in enumerate(orig_tweet):\n",
    "            selected_tweet = orig_selected[px]\n",
    "            tweet_sentiment = sentiment[px]\n",
    "            ont_hot_class = [np.argmax(i[px, :]) for i in outputs_class]\n",
    "            class_number = ont_hot_class.index(max(ont_hot_class))\n",
    "            if tweet_sentiment == class_number:\n",
    "                if class_number == 0:\n",
    "                    TN += 1\n",
    "                else:\n",
    "                    TP += 1\n",
    "            else:\n",
    "                if class_number == 0:\n",
    "                    FN += 1 # 원래는 양성인데 음성으로 예측\n",
    "                else:\n",
    "                    FP += 1\n",
    "                    \n",
    "            _, output_sentence = calculate_jaccard_score(\n",
    "                original_tweet=tweet,\n",
    "                target_string=selected_tweet,\n",
    "                sentiment_val=tweet_sentiment,\n",
    "                idx_start=np.argmax(outputs_start[px, :]),\n",
    "                idx_end=np.argmax(outputs_end[px, :]),\n",
    "                offsets=offsets[px],\n",
    "                class_number = class_number\n",
    "            )\n",
    "            \n",
    "            final_output.append([np.argmax(outputs_start[px, :]), np.argmax(outputs_end[px, :]), output_sentence, class_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(TP + TN)  / (TN+TP+FN+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"tw\": \"sequence 0: As the Sea of Japan warms, outbreaks of giant jellyfish are becoming nearly a yearly occurrence.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Still, the top government spokesman underscored Tokyo\\u2019s rejection of South Korea\\u2019s demand, adding, \\u201cThe Sea of Japan is the only name that has been established internationally, and there is no need or reason for changing it.\\u201d\",\n",
      "    \"keyword\": \"The Sea of Japan is the only name that has been established internationally, and there is no need or reason for changing it\",\n",
      "    \"predicted_keyword\": \"The Sea of Japan is the only name that has been established internationally, and there is no need or reason for changing it\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Survey on the historic transition of the name - Sea of Japan (Japan Sea) in maps made in countries other than Japan and the ROK revealed however that the name - \\\"East Sea\\\" was much less prevalent than other names, and the name - Sea of Japan had already been internationally recognized and firmly established by the early 19th century during which Japan had been in a state of national seclusion.\",\n",
      "    \"keyword\": \"Sea of Japan had already been internationally recognized and firmly established by the early 19th\",\n",
      "    \"predicted_keyword\": \"Sea of Japan had already been internationally recognized and firmly established by the early 19th \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 1: Hydrographic Authorities of the U.K. (since 1863), the U.S.A (since 1854), Russia and France (since each country\\u2019s Hydrographic Department began the publication of a nautical chart of the sea area around the Sea of Japan (Japan Sea)) have solely used the Sea of Japan (Japan Sea) in their nautical charts related to this sea area since their first edition.\",\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea)\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Japan did not participate in the process of the establishment of this name at all (Reference 1: The establishment of the name - Sea of Japan (Japan Sea)).\",\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea)\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea)\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The Seas of Japan and Okhotsk.\",\n",
      "    \"keyword\": \"Seas of Japan\",\n",
      "    \"predicted_keyword\": \"Seas of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Accordingly, Japan's Imperialism is not directly related to the establishment and popularization of the term \\\"Sea of Japan.\\\"\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: This is because the name - Sea of Japan (Japan Sea) was already established internationally as the sole name indicating the Sea of Japan (Japan Sea) when \\\"Limits of Oceans and Seas\\\" was published in 1928.\",\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea) was already established internationally as the sole name indicating the Sea of Japan (Japan Sea)\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) was already established internationally as the sole name indicating the Sea of Japan (Japan Sea) \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: ( Japan's Military Annexation of Takeshima (Dokdo) in 1905 ) Today, South Korea classifies the islets as a part of Ulleung County, North Gyeongsan Province, while Japan classifies them as part of Okinoshima, in Oki District, Shimane Prefecture.\",\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Economic Aspects Fisheries and mineral deposits form the main economic resources of the Sea of Japan.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: This is because the name - Sea of Japan (Japan Sea) is established in the guidelines on names of seas, published by the International Hydrographic Organization (IHO) entitled \\\"Limits of Oceans and Seas\\\" to which countries in the world refer when making nautical charts.\",\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea) is established in the guidelines on names of seas, published by the International Hydrographic Organization (IHO)\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) is established in the guidelines on names of seas, published by the International Hydrographic Organization (IHO) \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The world map is the extant oldest map describing a sea area between Eurasia and the Japanese Islands using the term \\\"Sea of Japan.\\\"\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Until the end of the 18th Century, the Sea of Japan (Japan Sea) was an unknown sea area, and the shape of the Sea of Japan (Japan Sea) seen in European maps of that area was far from the shape we know at present.\",\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea)\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: In the Sea of Japan, surveys using the latest surveying technology were carried out by explorers such as La Perouse (France), Broughton (British), and Krusenstern (Russia).\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Giant jellyfish outbreak in the Sea of Japan.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Of these maps still in existence, the term \\\"Sea of Japan\\\" was first adopted by Christopherus Blancus who made a map of Japan in 1617.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: LME 50: Sea of Japan Bordering countries: Japan, Korea, Democratic Republic of Korea LME total area: 1,054,305 km2\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The coral\\u2013eating crown\\u2013of\\u2013thorns starfish (Acanthaster planci) has already expanded its range into the Sea of Japan,24,25 causing declines in reef fish25 and affecting tourism throughout the region.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: If \\\"East Sea\\\" were used alongside the name - Sea of Japan (Japan Sea) in \\\"Limits of Oceans and Seas\\\" which is the guideline regarding names on sea, it would surely lead to confusion among navigators.\",\n",
      "    \"keyword\": \"If \\\"East Sea\\\" were used alongside the name - Sea of Japan (Japan Sea) in \\\"Limits of Oceans and Seas\\\" which is the guideline regarding names on sea, it would surely lead to confusion\",\n",
      "    \"predicted_keyword\": \"If \\\"East Sea\\\" were used alongside the name - Sea of Japan (Japan Sea) in \\\"Limits of Oceans and Seas\\\" which is the guideline regarding names on sea, it would surely lead to confusion \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Against this historical background, the term \\\"Sea of Japan\\\" began to spread in Europe, the U.S., Japan and other areas.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: After that, the term \\\"North Sea of Japan\\\" was adopted by Sir Robert Dudley in 1646, while the term \\\"Sea of Japan\\\" was adopted by Vincenzo Maria Coronelli in 1690, then by Nicolaas Witsen in 1692, and gradually started to gain currency in the 17th and 18th centuries.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The Sea of Japan is a marginal sea set off from the Pacific Ocean by the Japanese Archipelago, the Korean Peninsula, the Sakhalin Islands, and Russia.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: (Reference 2: The name - Sea of Japan (Japan Sea) established by \\\"Limits of Oceans and Seas\\\" ).\",\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea) established by \\\"Limits of Oceans and Seas\\\"\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) established by \\\"Limits of Oceans and Seas\\\" \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Bottom deposits in the Sea of Japan indicate that sediments of continental origin, such as mud, sand, gravel, and fragments of rock, exist down to depths of some 650 to 1,000 feet (200 to 300 metres); hemipelagic sediments (i.e., half of oceanic origin), mainly consisting of blue mud rich in organic matter, are found down to depths of about 1,000 to 2,600 feet (300 to 800 metres); and deeper pelagic sediments, consisting of red mud, are found down to the sea\\u2019s greatest depths.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The Japanese fishing industry provides a critical local food source: per capita consumption of fish and seafood in Japan is nearly three times the world average.3 The Japanese industry also accounted for nearly 5 percent of fish catches globally in 2009.2 As the Sea of Japan warms, however, the ecosystems supporting its productive fisheries are changing.5,18,24,25\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Therefore, it is no coincidence that the term \\\"Sea of Japan\\\" appeared at this time.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: a map of Dokdo Takeshima \\ub3c5\\ub3c4 \\u305f\\u3051\\u3057\\u307e \\u7368\\u5cf6 \\u7af9\\u5cf6Dokdo Island is composed mainly of two islets, 150 meters apart (Seodo and Dongdo in Korean, Nishi-jima and Higashi-jima in Japanese) both literally meaning western island and eastern island respectively).\",\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: While the precise cause of the outbreaks is not yet certain, warmer conditions are linked to increases in the size of jellyfish populations.12,13,14,15 From 1976 to 2000, temperatures in the Yellow Sea\\u2014 likely spawning ground for N. nomurai jellyfish\\u2014rose by 3.1\\u00b0 F (1.7\\u00b0 C).16 In the Sea of Japan itself, winter sea surface temperatures have increased by 2.9\\u20134.3\\u00b0 F (1.6\\u20132.4\\u00b0 C) over the last century.17,18 Warming seas may also be exacerbating jellyfish outbreaks already linked to nutrient loading, coastal development, and overfishing.5,32 For instance, overfishing species that prey on jellyfish removes a critical check on the jellyfish population, creating an opportunity for these strong survivors to multiply.32\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"0: While the precise cause of the outbreaks is not yet certain, warmer conditions are linked to increases in the size of jellyfish populations.12,13,14,15 From 1976 to 2000, temperatures in the Yellow Sea\\u2014 likely spawning ground for N. nomurai jellyfish\\u2014rose by 3.1\\u00b0 F (1.7\\u00b0 C).16 In the Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Further surveys into the Sea of Japan (Japan Sea) were conducted by European cartographers, explorers and navigators succeedingly.\",\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea)\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: ( Korea's Political Situation in 1905 When Japan Seized Dokdo \\u2013 Takeshima Island ) To the right: Korean citizens protest Japan's claim to Dokdo accusing Japan's Government of whitewashing her past aggressions and colonial past.\",\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Further, as stated under item4 the UN has already officially confirmed its policy requiring the use of Sea of Japan as the standard geographical term in all official UN publications.\",\n",
      "    \"keyword\": \"officially confirmed its policy requiring the use of Sea of Japan as the standard geographical term in all official UN publications\",\n",
      "    \"predicted_keyword\": \"officially confirmed its policy requiring the use of Sea of Japan as the standard geographical term in all official UN publications\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The pros and cons of the term \\\"Sea of Japan\\\" were widely reported in the media this past August and September.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: If Japan had any intention to actively propagate the name - Sea of Japan (Japan Sea) worldwide, it would not have had any concern about the political and diplomatic problems regarding the names and limits of seas as such, nor objected the proposal to prepare the guidelines, even temporarily.\",\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea) worldwide\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) worldwide\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Kunyu Wanguo Quantu was also introduced to Japan early on, but the term \\\"Sea of Japan\\\" was not established right away.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: In other words, there is the historic fact that the ROK did not make an objection against the name - Sea of Japan (Japan Sea) even after World War II.\",\n",
      "    \"keyword\": \"there is the historic fact that the ROK did not make an objection against the name - Sea of Japan (Japan Sea) even after World War II.\",\n",
      "    \"predicted_keyword\": \"there is the historic fact that the ROK did not make an objection against the name - Sea of Japan (Japan Sea) even after World War II.\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: By the beginning of 19th Century, the name - Sea of Japan (Japan Sea) became established internationally as the name indicating this sea area.\",\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea) became established internationally as the name\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) became established internationally as the name \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: What the Future Holds The rise in water temperature in the Sea of Japan and associated ecosystem migrations are occurring in the context of a global rise in sea surface temperature of 0.18\\u00b0 F (0.1\\u00b0 C) per decade since 1901.27 Continued burning of oil, gas, and trees\\u2014and a mid\\u2013level scenario for future heat\\u2013trapping emissions\\u2014are projected to cause temperatures in the Sea of Japan to rise by another 5\\u20137\\u00b0F (3\\u20134\\u00b0C) by the end of this century.28,29\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 1: Until the end of the 18th Century, the Sea of Japan (Japan Sea) was an unknown sea area, and the shape of the Sea of Japan (Japan Sea) seen in European maps of that area was far from the shape we know at present.\",\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea)\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: (Reference 1:The establishment of the name - Sea of Japan (Japan Sea) )\",\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea)\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The Seas of Japan and Okhotsk.\",\n",
      "    \"keyword\": \"Seas of Japan\",\n",
      "    \"predicted_keyword\": \"Seas of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: In short, the contentious point of this issue is how the history of the term \\\"Sea of Japan\\\" is to be treated.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Hydrographic Authorities of the U.K. (since 1863), the U.S.A (since 1854), Russia and France (since each country\\u2019s Hydrographic Department began the publication of a nautical chart of the sea area around the Sea of Japan (Japan Sea)) have solely used the Sea of Japan (Japan Sea) in their nautical charts related to this sea area since their first edition.\",\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea))\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea)) \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Japan has argued \\u201cJapan Sea\\u201d is the only internationally established name and there is no need to change it.\",\n",
      "    \"keyword\": \"\\u201cJapan Sea\\u201d is the only internationally established name\",\n",
      "    \"predicted_keyword\": \"\\u201cJapan Sea\\u201d is the only internationally established name \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The name - Sea of Japan (Japan Sea) became recognized internationally by the early 19th Century during which Japan had been in a state of national seclusion.\",\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea) became recognized internationally by the early 19th Century\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) became recognized internationally by the early 19th Century \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The IHO has consistently used Sea of Japan (Japan Sea) as the name for the sea area concerned since the first edition (1928) of \\\"Limits of Oceans and Seas\\\".\",\n",
      "    \"keyword\": \"IHO has consistently used Sea of Japan (Japan Sea)\",\n",
      "    \"predicted_keyword\": \"IHO has consistently used Sea of Japan (Japan Sea) \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: South Korea has urged the IHO to change the name to \\u201cEast Sea\\u201d or use both.\",\n",
      "    \"keyword\": \"East Sea\",\n",
      "    \"predicted_keyword\": \"East Sea\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Climate The Sea of Japan influences the climate of Japan because of its relatively warm waters; evaporation is especially noticeable in winter, when an enormous quantity of water vapour rises in the region between the cold, dry polar air mass and the warm, moist tropical air mass.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Dokdo \\u2013 Takeshima Island A Brief Introduction to Korea's Dokdo [Takeshima] Island map showing distances to Dokdo TakeshimaDokdo Island (also called Liancourt Rocks by some nations and Takeshima by Japan) is 215 kms from mainland Korea and 250 kms from Japan proper.\",\n",
      "    \"keyword\": \"[Takeshima] Island map showing distances to Dokdo TakeshimaDokdo Island (also called Liancourt Rocks by some nations and Takeshima by Japan)\",\n",
      "    \"predicted_keyword\": \"[Takeshima] Island map showing distances to Dokdo TakeshimaDokdo Island (also called Liancourt Rocks by some nations and Takeshima by Japan) \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The Name - Sea of Japan (Japan Sea)\",\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea)\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea)\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: But from the end of the 18th century onward, the name of this sea area began to be standardized as the Sea of Japan.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: After that, the term \\\"Sea of Japan\\\" eventually started to be used mostly in maps for the study of Western sciences in Dutch.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The name - Sea of Japan (Japan Sea) is the only internationally established name for the sea area concerned.\",\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea) is the only internationally established name\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) is the only internationally established name \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Inflow of water takes place primarily through the eastern and western channels of the Korea Strait; the inflow of water into the Sea of Japan through the narrow and shallow Tatar Strait is negligible, while through the Tsugaru and La Perouse straits the water flows out of the Sea of Japan.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Discovery and naming of the Sea of Japan\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Tidal difference between the Sea of Japan and Pacific Ocean\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Further, these resolutions presume that the geographical feature concerned is under the sovereignty of two or more countries, such as in the case of a bay or strait, and does not apply to the high seas such as with the Sea of Japan.\",\n",
      "    \"keyword\": \"seas such as with the Sea of Japan\",\n",
      "    \"predicted_keyword\": \"seas such as with the Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Regarding the name - Sea of Japan (Japan Sea), Krusenstern described, based on an accurate map, that \\\"People also call this sea area the Sea of Korea, but because only a small part of this sea touches the Korean coast, it is appropriate to name it \\\"the Sea of Japan\\\" (Journey around the World in the Years 1803, 1804, 1805, and 1806 Volume 3).\",\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea),\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea), \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Because of the very fact that the Sea of Japan was \\\"discovered\\\" through such awareness of geographical features, conditions were ripe to give a name to the closed sea.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The Sea of Japan is home to one of the world's most productive fisheries, accounting for nearly 5 percent of the global fish catch in 2009.2 Japan maintains one of the world's largest fishing fleets, and annual per capita fish and seafood consumption in the country is 134 pounds (60.8 kilograms)\\u2014more than three times the world average.3\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: However, it was not published due to the lack of consensus among the member states for areas other than Sea of Japan.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: This demonstrates that there is no UN or IHO resolution recommending the use of \\\"East Sea\\\" together with Sea of Japan.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Reference 2: The name - Sea of Japan (Japan Sea) established by \\\"Limits of Oceans and Seas\\\"\",\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea) established\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) established \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The Japanese Government's survey of historical maps confirms that the name Sea of Japan was already prevalent at the early 19th century.\",\n",
      "    \"keyword\": \"confirms that the name Sea of Japan was already prevalent at the early 19th century\",\n",
      "    \"predicted_keyword\": \"confirms that the name Sea of Japan was already prevalent at the early 19th century\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The Sea of Japan has no large islands, bays or capes.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The Japan Coast Guard, in cooperation with the Ministry of Foreign Affairs of Japan, emphatically refutes the ROK's groundless assertions based on the historic facts and evidence under the firm principle that the name - Sea of Japan (Japan Sea) has been the only internationally established name and will call for better understanding of the issue and support for the position of Japan from the international community including the IHO.\",\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea) has been the only internationally established name\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) has been the only internationally established name \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Takeshima Day Demonstration in Japan Turns Ugly\",\n",
      "    \"keyword\": \"Takeshima Day\",\n",
      "    \"predicted_keyword\": \"Takeshima Day \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Before 2002, massive outbreaks of the giant jellyfish Nemopilema nomurai occurred about every 40 years.6,7,8 Since 2002, outbreaks have occurred nearly every year, crippling commercial fisheries.4,5 Reef\\u2013building corals in the region have rapidly expanded their range northward in response to warming temperatures.18 At the same time, the coral\\u2013eating crown-of-thorns starfish has invaded the region.24,25 Given a mid\\u2013level scenario for future heat\\u2013trapping emissions, temperatures in the Sea of Japan are projected to rise by 5\\u20137\\u00b0 F ( 3\\u20134\\u00b0 C) by the end of the century.28,29 This warming will likely create more favorable conditions for invasive species.12,14 Details\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: | MARITIME SELF-DEFENSE FORCE / VIA KYODO KYODO SHARE Nov 17, 2020 The International Hydrographic Organization has tentatively approved a proposal that supports the exclusive use of the name \\u201cJapan Sea,\\u201d or the Sea of Japan, in referring to the waters between Japan and the Korean Peninsula, the Japanese government said Tuesday.\",\n",
      "    \"keyword\": \"Japan Sea\",\n",
      "    \"predicted_keyword\": \"Japan Sea\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Recent state of affairs surrounding the term \\\"Sea of Japan\\\"\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The name - Sea of Japan (Japan Sea) has been consistently and solely used in \\\"Limits of Oceans and Seas\\\" since its first publication in 1928.\",\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea) has been consistently and solely used in \\\"Limits of Oceans and Seas\\\"\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) has been consistently and solely used in \\\"Limits of Oceans and Seas\\\" \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The debate started when the International Hydrographic Organization (IHO), which formulates international guidelines for nautical charts, accepted Korean allegations and proposed that the term \\\"Sea of Japan,\\\" which has been used internationally, be scratched.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The Sea of Japan is a classic semienclosed sea, since its connections with adjacent bodies of water are greatly restricted by the narrow straits.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: This trend resulted from the widespread adoption of the term \\\"Sea of Japan\\\" in Western maps drawn from the end of the 18th century onward, which were used as sources for the former maps.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The missionary spread the term \\\"Sea of Japan\\\" to Europe with a new awareness of geographical features.\",\n",
      "    \"keyword\": \"The missionary spread the term \\\"Sea of Japan\\\" to Europe with a new awareness of geographical features\",\n",
      "    \"predicted_keyword\": \"The missionary spread the term \\\"Sea of Japan\\\" to Europe with a new awareness of geographical features\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Why the East Sea not the Sea of Japan?\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: ( Korea's 1900 Incorporation of Dokdo Island ) Japanese claims come from seventeenth century records ( Dokdo \\u2013 Takeshima and Japan's Historical Territorial Limits ), as well as a \\\"terra nullius\\\" incorporation in 1905.\",\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Consequently, the ROK's assertion that the name Sea of Japan became widespread as a result of \\\"expansionism and colonial rule\\\" in the latter half of the 19th century is wholly invalid.\",\n",
      "    \"keyword\": \"the ROK's assertion that the name Sea of Japan became widespread as a result of \\\"expansionism and colonial rule\\\"\",\n",
      "    \"predicted_keyword\": \"the ROK's assertion that the name Sea of Japan became widespread as a result of \\\"expansionism and colonial rule\\\" \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: World Cup>Past Tournaments>2002 Japan-Korea>Overview\",\n",
      "    \"keyword\": \"2002 Japan-Korea\",\n",
      "    \"predicted_keyword\": \"2002 Japan-Korea\",\n",
      "    \"class\": 1,\n",
      "    \"predicted_class\": 1\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: In other words, the term \\\"Sea of Japan\\\" was \\\"imported.\\\"\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 1: Regarding the name - Sea of Japan (Japan Sea), Krusenstern described, based on an accurate map, that \\\"People also call this sea area the Sea of Korea, but because only a small part of this sea touches the Korean coast, it is appropriate to name it \\\"the Sea of Japan\\\" (Journey around the World in the Years 1803, 1804, 1805, and 1806 Volume 3).\",\n",
      "    \"keyword\": \"appropriate to name it \\\"the Sea of Japan\\\"\",\n",
      "    \"predicted_keyword\": \"appropriate to name it \\\"the Sea of Japan\\\" \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The middle water\\u2019s origin is in the intermediate water layers of the Kuroshio off the coast of Kyushu that enter the Sea of Japan via the Tsushima Current during the winter and spring.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Japan during the Edo Period (1603-1867) had an isolationist policy, and was unable to exercise any influence to establish the name Sea of Japan.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Reference 1: The establishment of the name - Sea of Japan (Japan Sea)\",\n",
      "    \"keyword\": \"The establishment of the name - Sea of Japan (Japan Sea)\",\n",
      "    \"predicted_keyword\": \"The establishment of the name - Sea of Japan (Japan Sea)\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: In addition, Japan did not undertake any kind of demarche in order to have the name Sea of Japan put in \\\"Limits of Oceans and Seas\\\" in its first edition.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Part of a Larger Pattern The recent jellyfish outbreaks are part of a larger pattern of changes in marine ecosystems in the Sea of Japan.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The point of its allegations is that the term \\\"Sea of Japan\\\" was established when Japan colonized the Korean Peninsula, therefore this kind of imperialistic term should be abolished and the term should be renamed \\\"East Sea,\\\" which is used in Korea, or at least both terms should be used.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: To the east it is also connected with the Inland Sea of Japan by the Kanmon Strait and to the Pacific by the Tsugaru Strait.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: \\u201cIn papers, \\u2018Japan Sea\\u2019 will remain.\",\n",
      "    \"keyword\": \"Japan Sea\",\n",
      "    \"predicted_keyword\": \"Japan Sea\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Photographs of Dokdo Island From Korea's Ulleungdo Island Dokdo Takeshima on a clear day from Korea's UlleungdoKorea's Dokdo from Ulleungdo \\ub3c5\\ub3c4 \\u305f\\u3051\\u3057\\u307e \\u7368\\u5cf6 \\u7af9\\u5cf6 The above images are pictures of Dokdo Island from Korea's Ulleungdo Island.\",\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima \",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: (2) d.) , what is notable is that the report states that \\\"there was a rapid increase in the use of the name Sea of Japan from the 19th century (1830 onward)\\\" (italics added).\",\n",
      "    \"keyword\": \"was a rapid increase in the use of the name Sea of Japan\",\n",
      "    \"predicted_keyword\": \"was a rapid increase in the use of the name Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: 1986\\tCompilation of the draft fourth edition of \\\"Limits of Oceans and Seas\\\"\\tThe name Sea of Japan (Japan Sea) was described solely in the draft.\",\n",
      "    \"keyword\": \"The name Sea of Japan (Japan Sea) was described solely in the draft\",\n",
      "    \"predicted_keyword\": \"The name Sea of Japan (Japan Sea) was described solely in the draft\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 1: The IHO guidelines, \\u201cLimits of Oceans and Seas,\\u201d have used only \\u201cJapan Sea\\u201d for the Sea of Japan since the first edition was published in 1929.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: South Korea has been calling for the adoption of the alternative name \\u201cEast Sea,\\u201d claiming the name \\u201cJapan Sea\\u201d came into use under Japan\\u2019s 1910-1945 colonial rule of the Korean Peninsula.\",\n",
      "    \"keyword\": \"Japan Sea\",\n",
      "    \"predicted_keyword\": \"Japan Sea\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Secondly, whilst the sea level of the Pacific Ocean fluctuates over two meters between high and low tides, the Sea of Japan only fluctuates around 30cm.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Trade across the Sea of Japan is only moderate, since most of Japan\\u2019s trade is with countries not bordering the sea.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Popularization of the term \\\"Sea of Japan\\\"\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: At that time, terms named after more specific place names, such as the Sea of Japan, or the Sea of Korea, were not yet used.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: In the geologic past, the Sea of Japan was a landlocked body of water.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Tide of the Sea of Japan and Funagoya Traditional Boat Houses\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The ROK did not make an objection against the name - Sea of Japan (Japan Sea).\",\n",
      "    \"keyword\": \"ROK did not make an objection against the name - Sea of Japan (Japan Sea).\",\n",
      "    \"predicted_keyword\": \"ROK did not make an objection against the name - Sea of Japan (Japan Sea).\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The Limits of Oceans and Seas, an IHO guideline for countries to use as a reference in drawing up ocean maps, has described the area as \\u201cJapan Sea\\u201d since its first edition in 1928.\",\n",
      "    \"keyword\": \"Japan Sea\",\n",
      "    \"predicted_keyword\": \"Japan Sea\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: When Japan's Shimane prefecture announced a \\\"Takeshima Day\\\" in 2005, ( Japan's Legacy of Expansionism Continues ) Koreans reacted with demonstrations and protests throughout the country, extreme examples of which included a mother and son slicing off their own fingers, and a man who self-immolated.\",\n",
      "    \"keyword\": \"Takeshima Day\",\n",
      "    \"predicted_keyword\": \"Takeshima Day\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Toward that purpose, the Japanese Government should patiently explain the historical facts about the term \\\"Sea of Japan\\\" while giving due consideration to the circumstances of the time when the international agreement was concluded.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The ROK, however, had not raised this issue in \\\"United Nations Conference on Standardization of Geographical Names\\\" or \\\"the International Hydrographic Organization\\\" (IHO) until the beginning of the 1990s and the ROK used the name of Sea of Japan (Japan Sea) even in its own nautical charts.\",\n",
      "    \"keyword\": \"ROK used the name of Sea of Japan (Japan Sea)\",\n",
      "    \"predicted_keyword\": \"ROK used the name of Sea of Japan (Japan Sea) \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The IHO guidelines, \\u201cLimits of Oceans and Seas,\\u201d have used only \\u201cJapan Sea\\u201d for the Sea of Japan since the first edition was published in 1929.\",\n",
      "    \"keyword\": \"Japan Sea\",\n",
      "    \"predicted_keyword\": \"Japan Sea\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: This view of the traditional boat houses right on the beach in Tsuma can be thought of as a product of the small tidal range of the Sea of Japan.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: It is said that the first appearance of the name - Sea of Japan (Japan Sea) was in \\\"Kunyu Wanguo Quantu\\\" by Matteo Ricci (1602).\",\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea)\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: \\\"Limits of Oceans and Seas\\\" has consistently used the name - Sea of Japan (Japan Sea) as the name for the concerned sea area since its first edition (1928).\",\n",
      "    \"keyword\": \"has consistently used the name - Sea of Japan (Japan Sea) as the name\",\n",
      "    \"predicted_keyword\": \"has consistently used the name - Sea of Japan (Japan Sea) as the name \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The Japanese Government opposes this argument, and says that the term \\\"Sea of Japan\\\" was established before the colonization of the Korean Peninsula, so it is nothing to do with imperialism.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Countries such as the United Kingdom and the United States use the name - Sea of Japan (Japan Sea) for making their nautical chart.\",\n",
      "    \"keyword\": \"use the name - Sea of Japan (Japan Sea) for making their nautical chart\",\n",
      "    \"predicted_keyword\": \"use the name - Sea of Japan (Japan Sea) for making their nautical chart\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: In the map Kunyu Wanguo Quantu drawn up by an Italian missionary priest of the Jesuit Order Matteo Ricci 400 years ago, the \\\"Sea of Japan\\\" was written down in kanji already, subsequently that name was imported into and pervaded Japan from the West.\",\n",
      "    \"keyword\": \"In the map Kunyu Wanguo Quantu drawn up by an Italian missionary priest of the Jesuit Order Matteo Ricci 400 years ago, the \\\"Sea of Japan\\\" was written down in kanji already\",\n",
      "    \"predicted_keyword\": \"In the map Kunyu Wanguo Quantu drawn up by an Italian missionary priest of the Jesuit Order Matteo Ricci 400 years ago, the \\\"Sea of Japan\\\" was written down in kanji already\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: \\ub3c5\\ub3c4 \\u305f\\u3051\\u3057\\u307e \\u7368\\u5cf6 \\u7af9\\u5cf6 Kim Seong Do and Kim Shin Yeol with Steve Barber on Dokdo Above left: Dokdo-Takeshima.com webmaster, Steven J. Barber, with Dokdo Island's only residents Kim Seong Do and wife Kim Shin Yeol on a recent research trip to Dokdo.\",\n",
      "    \"keyword\": \"Takeshima\",\n",
      "    \"predicted_keyword\": \"Takeshima\",\n",
      "    \"class\": 3,\n",
      "    \"predicted_class\": 3\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The International Hydrographic Organization (IHO) adopted the term \\\"Sea of Japan\\\" as an international term in 1929 based on this historical background and the international situation.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Sea of Japan\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: 1928\\tPublication of the first edition of the \\\"Limits of Oceans and Seas\\\"\\tPublished by \\\"the International Hydrographic Bureau\\\".The Sea of Japan (Japan Sea) was used solely in the guideline.\",\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea) was used solely in the guideline\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) was used solely in the guideline\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: 1937\\tPublication of the second edition of the \\\"Limits of Oceans and Seas\\\"\\tThe name - Sea of Japan (Japan Sea) was used solely in the guideline.\",\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea) was used solely in the guideline\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) was used solely in the guideline\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: ROK's challenge without solid foundation (1) In recent years, a few countries have suddenly begun to challenge the sole use of the name Sea of Japan.\",\n",
      "    \"keyword\": \"suddenly begun to challenge the sole use of the name Sea of Japan\",\n",
      "    \"predicted_keyword\": \"suddenly begun to challenge the sole use of the name Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: The name - Sea of Japan (Japan Sea)\",\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea)\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea)\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: 1953\\tPublication of the third edition of the \\\"Limits of Oceans and Seas\\\"\\tThe name - Sea of Japan (Japan Sea) was used solely in the guideline.\",\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea) was used solely in the guideline\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) was used solely in the guideline\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: ), and can be interpreted as affirming that the name Sea of Japan was in widespread use well before Japan's colonial rule over the Korean Peninsula.\",\n",
      "    \"keyword\": \"can be interpreted as affirming that the name Sea of Japan was in widespread use well\",\n",
      "    \"predicted_keyword\": \"can be interpreted as affirming that the name Sea of Japan was in widespread use well \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: In addition, maps and books written in European languages by those related to the Society of Jesus were circulated, and maps with the term \\\"Sea of Japan\\\" started to be drawn in Europe based on them.\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"0: In addition, maps and books written in European languages by those related to the Society of Jesus were circulated, and maps with the term \\\"Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: No.55 November 20, 2002 The History of the Name of the Sea of Japan Hiroo Aoyama Associate Professor, National Museum of Japanese History Selected Papers No.5 Captain, Go Back to Your Old School - Share the Ship and the Ocean with Kids - Keiichi Sawayama Japan Captains' Association Shibukawa Seaside Cleaning Robot Contest - Potential Return to a Picturesque Beach - Minoru Koyama Department of Mechanics, Okayama Shoka University High School The History of the Name of the Sea of Japan\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Submarine topography of the Sea of Japan This is due to the particular characteristics of the Sea of Japan.\",\n",
      "    \"keyword\": \"Sea of Japan This is due to the particular characteristics of the Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan This is due to the particular characteristics of the Sea of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: This clearly shows the fallacy of the ROK's assertion that the name Sea of Japan was the result of the Japanese policy of expansionism and colonial rule (item 2.\",\n",
      "    \"keyword\": \"This clearly shows the fallacy of the ROK's assertion that the name Sea of Japan\",\n",
      "    \"predicted_keyword\": \"This clearly shows the fallacy of the ROK's assertion that the name Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Sea Of Japan\",\n",
      "    \"keyword\": \"Sea Of Japan\",\n",
      "    \"predicted_keyword\": \"Sea Of Japan\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Japan\\u2019s position for the name - Sea of Japan (Japan Sea) (Web page of the Ministry of Foreign Affairs)\",\n",
      "    \"keyword\": \"Sea of Japan (Japan Sea)\",\n",
      "    \"predicted_keyword\": \"Sea of Japan (Japan Sea) \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Sea of Japan Click here to choose an LME\",\n",
      "    \"keyword\": \"Sea of Japan\",\n",
      "    \"predicted_keyword\": \"Sea of Japan \",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n",
      "{\n",
      "    \"tw\": \"sequence 0: Even though the name \\\"East Sea\\\" is in use today in the ROK, it is obvious that the name \\\"East Sea\\\" is nothing but a local name used only in the ROK,and that the name Sea of Japan is the only name that has been in wide use internationally for a long period of time.\",\n",
      "    \"keyword\": \"the name Sea of Japan is the only name that has been in wide use internationally for a long period of time.\",\n",
      "    \"predicted_keyword\": \"the name Sea of Japan is the only name that has been in wide use internationally for a long period of time.\",\n",
      "    \"class\": 2,\n",
      "    \"predicted_class\": 2\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "for i in range(0, len(test_dataset)):\n",
    "    if test_dataset[i]['sentiment'] !=0 and test_dataset[i]['sentiment'] != 0:\n",
    "        temp = {\n",
    "            \"tw\": test_dataset[i]['orig_tweet'][2],\n",
    "            \"keyword\": test_dataset[i]['orig_selected'],\n",
    "            \"predicted_keyword\": (final_output[i][2] if final_output[i][3] != 0 else \"\"),\n",
    "            \"class\": test_dataset[i]['sentiment'],\n",
    "            \"predicted_class\": final_output[i][3]\n",
    "        }\n",
    "        print(json.dumps(temp, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
