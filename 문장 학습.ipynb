{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 승화 문서에서의 표기 오류 검출\n",
    "\n",
    "BERT(Bidirectional Encoder Representations from Transformers)는 구글이 개발한 사전훈련된(pre-training) 모델입니다. 이 모델은 위키피디아같은 텍스트 코퍼스(말뭉치)를 사용하여 미리 학습되었다는 특징이 있습니다. 그리고 BERT의 특성으로 단어를 학습할 때 문맥을 함께 고려하기때문에 언어의 패턴을 이해한 모델이 만들어집니다.\n",
    "\n",
    "이를 기반으로 새로운 문제에 적용하는 전이학습(transfer learning)을 수행할 수 있습니다. 미리 학습된 모델을 사용하기 때문에 적은 데이터로도 빠르게 학습이 가능하다는 이점이 있습니다.\n",
    "\n",
    "따라서 해당 모델을 기반으로 문서에서의 잘못된 표기 오류를 검출하는 알고리즘을 개발하였습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목표\n",
    "\n",
    "웹사이트나 텍스트 문서는 긴 여러개의 문장으로 이루어져있습니다. 각 문장이 잘못되었는지를 검사하고 잘못된 경우 잘못된 표현이 어디에 있는지 정확한 위치를 예측하여 알려주는 것이 저희 모델의 최종 목표입니다.\n",
    "\n",
    "따라서 저희는 BERT 모델에 linear regression를 적용한 네트워크 모델을 사용할 예정입니다. Output의 [CLS] 토큰을 통해 문장의 표기 오류를 분류하고 linear regression을 통해 그 위치를 예측할 것입니다.\n",
    "\n",
    "또한 표기 오류가 있지만 문제가 되지 않는 경우가 있습니다.\n",
    "\n",
    "<pre>\n",
    "한국 옆에는 작은 섬이 있는데 이것은 다케시마라고 불리기도 한다. \n",
    "그러나 이것은 잘못된 것으로 올바른 표기는 독도이다.\n",
    "</pre>\n",
    "\n",
    "문장 단위로 표기 오류를 검출한다면 인공지능은 표기 오류 결과로 \"다케시마\"를 지목할 것입니다.\n",
    "그러나 전체적인 문맥을 보면 해당 문장은 잘못된 사례를 이야기 해줄 뿐, 오류가 있는 문장이라고 말을 할수는 없습니다.\n",
    "\n",
    "따라서 BERT 모델에 주변 문장을 함께 학습시키는 모델을 구상하였습니다.\n",
    "**학습 예시**\n",
    "<pre>\n",
    "[index-2] None\n",
    "[index-1] None\n",
    "[판단할 문장] 한국 옆에는 작은 섬이 있는데 이것은 다케시마라고 불리기도 한다. \n",
    "[index+1] 그러나 이것은 잘못된 것으로 올바른 표기는 독도이다.\n",
    "[index+2]\n",
    "</pre>\n",
    "\n",
    "이 알고리즘을 이용해 sentence window를 슬라이딩 시키며 학습을 진행하면 문맥을 함께 고려하는 모델을 만들 수 있을 것이라 생각하였습니다.\n",
    "표기 오류는 오직 [판단할 문장]에 있는지만 체크하도록 학습을 시킬 것으로, 주변 문장의 표기 오류를 검출하여 모델이 혼잡해지는 경우를 최소화하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기\n",
    "API를 통해 서버로부터 사용 가능한 학습 데이터를 불러옵니다. \n",
    "\n",
    "API는 [ {no, contents, errors[code,keyword] } , ...] 형태로 데이터를 보내주도록 되어있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "class APIDokdo:\n",
    "    def __init__(self, apikey):\n",
    "      self.apiurl = \"https://api.easylab.kr\"\n",
    "      self.headers =  {'authorization': apikey}\n",
    "    def getTrainingData(self):\n",
    "        return requests.get(self.apiurl + \"/deeplearning/data/sentences\", headers=self.headers).json()['list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불러온 문서의 개수:  4\n"
     ]
    }
   ],
   "source": [
    "original_data = APIDokdo(\"godapikey12\").getTrainingData()\n",
    "print(\"불러온 문서의 개수: \", len(original_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리\n",
    "불러온 데이터를 문장 단위로 분리를 하고, 표기 오류를 검색합니다. 또한 문맥을 고려할 수 있게 5개의 문장씩 관리합니다 (0-5, 1-6, 2-7, 3-8 ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 문장\n",
      "World Cup>Past Tournaments>2002 Japan-Korea>Overview\n",
      "Error 2002 Japan-Korea\n",
      "4 문장\n",
      "The 2002 world cup was held in South Korea and Japan and certainly did not disappoint.\n",
      "5 문장\n",
      "The opening group stage consisted of 32 teams.\n",
      "6 문장\n",
      "Group E was considered the ‘group of death’ including favourites Argentina, England, Sweden and Nigeria.\n",
      "7 문장\n",
      "England drew their opening match against Sweden and followed that up with a glorious 1-0 victory over Argentina thanks to a David Beckham penalty in the first half.\n",
      "8 문장\n",
      "England and Sweden qualified for the second round.\n",
      "9 문장\n",
      "Brazil was on top form, winning their opening three matches, Ronaldo scoring four goals for Brazil.\n",
      "10 문장\n",
      "Turkey also went through that group.\n",
      "11 문장\n",
      "The surprise team of the tournament proved to be Senegal, they won their opening match against the holders France and went on to reach the quarter finals, however they lost to Turkey.\n",
      "12 문장\n",
      "The Germans started off the world cup in fine form, thrashing Saudi Arabia 8-0; they topped their group and followed that up by beating Paraguay 1-0 in the second round, beating USA in the quarter finals and then ending South Koreas world cup adventure in the semi- finals.\n",
      "14 문장\n",
      "Italy met South Korea in the second round; Korea won 2-1 after extra time, and booked a place in the quarter finals against Spain, who had beaten Republic of Ireland in the second round on penalties.\n",
      "16 문장\n",
      "England met Denmark in the second round; it proved to be England finest performance of the world cup, easily demolishing Denmark 3-0, Michael Owen featured on the score sheet.\n",
      "17 문장\n",
      "England met Brazil in the quarter finals, a 10-man Brazil side, beat England 2-1 with ease, to qualify for the semi-finals, playing against Turkey.\n",
      "19 문장\n",
      "The holders, France were knocked out in the first stage without a victory and so to were Argentina despite gaining four points from their group matches.\n",
      "20 문장\n",
      "Portugal had a disappointing world cup, also being knocked out in the opening round.\n",
      "22 문장\n",
      "Turkey defeated Senagel in the quarter finals, which then saw them progress on to meet Brazil, whom they lost to in the group phase, Ronaldo proved to be too much for the Turkish defence as he sent Brazil into the final.\n",
      "23 문장\n",
      "Germany were to strong for the hosts, South Korea in the other semi final, the Germans were in emphatic form and defeated the hosts 1-0.\n",
      "25 문장\n",
      "Once again Germany progressed to a world cup finals and Brazil were looking to banish the memories of their 3-0 defeat to France four years earlier in the world cup final.\n",
      "26 문장\n",
      "The first half witnessed chances falling for both nations, however Ronaldo was on target twice during the mid stages of the second half.\n",
      "27 문장\n",
      "Brazil comfortably beat Germany 2-0 and they were world champions for the fifth occasion.\n",
      "28 문장\n",
      "Ronaldo ended up top scorer of the competition, scoring eight goals.\n",
      "2 문장\n",
      "Alternative Titles: Donghae, East Sea, Nihon-kai, Tonghae, Yaponskoye More Sea of Japan, Japanese Nihon-kai, Russian Yaponskoye More, also called East Sea, Korean Tonghae or Donghae, marginal sea of the western Pacific Ocean.\n",
      "3 문장\n",
      "It is bounded by Japan and Sakhalin Island to the east and by Russia and Korea on the Asian mainland to the west.\n",
      "4 문장\n",
      "Its area is 377,600 square miles (978,000 square km).\n",
      "5 문장\n",
      "It has a mean depth of 5,748 feet (1,752 metres) and a maximum depth of 12,276 feet (3,742 metres).\n",
      "7 문장\n",
      "The Seas of Japan and Okhotsk.\n",
      "Error Seas of Japan\n",
      "8 문장\n",
      "The Seas of Japan and Okhotsk.\n",
      "Error Seas of Japan\n",
      "9 문장\n",
      "Encyclopædia Britannica, Inc. wave.\n",
      "10 문장\n",
      "ocean.\n",
      "11 문장\n",
      "Cresting ocean wave.\n",
      "12 문장\n",
      "Large sea waves.\n",
      "13 문장\n",
      "storm, hurricane, tropical cyclone BRITANNICA QUIZ All About Oceans and Seas Quiz What is the world’s largest inland sea?\n",
      "14 문장\n",
      "Where is the Puerto Rico Trench?\n",
      "15 문장\n",
      "Find out how deep your knowledge of oceans and seas goes with this quiz.\n",
      "16 문장\n",
      "Physical Features Physiography The sea is almost elliptical, having its major axis from southwest to northeast.\n",
      "17 문장\n",
      "To the north it is approximately bounded by latitude 51°45′ N, while to the south it is bounded by a line drawn from the Japanese island of Kyushu westward through the Gotō Islands of Japan to the South Korean island of Cheju and then northward to the Korean Peninsula.\n",
      "19 문장\n",
      "The sea itself lies in a deep basin, separated from the East China Sea to the south by the Tsushima and Korea straits and from the Sea of Okhotsk to the north by the La Perouse (or Sōya) and Tatar straits.\n",
      "20 문장\n",
      "To the east it is also connected with the Inland Sea of Japan by the Kanmon Strait and to the Pacific by the Tsugaru Strait.\n",
      "Error Sea of Japan\n",
      "22 문장\n",
      "The Sea of Japan is a classic semienclosed sea, since its connections with adjacent bodies of water are greatly restricted by the narrow straits.\n",
      "Error Sea of Japan\n",
      "23 문장\n",
      "Inflow of water takes place primarily through the eastern and western channels of the Korea Strait; the inflow of water into the Sea of Japan through the narrow and shallow Tatar Strait is negligible, while through the Tsugaru and La Perouse straits the water flows out of the Sea of Japan.\n",
      "Error Sea of Japan\n",
      "25 문장\n",
      "Get exclusive access to content from our 1768 First Edition with your subscription.\n",
      "26 문장\n",
      "Subscribe today Underwater the sea is separated into the Japan Basin to the north, the Yamato Basin to the southeast, and the Tsushima Basin to the southwest.\n",
      "27 문장\n",
      "While a narrow continental shelf fringes Siberia and the Korean Peninsula, on the Japanese side of the sea there are wider continental shelves, as well as groups of banks, troughs, and basins lying offshore.\n",
      "28 문장\n",
      "The banks lying off the coasts of Japan are divided into groups, which include Okujiri Ridge, Sado Ridge, Hakusan Banks, Wakasa Ridge, and Oki Ridge.\n",
      "30 문장\n",
      "Geology Yamato Ridge consists of granite, rhyolite, andesite, and basalt, with boulders of volcanic rock scattered on the seabed.\n",
      "31 문장\n",
      "Geophysical investigation has revealed that, while the ridge is of continental origin, the Japan Basin and the Yamato Basin are of oceanic origin.\n",
      "33 문장\n",
      "Bottom deposits in the Sea of Japan indicate that sediments of continental origin, such as mud, sand, gravel, and fragments of rock, exist down to depths of some 650 to 1,000 feet (200 to 300 metres); hemipelagic sediments (i.e., half of oceanic origin), mainly consisting of blue mud rich in organic matter, are found down to depths of about 1,000 to 2,600 feet (300 to 800 metres); and deeper pelagic sediments, consisting of red mud, are found down to the sea’s greatest depths.\n",
      "Error Sea of Japan\n",
      "35 문장\n",
      "The four straits connecting the sea either to the Pacific Ocean or to marginal seas were formed in recent geologic periods.\n",
      "36 문장\n",
      "The oldest of these are the Tsugaru and Tsushima straits, the formation of which interrupted the migration of elephants into the Japanese islands at the end of the Neogene Period (about 2.6 million years ago); the most recent is La Perouse Strait, which was formed about 60,000 to 11,000 years ago and which closed the route used by the mammoths whose fossils have been found in Hokkaido.\n",
      "38 문장\n",
      "Climate The Sea of Japan influences the climate of Japan because of its relatively warm waters; evaporation is especially noticeable in winter, when an enormous quantity of water vapour rises in the region between the cold, dry polar air mass and the warm, moist tropical air mass.\n",
      "Error Sea of Japan\n",
      "39 문장\n",
      "From December to March the prevailing northwest monsoon wind carries cold and dry continental polar air masses over the warmer waters of the sea, resulting in snow along the mountainous western coasts of Japan.\n",
      "40 문장\n",
      "In summer the southerly tropical monsoon blows from the North Pacific onto the Asian mainland, causing dense fog when its warm and moist winds blow over the cold currents that prevail over the northern part of the sea at that season.\n",
      "41 문장\n",
      "The winter monsoon brings rough seas and causes coastal erosion along the western coasts of Japan.\n",
      "43 문장\n",
      "The northern part of the sea, especially off the Siberian coast as well as in the Tatar Strait, freezes in winter; as a result of convection, melted ice feeds the cold currents in that part of the sea in spring and summer.\n",
      "45 문장\n",
      "Hydrology The waters of the sea generally circulate in a counterclockwise pattern.\n",
      "46 문장\n",
      "A branch of the Kuroshio (Japan Current), the Tsushima Current, together with its northern branch, the East Korea Warm Current, flows north, bringing warmer and more saline water before flowing into the Pacific through the Tsugaru Strait as the Tsugaru Current, as well as into the Sea of Okhotsk through the La Perouse Strait as the Sōya Current.\n",
      "47 문장\n",
      "Along the coast of the Asian mainland three cold currents—the Liman, North Korea, and Central (or Mid-) Japan Sea cold currents—bring cooler, relatively fresh water southward.\n",
      "49 문장\n",
      "The enclosed nature of the Sea of Japan and its three deep basins facilitate the formation of distinct water masses.\n",
      "50 문장\n",
      "In the summer months, when the surface is heated by higher air temperatures, a distinct thermocline separates the surface water from the middle water, which is characterized by relatively high temperatures and salinities.\n",
      "51 문장\n",
      "The middle water’s origin is in the intermediate water layers of the Kuroshio off the coast of Kyushu that enter the Sea of Japan via the Tsushima Current during the winter and spring.\n",
      "Error Sea of Japan\n",
      "52 문장\n",
      "A third water mass, the deep water, forms in February and March by the cooling of the water surface in the northern part of the sea; it is of very uniform character, with temperatures ranging narrowly between 32° and 33° F (0° and 0.5° C).\n",
      "53 문장\n",
      "The formation of these water masses, particularly the middle water, influences the concentration of dissolved oxygen, which is generally very high; and this contributes to the productive fisheries of the sea.\n",
      "55 문장\n",
      "Learn More!\n",
      "56 문장\n",
      "Economic Aspects Fisheries and mineral deposits form the main economic resources of the Sea of Japan.\n",
      "Error Sea of Japan\n",
      "57 문장\n",
      "Pelagic (oceanic) fishes include mackerel, horse mackerel, sardines, anchovies, herring, fishes of the salmon and trout family, sea bream, and squid; the demersal (sea-bottom) category includes cod, Alaskan pollack (bluefish), and Atka mackerel.\n",
      "58 문장\n",
      "Seals and whales are also to be found, as well as such crustaceans as shrimps and crabs.\n",
      "59 문장\n",
      "The fishing grounds are for the most part on the continental shelves and their adjacent waters.\n",
      "61 문장\n",
      "Herring, sardines, and bluefin tuna have traditionally been caught, but since World War II the fisheries have gradually been depleted.\n",
      "62 문장\n",
      "Squid fishing is carried on in the central part of the sea, salmon fishing in the shoal areas of the north and southwest, and crustacean trapping in the deeper parts.\n",
      "63 문장\n",
      "The sea is heavily fished by fleets from Japan, Russia, and North and South Korea.\n",
      "65 문장\n",
      "Mineral resources on or in the sea bottom include magnetite sands as well as natural gas and petroleum deposits off Japan and Sakhalin Island.\n",
      "67 문장\n",
      "Trade across the Sea of Japan is only moderate, since most of Japan’s trade is with countries not bordering the sea.\n",
      "Error Sea of Japan\n",
      "68 문장\n",
      "Consequently, the most important Japanese ports are located on its Pacific coast.\n",
      "69 문장\n",
      "Important ports of South Korea are Pusan, Ulsan, and P’ohang, located on the southeast coast of the country, but most of the shipping in and out of these ports is also destined for countries not bordering the sea.\n",
      "70 문장\n",
      "Primary Russian ports are Vladivostok, Nakhodka, and Vostochny.\n",
      "71 문장\n",
      "Vladivostok’s traffic is primarily with other Russian ports, while Nakhodka and Vostochny are international ports.\n",
      "72 문장\n",
      "Trade between countries around the sea, however, has increased, spurred by the growth of the South Korean economy and by the development of trade agreements with Russia.\n",
      "74 문장\n",
      "Study And Exploration The waters of the Sea of Japan have been traveled for centuries, and historically they have served to protect Japan from foreign invasions.\n",
      "75 문장\n",
      "European exploration in the region began in the 18th century.\n",
      "76 문장\n",
      "In the 1780s in one of the first voyages both to search for new trade opportunities and to collect scientific data, the Frenchman Jean-François de Galaup, Count de La Pérouse, traveled northward through the Sea of Japan and the strait that was named for him.\n",
      "77 문장\n",
      "Robert Broughton in 1796 also combined exploration with science on his track through the Tatar Strait and then south along the coast of the Russian Far East and around the Korean Peninsula.\n",
      "79 문장\n",
      "Other Russian voyages include those of Adam Johann Krusenstern in the Nadezhda, who explored the sea (1803–06) during his circumnavigation of the globe; the Pallada (1853–54), which monitored surface temperatures and made tidal observations; and the Vityaz (1886–89), which added to oceanographic knowledge of the sea.\n",
      "80 문장\n",
      "Also of note are the American North Pacific Exploring (1853–54) and British Challenger (1872–76) expeditions.\n",
      "82 문장\n",
      "Major research cruises by Japan began in 1928 and continued until the start of the Sino-Japanese War in 1937.\n",
      "83 문장\n",
      "After World War II the American Challenger seismic study of the sea (1951) and the work of the Baird added to scientific knowledge and stimulated Japan to reinstitute exploration and research.\n",
      "84 문장\n",
      "Most of the current oceanographic work in the sea is characterized by the use of advanced technology, by international cooperation, and by a variety of research projects.\n",
      "2 문장\n",
      "Sea Of Japan\n",
      "Error Sea Of Japan\n",
      "4 문장\n",
      "The Sea of Japan is a marginal sea set off from the Pacific Ocean by the Japanese Archipelago, the Korean Peninsula, the Sakhalin Islands, and Russia.\n",
      "Error Sea of Japan\n",
      "5 문장\n",
      "Due to its inclusion by these land masses, it has practically no tide changes.\n",
      "6 문장\n",
      "Very few rivers empty into it, and their water amounts to less than one percent of the sea’s content.\n",
      "7 문장\n",
      "The sea’s salinity is less than the adjacent Pacific as well.\n",
      "8 문장\n",
      "In the geologic past, the Sea of Japan was a landlocked body of water.\n",
      "Error Sea of Japan\n",
      "9 문장\n",
      "The Sea of Japan has no large islands, bays or capes.\n",
      "Error Sea of Japan\n",
      "2 문장\n",
      "The Name - Sea of Japan (Japan Sea)\n",
      "Error Sea of Japan (Japan Sea)\n",
      "4 문장\n",
      "1.\n",
      "5 문장\n",
      "The name - Sea of Japan (Japan Sea)\n",
      "Error Sea of Japan (Japan Sea)\n",
      "7 문장\n",
      "The name - Sea of Japan (Japan Sea) is the only internationally established name for the sea area concerned.\n",
      "Error Sea of Japan (Japan Sea) is the only internationally established name\n",
      "8 문장\n",
      "The name - Sea of Japan (Japan Sea) became recognized internationally by the early 19th Century during which Japan had been in a state of national seclusion.\n",
      "Error Sea of Japan (Japan Sea) became recognized internationally by the early 19th Century\n",
      "9 문장\n",
      "(Reference 1:The establishment of the name - Sea of Japan (Japan Sea) )\n",
      "Error Sea of Japan (Japan Sea)\n",
      "11 문장\n",
      "Countries such as the United Kingdom and the United States use the name - Sea of Japan (Japan Sea) for making their nautical chart.\n",
      "Error use the name - Sea of Japan (Japan Sea) for making their nautical chart\n",
      "12 문장\n",
      "This is because the name - Sea of Japan (Japan Sea) is established in the guidelines on names of seas, published by the International Hydrographic Organization (IHO) entitled \"Limits of Oceans and Seas\" to which countries in the world refer when making nautical charts.\n",
      "Error Sea of Japan (Japan Sea) is established in the guidelines on names of seas, published by the International Hydrographic Organization (IHO)\n",
      "13 문장\n",
      "(Reference 2: The name - Sea of Japan (Japan Sea) established by \"Limits of Oceans and Seas\" ).\n",
      "Error Sea of Japan (Japan Sea) established by \"Limits of Oceans and Seas\"\n",
      "15 문장\n",
      "2.\n",
      "16 문장\n",
      "The ROK's assertion for the name - Sea of Japan (Japan Sea)\n",
      "18 문장\n",
      "The Republic of Korea (the ROK) started to object to the name - Sea of Japan (Japan Sea) at the Sixth United Nations Conference on Standardization of Geographic Names, held in 1992 by asserting that the name - Sea of Japan (Japan Sea) became widespread as a result of Japan's \"expansionism and colonial rule\".\n",
      "19 문장\n",
      "In addition, the ROK asserted at the meetings of the IHO since 1997 that \"East Sea\" should be used together with the name - Sea of Japan in \"Limits of Oceans and Seas\".\n",
      "20 문장\n",
      "(Reference 3:Major historical background related to the IHO).\n",
      "22 문장\n",
      "The ROK, at every opportunity, has insisted afterwards that the name - Sea of Japan (Japan Sea) should be changed to \"East Sea\", or should be used together with \"East Sea\".\n",
      "24 문장\n",
      "3.\n",
      "25 문장\n",
      "The ROK's assertion in the International Hydrographic Organization\n",
      "27 문장\n",
      "The International Hydrographic Organization (IHO) is an international organization for the purpose of the utmost unification of the hydrographic charts and publications such as nautical chart and lighthouse lists.\n",
      "28 문장\n",
      "The IHO has consistently used Sea of Japan (Japan Sea) as the name for the sea area concerned since the first edition (1928) of \"Limits of Oceans and Seas\".\n",
      "Error IHO has consistently used Sea of Japan (Japan Sea)\n",
      "30 문장\n",
      "If \"East Sea\" were used alongside the name - Sea of Japan (Japan Sea) in \"Limits of Oceans and Seas\" which is the guideline regarding names on sea, it would surely lead to confusion among navigators.\n",
      "Error If \"East Sea\" were used alongside the name - Sea of Japan (Japan Sea) in \"Limits of Oceans and Seas\" which is the guideline regarding names on sea, it would surely lead to confusion\n",
      "31 문장\n",
      "This would be an act against a purpose of the IHO which is to standardize hydrographic charts and publications to the maximum extent possible, and is totally unacceptable.\n",
      "32 문장\n",
      "(Reference 4: \"East Sea\" applied to multiple sea areas).\n",
      "34 문장\n",
      "The Japan Coast Guard, in cooperation with the Ministry of Foreign Affairs of Japan, emphatically refutes the ROK's groundless assertions based on the historic facts and evidence under the firm principle that the name - Sea of Japan (Japan Sea) has been the only internationally established name and will call for better understanding of the issue and support for the position of Japan from the international community including the IHO.\n",
      "Error Sea of Japan (Japan Sea) has been the only internationally established name\n",
      "36 문장\n",
      "Link\n",
      "38 문장\n",
      "Japan’s position for the name - Sea of Japan (Japan Sea) (Web page of the Ministry of Foreign Affairs)\n",
      "Error Sea of Japan (Japan Sea)\n",
      "40 문장\n",
      "Information of the International Hydrographic Organization (IHO)\n",
      "42 문장\n",
      "Reference\n",
      "44 문장\n",
      "Reference 1: The establishment of the name - Sea of Japan (Japan Sea)\n",
      "Error The establishment of the name - Sea of Japan (Japan Sea)\n",
      "46 문장\n",
      "It is said that the first appearance of the name - Sea of Japan (Japan Sea) was in \"Kunyu Wanguo Quantu\" by Matteo Ricci (1602).\n",
      "Error Sea of Japan (Japan Sea)\n",
      "48 문장\n",
      "Until the end of the 18th Century, the Sea of Japan (Japan Sea) was an unknown sea area, and the shape of the Sea of Japan (Japan Sea) seen in European maps of that area was far from the shape we know at present.\n",
      "Error Sea of Japan (Japan Sea) was an unknown sea area, and the shape of the Sea of Japan (Japan Sea)\n",
      "50 문장\n",
      "In the late 18th Century, however, the great improvement of surveying technology such as the invention of the chronometer (a watch to measure correct time on the sea), enabled the measurement of longitude with high precision, indispensable for accurate surveying.\n",
      "51 문장\n",
      "In the Sea of Japan, surveys using the latest surveying technology were carried out by explorers such as La Perouse (France), Broughton (British), and Krusenstern (Russia).\n",
      "Error Sea of Japan\n",
      "52 문장\n",
      "Regarding the name - Sea of Japan (Japan Sea), Krusenstern described, based on an accurate map, that \"People also call this sea area the Sea of Korea, but because only a small part of this sea touches the Korean coast, it is appropriate to name it \"the Sea of Japan\" (Journey around the World in the Years 1803, 1804, 1805, and 1806 Volume 3).\n",
      "Error Sea of Japan (Japan Sea), Krusenstern described, based on an accurate map, that \"People also call this sea area the Sea of Korea, but because only a small part of this sea touches the Korean coast, it is appropriate to name it \"the Sea of Japan\"\n",
      "54 문장\n",
      "Further surveys into the Sea of Japan (Japan Sea) were conducted by European cartographers, explorers and navigators succeedingly.\n",
      "Error Sea of Japan (Japan Sea)\n",
      "55 문장\n",
      "By the beginning of 19th Century, the name - Sea of Japan (Japan Sea) became established internationally as the name indicating this sea area.\n",
      "Error Sea of Japan (Japan Sea) became established internationally as the name\n",
      "57 문장\n",
      "Hydrographic Authorities of the U.K. (since 1863), the U.S.A (since 1854), Russia and France (since each country’s Hydrographic Department began the publication of a nautical chart of the sea area around the Sea of Japan (Japan Sea)) have solely used the Sea of Japan (Japan Sea) in their nautical charts related to this sea area since their first edition.\n",
      "Error Sea of Japan (Japan Sea)) have solely used the Sea of Japan (Japan Sea)\n",
      "59 문장\n",
      "Figure: Transition of names in the maps made by countries other than Japan and the ROK\n",
      "61 문장\n",
      "The ROK asserts that the name - Sea of Japan (Japan Sea) became widespread as a result of Japanese expansionism and colonial rule.\n",
      "63 문장\n",
      "Survey on the historic transition of the name - Sea of Japan (Japan Sea) in maps made in countries other than Japan and the ROK revealed however that the name - \"East Sea\" was much less prevalent than other names, and the name - Sea of Japan had already been internationally recognized and firmly established by the early 19th century during which Japan had been in a state of national seclusion.\n",
      "Error Sea of Japan had already been internationally recognized and firmly established by the early 19th\n",
      "64 문장\n",
      "Therefore the ROK’s assertion has no solid foundation.\n",
      "66 문장\n",
      "Reference 2: The name - Sea of Japan (Japan Sea) established by \"Limits of Oceans and Seas\"\n",
      "Error Sea of Japan (Japan Sea) established\n",
      "68 문장\n",
      "\"Limits of Oceans and Seas\" has consistently used the name - Sea of Japan (Japan Sea) as the name for the concerned sea area since its first edition (1928).\n",
      "Error has consistently used the name - Sea of Japan (Japan Sea) as the name\n",
      "70 문장\n",
      "This is because the name - Sea of Japan (Japan Sea) was already established internationally as the sole name indicating the Sea of Japan (Japan Sea) when \"Limits of Oceans and Seas\" was published in 1928.\n",
      "Error Sea of Japan (Japan Sea) was already established internationally as the sole name indicating the Sea of Japan (Japan Sea)\n",
      "71 문장\n",
      "Japan did not participate in the process of the establishment of this name at all (Reference 1: The establishment of the name - Sea of Japan (Japan Sea)).\n",
      "Error Sea of Japan (Japan Sea)\n",
      "72 문장\n",
      "In addition, Japan did not undertake any kind of demarche in order to have the name Sea of Japan put in \"Limits of Oceans and Seas\" in its first edition.\n",
      "Error Sea of Japan\n",
      "73 문장\n",
      "This is clear from following Japan's remark recorded in the minutes of \"The first Supplementary International Hydrographic Conference.\n",
      "74 문장\n",
      "(April 1929)\".\n",
      "76 문장\n",
      "\"Japanese Delegates had objected to the proposal submitted to the previous Conference of 1926, since it was rather a political and diplomatic question and exceed the scope of the Conference.\n",
      "77 문장\n",
      "Nevertheless, the Japanese Delegation was in favour of a delimitation of the seas after due study of the problem in accordance with the guiding principles laid down by the Bureau\".\n",
      "79 문장\n",
      "If Japan had any intention to actively propagate the name - Sea of Japan (Japan Sea) worldwide, it would not have had any concern about the political and diplomatic problems regarding the names and limits of seas as such, nor objected the proposal to prepare the guidelines, even temporarily.\n",
      "Error Sea of Japan (Japan Sea) worldwide\n",
      "81 문장\n",
      "Figure: \"Limits of Oceans and Seas\"\n",
      "83 문장\n",
      "Reference 3: Major Historical Background Related to IHO\n",
      "85 문장\n",
      "Year\tEvent\tDetails 1921\tThe establishment of \"The International Hydrographic Bureau\" Japan became the member state of \"The International Hydrographic Bureau\"\t\"The International Hydrographic Bureau\"(IHB) is a predecessor of \"the International Hydrographic Organization\"(IHO).\n",
      "86 문장\n",
      "1928\tPublication of the first edition of the \"Limits of Oceans and Seas\"\tPublished by \"the International Hydrographic Bureau\".The Sea of Japan (Japan Sea) was used solely in the guideline.\n",
      "Error Sea of Japan (Japan Sea) was used solely in the guideline\n",
      "87 문장\n",
      "1937\tPublication of the second edition of the \"Limits of Oceans and Seas\"\tThe name - Sea of Japan (Japan Sea) was used solely in the guideline.\n",
      "Error Sea of Japan (Japan Sea) was used solely in the guideline\n",
      "88 문장\n",
      "1953\tPublication of the third edition of the \"Limits of Oceans and Seas\"\tThe name - Sea of Japan (Japan Sea) was used solely in the guideline.\n",
      "Error Sea of Japan (Japan Sea) was used solely in the guideline\n",
      "89 문장\n",
      "1957\tThe ROK became the member state of \"The International Hydrographic Bureau\" 1970\t\"The International Hydrographic Organization convention\" entered into effect\t\"The International Hydrographic Bureau\"(IHB) shifted to \"the International Hydrographic Organization\" (IHO) based on the convention.\n",
      "90 문장\n",
      "1986\tCompilation of the draft fourth edition of \"Limits of Oceans and Seas\"\tThe name Sea of Japan (Japan Sea) was described solely in the draft.\n",
      "Error The name Sea of Japan (Japan Sea) was described solely in the draft\n",
      "91 문장\n",
      "The ROK did not make an objection against the name - Sea of Japan (Japan Sea).\n",
      "Error ROK did not make an objection against the name - Sea of Japan (Japan Sea).\n",
      "92 문장\n",
      "However, it was not published due to the lack of consensus among the member states for areas other than Sea of Japan.\n",
      "Error Sea of Japan\n",
      "93 문장\n",
      "1991\tThe ROK became member state of the U.N. 1992\tThe Sixth United Nations Conference on Standardization of Geographical Names\tThe ROK officially asserted for the first time that the name - Sea of Japan (Japan Sea) be changed to \"East Sea\".\n",
      "94 문장\n",
      "1997\tThe 15th International Hydrographic Conference\tThe ROK insisted for the first time that the name of Sea of Japan (Japan Sea) in \"Limits of Oceans and Seas\" should be used　together with \"East Sea\".\n",
      "95 문장\n",
      "The ROK repeated similar assertion at the following 16th International Hydrographic Conference (2002) and the 17th International Hydrographic Conference (2007).\n",
      "96 문장\n",
      "2002\tThe IHB Circular Letter\tFollowing the ROK’s assertion, IHB sent to all member states its Circular Letter proposing that the draft of the fourth edition of \"Limits of Oceans and Seas\" which leaves blank pages related to the Sea of Japan (Japan Sea) in order to put to the vote its publication without any prior notices to Japan.\n",
      "97 문장\n",
      "Japan strongly protested against IHB’s such handling and the IHB withdrew the Circular Letter succeedingly.\n",
      "98 문장\n",
      "2012\tThe 18th International Hydrographic Conference\tThe ROK repeated the assertion that \"East Sea\" should be used together with the name - Sea of Japan (Japan Sea) in \"Limits of Oceans and Seas\".\n",
      "99 문장\n",
      "The conference agreed not to take any further decision on the guideline.\n",
      "100 문장\n",
      "The news release document by Ministry of Foreign Affairs (This site is only available in Japanese.)\n",
      "101 문장\n",
      "http://www.mofa.go.jp/mofaj/press/release/24/4/0426_07.html\n",
      "103 문장\n",
      "The name - Sea of Japan (Japan Sea) has been consistently and solely used in \"Limits of Oceans and Seas\" since its first publication in 1928.\n",
      "Error Sea of Japan (Japan Sea) has been consistently and solely used in \"Limits of Oceans and Seas\"\n",
      "104 문장\n",
      "The ROK, however, had not raised this issue in \"United Nations Conference on Standardization of Geographical Names\" or \"the International Hydrographic Organization\" (IHO) until the beginning of the 1990s and the ROK used the name of Sea of Japan (Japan Sea) even in its own nautical charts.\n",
      "Error ROK used the name of Sea of Japan (Japan Sea)\n",
      "105 문장\n",
      "In other words, there is the historic fact that the ROK did not make an objection against the name - Sea of Japan (Japan Sea) even after World War II.\n",
      "Error there is the historic fact that the ROK did not make an objection against the name - Sea of Japan (Japan Sea) even after World War II.\n",
      "106 문장\n",
      "It is in 1995 that the ROK started to use \"East Sea\" in its own nautical charts.\n",
      "108 문장\n",
      "Reference 4: “East Sea” Applied to Multiple Sea Area\n",
      "110 문장\n",
      "There are plural sea areas named \"East Sea\" around the world.\n",
      "112 문장\n",
      "For example, China calls the East China Sea \"Tonghai\", which also means \"East Sea\" in China.\n",
      "113 문장\n",
      "South China Sea is called \"Biendong\" in Vietnam, which means \"East Sea\" in Vietnamese and thus \"East Sea\" is officially used as an English name for the water by the Government of Vietnam.\n",
      "114 문장\n",
      "A name that means \"East Sea\" is frequently used in Europe as well, such as \"Ostsee\" in Germany and \"Ostersjon\" in Sweden both for Baltic Sea which mean \"East Sea\" in each language.\n",
      "116 문장\n",
      "Thus, \"East Sea\" can hardly be an international specific name for a certain limited area of water.\n",
      "117 문장\n",
      "Using such a name used for many different waters in the world as an international name must bring confusion among navigators worldwide.\n",
      "119 문장\n",
      "The ROK, in this regard, calls seas surrounding the Korean Peninsula \"Sohae\", \"Namhae\", and \"Donghae/Tonghae\" depending on the direction from the country (which will be “West Sea”, “South Sea”, and “East Sea” respectively if shown in English).\n",
      "120 문장\n",
      "However, the ROK insists on changing only the name - Sea of Japan (Japan Sea) to \"East Sea\", and have never asserted changing the Yellow Sea to \"West Sea\" and the East China Sea to \"South Sea\".\n"
     ]
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "training_data = []\n",
    "for i in original_data:\n",
    "    sentences = []\n",
    "    # text = i['contents'].replace(\"\\r\",\"\").replace(\"\\n\",\"\")\n",
    "    #sentences = [i.strip() for i in tokenize.sent_tokenize(text)] #문장 단위로 분리 및 문장 앞뒤 공백 제거\n",
    "    sentences = [i.strip() for i in i['contents'].split('\\n')]\n",
    "    \n",
    "    # 두줄 이상 공백이 있는 경우 제거\n",
    "    last = \"\"\n",
    "    remove_indexes = []\n",
    "    for j in range(0, len(sentences)):\n",
    "        if last == \"\" and sentences[j] == \"\":\n",
    "            remove_indexes.append(j)\n",
    "        last = sentences[j]\n",
    "        \n",
    "    for index in sorted(remove_indexes, reverse=True):\n",
    "        del sentences[index]\n",
    "    \n",
    "    # 문장별로 표기 오류 키워드 검색 && 2 + 1 + 2 문장 단위로 자동 구성\n",
    "    # padding\n",
    "    sentences = ['', ''] + sentences + ['', '']\n",
    "    for index in range(2, len(sentences)):\n",
    "        # 빈 문장 제거\n",
    "        if (len(sentences[index]) == 0): continue\n",
    "            \n",
    "        y_class = 11510\n",
    "        y_keyword = \"\"\n",
    "        print(index, \"문장\")\n",
    "        print(sentences[index])\n",
    "        # 현재 문장에 표기 오류가 있는지 확인\n",
    "        for error in i['errors']:\n",
    "            if (error['sentence_no'] != index - 2):\n",
    "                continue\n",
    "            print(\"Error\", error['keyword'])\n",
    "            if sentences[index].find(error['keyword']) != -1:\n",
    "                y_class = error['code']\n",
    "                y_keyword = error['keyword']\n",
    "        \n",
    "        training_data.append([[sentences[j] for j in range(index-2, index+2 + 1)], y_class, y_keyword])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['',\n",
       "   '',\n",
       "   'World Cup>Past Tournaments>2002 Japan-Korea>Overview',\n",
       "   '',\n",
       "   'The 2002 world cup was held in South Korea and Japan and certainly did not disappoint.'],\n",
       "  100,\n",
       "  '2002 Japan-Korea'],\n",
       " [['World Cup>Past Tournaments>2002 Japan-Korea>Overview',\n",
       "   '',\n",
       "   'The 2002 world cup was held in South Korea and Japan and certainly did not disappoint.',\n",
       "   'The opening group stage consisted of 32 teams.',\n",
       "   'Group E was considered the ‘group of death’ including favourites Argentina, England, Sweden and Nigeria.'],\n",
       "  11510,\n",
       "  '']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가공된 데이터는 별도로 저장\n",
    "import json\n",
    "with open(\"data/training_data.txt\", 'w') as outfile:\n",
    "    json.dump(training_data, outfile)\n",
    "training_data[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리 2\n",
    "가공된 데이터를 BERT 모델에 넣을 수 있도록 만들어야합니다. 모델에는 자연어를 그대로 입력할 수 없으니 사전 학습된 BERT 모델의 vocabulary를 활용하여 토큰화 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분활된 토큰 형태: ['[CLS]', 'england', 'drew', 'their', 'opening', 'match', 'against', 'sweden', 'and', 'followed', 'that', 'up', 'with', 'a', 'glorious', '1', '-', '0', 'victory', 'over', 'argentina', 'thanks', 'to', 'a', 'david', 'beck', '##ham', 'penalty', 'in', 'the', 'first', 'half', '.', '[SEP]']\n",
      "숫자형 토큰 형태: [101, 2563, 3881, 2037, 3098, 2674, 2114, 4701, 1998, 2628, 2008, 2039, 2007, 1037, 14013, 1015, 1011, 1014, 3377, 2058, 5619, 4283, 2000, 1037, 2585, 10272, 3511, 6531, 1999, 1996, 2034, 2431, 1012, 102]\n",
      "offsets: [(0, 0), (0, 7), (8, 12), (13, 18), (19, 26), (27, 32), (33, 40), (41, 47), (48, 51), (52, 60), (61, 65), (66, 68), (69, 73), (74, 75), (76, 84), (85, 86), (86, 87), (87, 88), (89, 96), (97, 101), (102, 111), (112, 118), (119, 121), (122, 123), (124, 129), (130, 134), (134, 137), (138, 145), (146, 148), (149, 152), (153, 158), (159, 163), (163, 164), (0, 0)]\n",
      "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "special_tokens_mask: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "special_tokens_mask: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "overflowing: []\n"
     ]
    }
   ],
   "source": [
    "import tokenizers\n",
    "tokenizer = tokenizers.BertWordPieceTokenizer(\n",
    "    f\"data/bert-base-uncased/vocab.txt\", \n",
    "    lowercase=True\n",
    ")\n",
    "tok_tweet = tokenizer.encode(\"England drew their opening match against Sweden and followed that up with a glorious 1-0 victory over Argentina thanks to a David Beckham penalty in the first half.\")\n",
    "print(\"분활된 토큰 형태: \" + str(tok_tweet.tokens))\n",
    "print(\"숫자형 토큰 형태: \" + str(tok_tweet.ids))\n",
    "print(\"offsets: \" + str(tok_tweet.offsets))\n",
    "print(\"attention_mask: \" + str(tok_tweet.attention_mask))\n",
    "print(\"special_tokens_mask: \" + str(tok_tweet.special_tokens_mask))\n",
    "print(\"special_tokens_mask: \" + str(tok_tweet.special_tokens_mask))\n",
    "print(\"overflowing: \" + str(tok_tweet.overflowing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text, y_class, y_keyword):\n",
    "    before = tokenizer.encode(text[0] + \" \" + text[1])\n",
    "    main = tokenizer.encode(text[2])\n",
    "    after = tokenizer.encode(text[3] + \" \" + text[4])\n",
    "\n",
    "\n",
    "    # 토큰 기준으로 키워드가 어디있는지 확인\n",
    "    keyword_position_in_token = -1\n",
    "    keyword_end_in_token = -1\n",
    "    if y_class != 11510:\n",
    "        keyword_position_in_string = text[2].find(y_keyword)\n",
    "        keyword_length_in_string = len(y_keyword) # 43까지...\n",
    "        for j in range(len(main.offsets)):\n",
    "            if keyword_position_in_token == -1 and main.offsets[j][0] >= keyword_position_in_string:\n",
    "                keyword_position_in_token = j\n",
    "            if main.offsets[j][1] == 0: continue\n",
    "            if main.offsets[j][1] <= (keyword_position_in_string + keyword_length_in_string):\n",
    "                keyword_end_in_token = j\n",
    "        keyword_position_in_token -= 1\n",
    "        keyword_end_in_token -= 1\n",
    "    else:        \n",
    "        keyword_position_in_token = 0\n",
    "        keyword_end_in_token = 0\n",
    "        \n",
    "    # ids = cls, classification number, sep, token, sep\n",
    "    ids = [101, y_class, 102] + before.ids[1:] + main.ids[1:] + after.ids[1:]\n",
    "    # mask = len(cls, classification number, sep, token, sep) = 1, else 0\n",
    "    mask = [1] * len(ids)\n",
    "    # token_type_ids len(token, sep) = 1, else 0\n",
    "    token_type_ids = [0,0,0] + [1] * (len(ids) - 3)\n",
    "\n",
    "    targets_start = keyword_position_in_token\n",
    "    targets_end = keyword_end_in_token\n",
    "\n",
    "    # offsets based on ids, token offsets (0,0)(0,0)(0,0)(0,a)...(0,0)\n",
    "    offsets = [(0, 0)] * 3 + before.offsets[1:] + main.offsets[1:] + after.offsets[1:]\n",
    "    \n",
    "    if y_class != 11510:\n",
    "        targets_start += 3 + len(before.offsets) - 1\n",
    "        targets_end += 3 + len(before.offsets) - 1\n",
    "        \n",
    "    # Pad sequence if its length < `max_len`\n",
    "    padding_length = 350 - len(ids)\n",
    "    if padding_length > 0:\n",
    "        ids = ids + ([0] * padding_length)\n",
    "        mask = mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        offsets = offsets + ([(0, 0)] * padding_length)\n",
    "        \n",
    "        \n",
    "    return {\n",
    "            'ids': ids,\n",
    "            'mask': mask, \n",
    "            'token_type_ids': token_type_ids,\n",
    "            'targets_start': targets_start, \n",
    "            'targets_end': targets_end, \n",
    "            'orig_text': text,\n",
    "            'orig_keyword': y_keyword,\n",
    "            'class': y_class,\n",
    "            'offsets': offsets ,\n",
    "            'main_offsets': main.offsets\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "a = preprocessing(training_data[i][0], training_data[i][1], training_data[i][2])\n",
    "a['targets_end']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이토치를 위한 데이터셋 클래스 생성\n",
    "이 클래스는 파이토치에서 데이터를 로드할 때 사용되는 인터페이스입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset:\n",
    "    \"\"\"\n",
    "    Dataset which stores the tweets and returns them as processed features\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        data = preprocessing(\n",
    "            self.dataset[item][0], \n",
    "            self.dataset[item][1], \n",
    "            self.dataset[item][2],\n",
    "        )\n",
    "        \n",
    "        # Return the processed data where the lists are converted to `torch.tensor`s\n",
    "        return {\n",
    "            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n",
    "            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n",
    "            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n",
    "            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n",
    "            'orig_tweet': data[\"orig_text\"],\n",
    "            'orig_selected': data[\"orig_keyword\"],\n",
    "            'sentiment': data[\"class\"],\n",
    "            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이토치 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "class TweetModel(transformers.BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Model class that combines a pretrained bert model with a linear later\n",
    "    \"\"\"\n",
    "    def __init__(self, conf):\n",
    "        super(TweetModel, self).__init__(conf)\n",
    "        # pretrained BERT model을 불러옵니다.\n",
    "        self.bert = transformers.BertModel.from_pretrained(config.BERT_PATH, config=conf)\n",
    "        \n",
    "        # Set 10% dropout to be applied to the BERT backbone's output\n",
    "        # dropout은 은닉층에서 일정 확률로 유닛을 사용하지 않도록(=0) 합니다.\n",
    "        # 따라서 해당 케이스에서는 사용된 유닛만을 이용해 loss를 구하고 grident를 수행합니다.\n",
    "        # 결국 오버피팅 방지 가능!! (하나의 유닛에 의존하는 현상을 제거)\n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "        # 768 is the dimensionality of bert-base-uncased's hidden representations\n",
    "        # Multiplied by 2 since the forward pass concatenates the last two hidden representation layers\n",
    "        # The output will have two dimensions (\"start_logits\", and \"end_logits\")\n",
    "        self.l0 = nn.Linear(768 * 2, 2)\n",
    "        torch.nn.init.normal_(self.l0.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        # Return the hidden states from the BERT backbone\n",
    "        _, _, out = self.bert(\n",
    "            ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        ) # bert_layers x bs x SL x (768)\n",
    "\n",
    "        # Concatenate the last two hidden states\n",
    "        # This is done since experiments have shown that just getting the last layer\n",
    "        # gives out vectors that may be too taylored to the original BERT training objectives (MLM + NSP)\n",
    "        # Sample explanation: https://bert-as-service.readthedocs.io/en/latest/section/faq.html#why-not-the-last-hidden-layer-why-second-to-last\n",
    "        out = torch.cat((out[-1], out[-2]), dim=-1) # bs x SL x (768 * 2)\n",
    "        # Apply 10% dropout to the last 2 hidden states\n",
    "        out = self.drop_out(out) # bs x SL x (768 * 2)\n",
    "        # The \"dropped out\" hidden vectors are now fed into the linear layer to output two scores\n",
    "        logits = self.l0(out) # bs x SL x 2\n",
    "\n",
    "        # Splits the tensor into start_logits and end_logits\n",
    "        # (bs x SL x 2) -> (bs x SL x 1), (bs x SL x 1)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "\n",
    "        start_logits = start_logits.squeeze(-1) # (bs x SL)\n",
    "        end_logits = end_logits.squeeze(-1) # (bs x SL)\n",
    "\n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "    \"\"\"\n",
    "    Return the sum of the cross entropy losses for both the start and end logits\n",
    "    \"\"\"\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    start_loss = loss_fct(start_logits, start_positions)\n",
    "    end_loss = loss_fct(end_logits, end_positions)\n",
    "    total_loss = (start_loss + end_loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, optimizer, device, scheduler=None):\n",
    "    \"\"\"\n",
    "    Trains the bert model on the twitter data\n",
    "    \"\"\"\n",
    "    # Set model to training mode (dropout + sampled batch norm is activated)\n",
    "    model.train()\n",
    "    losses = utils.AverageMeter()\n",
    "    jaccards = utils.AverageMeter()\n",
    "\n",
    "    # Set tqdm to add loading screen and set the length\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    \n",
    "    # Train the model on each batch\n",
    "    for bi, d in enumerate(tk0):\n",
    "\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        targets_start = d[\"targets_start\"]\n",
    "        targets_end = d[\"targets_end\"]\n",
    "        sentiment = d[\"sentiment\"]\n",
    "        orig_selected = d[\"orig_selected\"]\n",
    "        orig_tweet = d[\"orig_tweet\"][2]\n",
    "        offsets = d[\"offsets\"]\n",
    "\n",
    "        # Move ids, masks, and targets to gpu while setting as torch.long\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets_start = targets_start.to(device, dtype=torch.long)\n",
    "        targets_end = targets_end.to(device, dtype=torch.long)\n",
    "\n",
    "        # Reset gradients\n",
    "        model.zero_grad()\n",
    "        # Use ids, masks, and token types as input to the model\n",
    "        # Predict logits for each of the input tokens for each batch\n",
    "        outputs_start, outputs_end = model(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        ) # (bs x SL), (bs x SL)\n",
    "        # Calculate batch loss based on CrossEntropy\n",
    "        loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
    "        # Calculate gradients based on loss\n",
    "        loss.backward()\n",
    "        # Adjust weights based on calculated gradients\n",
    "        optimizer.step()\n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Apply softmax to the start and end logits\n",
    "        # This squeezes each of the logits in a sequence to a value between 0 and 1, while ensuring that they sum to 1\n",
    "        # This is similar to the characteristics of \"probabilities\"\n",
    "        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "        \n",
    "        # Calculate the jaccard score based on the predictions for this batch\n",
    "        jaccard_scores = []\n",
    "        for px, tweet in enumerate(orig_tweet):\n",
    "            selected_tweet = orig_selected[px]\n",
    "            tweet_sentiment = sentiment[px]\n",
    "            jaccard_score, _ = calculate_jaccard_score(\n",
    "                original_tweet=tweet, # Full text of the px'th tweet in the batch\n",
    "                target_string=selected_tweet, # Span containing the specified sentiment for the px'th tweet in the batch\n",
    "                sentiment_val=tweet_sentiment, # Sentiment of the px'th tweet in the batch\n",
    "                idx_start=np.argmax(outputs_start[px, :]), # Predicted start index for the px'th tweet in the batch\n",
    "                idx_end=np.argmax(outputs_end[px, :]), # Predicted end index for the px'th tweet in the batch\n",
    "                offsets=offsets[px] # Offsets for each of the tokens for the px'th tweet in the batch\n",
    "            )\n",
    "            jaccard_scores.append(jaccard_score)\n",
    "        # Update the jaccard score and loss\n",
    "        # For details, refer to `AverageMeter` in https://www.kaggle.com/abhishek/utils\n",
    "        jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
    "        losses.update(loss.item(), ids.size(0))\n",
    "        # Print the average loss and jaccard score at the end of each batch\n",
    "        tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(data_loader, model, device):\n",
    "    \"\"\"\n",
    "    Evaluation function to predict on the test set\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    # I.e., turn off dropout and set batchnorm to use overall mean and variance (from training), rather than batch level mean and variance\n",
    "    # Reference: https://github.com/pytorch/pytorch/issues/5406\n",
    "    model.eval()\n",
    "    losses = utils.AverageMeter()\n",
    "    jaccards = utils.AverageMeter()\n",
    "    \n",
    "    # Turns off gradient calculations (https://datascience.stackexchange.com/questions/32651/what-is-the-use-of-torch-no-grad-in-pytorch)\n",
    "    with torch.no_grad():\n",
    "        tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "        # Make predictions and calculate loss / jaccard score for each batch\n",
    "        for bi, d in enumerate(tk0):\n",
    "            ids = d[\"ids\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            sentiment = d[\"sentiment\"]\n",
    "            orig_selected = d[\"orig_selected\"]\n",
    "            orig_tweet = d[\"orig_tweet\"][2]\n",
    "            targets_start = d[\"targets_start\"]\n",
    "            targets_end = d[\"targets_end\"]\n",
    "            offsets = d[\"offsets\"].numpy()\n",
    "\n",
    "            # Move tensors to GPU for faster matrix calculations\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            targets_start = targets_start.to(device, dtype=torch.long)\n",
    "            targets_end = targets_end.to(device, dtype=torch.long)\n",
    "\n",
    "            # Predict logits for start and end indexes\n",
    "            outputs_start, outputs_end = model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            # Calculate loss for the batch\n",
    "            loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
    "            # Apply softmax to the predicted logits for the start and end indexes\n",
    "            # This converts the \"logits\" to \"probability-like\" scores\n",
    "            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "            # Calculate jaccard scores for each tweet in the batch\n",
    "            jaccard_scores = []\n",
    "            for px, tweet in enumerate(orig_tweet):\n",
    "                selected_tweet = orig_selected[px]\n",
    "                tweet_sentiment = sentiment[px]\n",
    "                jaccard_score, _ = calculate_jaccard_score(\n",
    "                    original_tweet=tweet,\n",
    "                    target_string=selected_tweet,\n",
    "                    sentiment_val=tweet_sentiment,\n",
    "                    idx_start=np.argmax(outputs_start[px, :]),\n",
    "                    idx_end=np.argmax(outputs_end[px, :]),\n",
    "                    offsets=offsets[px]\n",
    "                )\n",
    "                jaccard_scores.append(jaccard_score)\n",
    "\n",
    "            # Update running jaccard score and loss\n",
    "            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
    "            losses.update(loss.item(), ids.size(0))\n",
    "            # Print the running average loss and jaccard score\n",
    "            tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n",
    "    \n",
    "    print(f\"Jaccard = {jaccards.avg}\")\n",
    "    return jaccards.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import tokenizers\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.autonotebook import tqdm\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jaccard_score(\n",
    "    original_tweet, \n",
    "    target_string, \n",
    "    sentiment_val, \n",
    "    idx_start, \n",
    "    idx_end, \n",
    "    offsets,\n",
    "    verbose=False):\n",
    "    \"\"\"\n",
    "    Calculate the jaccard score from the predicted span and the actual span for a batch of tweets\n",
    "    \"\"\"\n",
    "    # A span's end index has to be greater than or equal to the start index\n",
    "    # If this doesn't hold, the start index is set to equal the end index (the span is a single token)\n",
    "    if idx_end < idx_start:\n",
    "        idx_end = idx_start\n",
    "    \n",
    "    # Combine into a string the tokens that belong to the predicted span\n",
    "    filtered_output  = \"\"\n",
    "    for ix in range(idx_start, idx_end + 1):\n",
    "        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n",
    "        # If the token is not the last token in the tweet, and the ending offset of the current token is less\n",
    "        # than the beginning offset of the following token, add a space.\n",
    "        # Basically, add a space when the next token (word piece) corresponds to a new word\n",
    "        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n",
    "            filtered_output += \" \"\n",
    "    #print(filtered_output)\n",
    "    # Set the predicted output as the original tweet when the tweet's sentiment is \"neutral\", or the tweet only contains one word\n",
    "    if sentiment_val == 11510 or len(original_tweet.split()) < 2:\n",
    "        filtered_output = original_tweet\n",
    "    # Calculate the jaccard score between the predicted span, and the actual span\n",
    "    # The IOU (intersection over union) approach is detailed in the utils module's `jaccard` function:\n",
    "    # https://www.kaggle.com/abhishek/utils\n",
    "    jac = utils.jaccard(target_string.strip(), filtered_output.strip())\n",
    "    return jac, filtered_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is Starting for fold=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a269af1a878a45efa27333273e433fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=44.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab2ba96794940ea95aad4a133829d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=44.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.09709647819126917\n",
      "Jaccard Score = 0.09709647819126917\n",
      "Validation score improved (-inf --> 0.09709647819126917). Saving model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385363299d914bd08ed6b8c11cc2d7f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=44.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf0ed16df5647f68a8148de97df73e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=44.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.1388887050710359\n",
      "Jaccard Score = 0.1388887050710359\n",
      "Validation score improved (0.09709647819126917 --> 0.1388887050710359). Saving model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92024a14585e4724952a097bdd0459a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=44.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252314a8fea043fc9ae28231e03aae3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=44.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.1705243093864432\n",
      "Jaccard Score = 0.1705243093864432\n",
      "Validation score improved (0.1388887050710359 --> 0.1705243093864432). Saving model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16d0c2757db4b7fbbd5151441bb6df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=44.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c8d22a6f7c54730be19fe05dd64cf20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=44.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.18150343740128547\n",
      "Jaccard Score = 0.18150343740128547\n",
      "Validation score improved (0.1705243093864432 --> 0.18150343740128547). Saving model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e211f0e5b334436ac4d762fa67ce5af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=44.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0642f05f8f574bdc86d9f51a0b1ce146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=44.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.1899449958428439\n",
      "Jaccard Score = 0.1899449958428439\n",
      "Validation score improved (0.18150343740128547 --> 0.1899449958428439). Saving model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc229912871495d86b98a636056ffcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=44.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3b98f4d73e4e69898316ac65191724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=44.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.1899449958428439\n",
      "Jaccard Score = 0.1899449958428439\n",
      "EarlyStopping counter: 1 out of 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47e983eb9954ff8844c423cb91eaa3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=44.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7eaae0b388427caaf3598ca0f2fc09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=44.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard = 0.1899449958428439\n",
      "Jaccard Score = 0.1899449958428439\n",
      "EarlyStopping counter: 2 out of 2\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "class config:\n",
    "    MAX_LEN = 128\n",
    "    TRAIN_BATCH_SIZE = 4\n",
    "    VALID_BATCH_SIZE = 1\n",
    "    EPOCHS = 5\n",
    "    BERT_PATH = \"../input/bert-base-uncased/\"\n",
    "    MODEL_PATH = \"model.bin\"\n",
    "    TRAINING_FILE = \"../input/tweet-train-folds/train_folds.csv\"\n",
    "    TOKENIZER = tokenizers.BertWordPieceTokenizer(\n",
    "        f\"{BERT_PATH}/vocab.txt\", \n",
    "        lowercase=True\n",
    "    )\n",
    "    \n",
    "train_dataset = TextDataset(training_data)\n",
    "\n",
    "# Instantiate DataLoader with `train_dataset`\n",
    "# This is a generator that yields the dataset in batches\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.TRAIN_BATCH_SIZE,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "valid_data_loader = TextDataset(training_data)\n",
    "\n",
    "# Instantiate DataLoader with `train_dataset`\n",
    "# This is a generator that yields the dataset in batches\n",
    "valid_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.TRAIN_BATCH_SIZE,\n",
    "    num_workers=0\n",
    ")\n",
    "# Set device as `cuda` (GPU)\n",
    "device = torch.device(\"cuda\")\n",
    "# Load pretrained BERT (bert-base-uncased)\n",
    "model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\n",
    "# Output hidden states\n",
    "# This is important to set since we want to concatenate the hidden states from the last 2 BERT layers\n",
    "model_config.output_hidden_states = True\n",
    "# Instantiate our model with `model_config`\n",
    "model = TweetModel(conf=model_config)\n",
    "# Move the model to the GPU\n",
    "model.to(device)\n",
    "\n",
    "# Calculate the number of training steps\n",
    "num_train_steps = int(len(training_data) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
    "# Get the list of named parameters\n",
    "param_optimizer = list(model.named_parameters())\n",
    "# Specify parameters where weight decay shouldn't be applied\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "# Define two sets of parameters: those with weight decay, and those without\n",
    "optimizer_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "# Instantiate AdamW optimizer with our two sets of parameters, and a learning rate of 3e-5\n",
    "optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "# Create a scheduler to set the learning rate at each training step\n",
    "# \"Create a schedule with a learning rate that decreases linearly after linearly increasing during a warmup period.\" (https://pytorch.org/docs/stable/optim.html)\n",
    "# Since num_warmup_steps = 0, the learning rate starts at 3e-5, and then linearly decreases at each training step\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=num_train_steps\n",
    ")\n",
    "\n",
    "# Apply early stopping with patience of 2\n",
    "# This means to stop training new epochs when 2 rounds have passed without any improvement\n",
    "es = utils.EarlyStopping(patience=2, mode=\"max\")\n",
    "fold = 0\n",
    "print(f\"Training is Starting for fold={fold}\")\n",
    "\n",
    "# I'm training only for 3 epochs even though I specified 5!!!\n",
    "for epoch in range(10):\n",
    "    train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n",
    "    jaccard = eval_fn(valid_data_loader, model, device)\n",
    "    print(f\"Jaccard Score = {jaccard}\")\n",
    "    es(jaccard, model, model_path=f\"model_{fold}.bin\")\n",
    "    if es.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TweetModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (drop_out): Dropout(p=0.1, inplace=False)\n",
       "  (l0): Linear(in_features=1536, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\n",
    "model_config.output_hidden_states = True\n",
    "\n",
    "model1 = TweetModel(conf=model_config)\n",
    "model1.to(device)\n",
    "model1.load_state_dict(torch.load(\"model_0.bin\"))\n",
    "model1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2346a98443400e9e7470b623874cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=175.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final_output = []\n",
    "\n",
    "# Instantiate TweetDataset with the test data\n",
    "test_dataset = TextDataset(training_data)\n",
    "\n",
    "# Instantiate DataLoader with `test_dataset`\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=config.VALID_BATCH_SIZE,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Turn of gradient calculations\n",
    "with torch.no_grad():\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    # Predict the span containing the sentiment for each batch\n",
    "    for bi, d in enumerate(tk0):\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        sentiment = d[\"sentiment\"]\n",
    "        orig_selected = d[\"orig_selected\"]\n",
    "        orig_tweet = d[\"orig_tweet\"][2]\n",
    "        targets_start = d[\"targets_start\"]\n",
    "        targets_end = d[\"targets_end\"]\n",
    "        offsets = d[\"offsets\"].numpy()\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets_start = targets_start.to(device, dtype=torch.long)\n",
    "        targets_end = targets_end.to(device, dtype=torch.long)\n",
    "\n",
    "        # Predict start and end logits for each of the five models\n",
    "        outputs_start, outputs_end = model1(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        # Apply softmax to the predicted start and end logits\n",
    "        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "\n",
    "        # Convert the start and end scores to actual predicted spans (in string form)\n",
    "        for px, tweet in enumerate(orig_tweet):\n",
    "            selected_tweet = orig_selected[px]\n",
    "            tweet_sentiment = sentiment[px]\n",
    "            _, output_sentence = calculate_jaccard_score(\n",
    "                original_tweet=tweet,\n",
    "                target_string=selected_tweet,\n",
    "                sentiment_val=tweet_sentiment,\n",
    "                idx_start=np.argmax(outputs_start[px, :]),\n",
    "                idx_end=np.argmax(outputs_end[px, :]),\n",
    "                offsets=offsets[px]\n",
    "            )\n",
    "            final_output.append(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2002 Japan-Korea] , output [2002 Japan-Korea]\n",
      "[Seas of Japan] , output [Seas of Japan ]\n",
      "[Seas of Japan] , output [Seas of Japan ]\n",
      "[Sea of Japan] , output [Sea of Japan ]\n",
      "[Sea of Japan] , output [Sea of Japan ]\n",
      "[Sea of Japan] , output [Sea of Japan ]\n",
      "[Sea of Japan] , output [Sea of Japan ]\n",
      "[Sea of Japan] , output [Sea of Japan ]\n",
      "[Sea of Japan] , output [Sea of Japan ]\n",
      "[Sea of Japan] , output [Sea of Japan]\n",
      "[Sea of Japan] , output [Sea of Japan ]\n",
      "[Sea Of Japan] , output [f J ]\n",
      "[Sea of Japan] , output [Sea of Japan ]\n",
      "[Sea of Japan] , output [Sea of Japan ]\n",
      "[Sea of Japan] , output [Sea of Japan ]\n",
      "[Sea of Japan (Japan Sea)] , output [Sea of Japan (Japan Sea)]\n",
      "[Sea of Japan (Japan Sea)] , output [ea  f  apan  Japan  ea)       ]\n",
      "[Sea of Japan (Japan Sea) is the only internationally established name] , output [Sea ]\n",
      "[Sea of Japan (Japan Sea) became recognized internationally by the early 19th Century] , output [Sea of Japan (Japan Sea) became recognized internationally by the early 19th Century ]\n",
      "[Sea of Japan (Japan Sea)] , output [Sea ]\n",
      "[use the name - Sea of Japan (Japan Sea) for making their nautical chart] , output [d K ]\n",
      "[Sea of Japan (Japan Sea) is established in the guidelines on names of seas, published by the International Hydrographic Organization (IHO)] , output [Sea of Japan (Japan Sea) is established in the guidelines on names of seas, published by the International Hydrographic Organization (IHO) entitled \"Limits of Oceans and Seas\" to which countries in the world refer when making nautical charts.This is be au e t e na e - S a  f Jap n (Jap n Se ) is establ sh d in th  g idelin s o  name ]\n",
      "[Sea of Japan (Japan Sea) established by \"Limits of Oceans and Seas\"] , output [Sea of Japan (Japan Sea) established by \"Limits of Oceans and Seas\" ]\n",
      "[IHO has consistently used Sea of Japan (Japan Sea)] , output [IHO has consistently used Sea of Japan (Japan Sea) as the name ]\n",
      "[If \"East Sea\" were used alongside the name - Sea of Japan (Japan Sea) in \"Limits of Oceans and Seas\" which is the guideline regarding names on sea, it would surely lead to confusion] , output [Sea of Japan (Japan Sea) in \"Limits of Oceans and Seas\" which is the guideline regarding names on sea, it would surely lead to confusion ]\n",
      "[Sea of Japan (Japan Sea) has been the only internationally established name] , output [Sea of Japan (Japan Sea) has been the only internationally established name ]\n",
      "[Sea of Japan (Japan Sea)] , output [Sea of Japan (Japan Sea) ]\n",
      "[The establishment of the name - Sea of Japan (Japan Sea)] , output [Jap n  Japan Sea)  ]\n",
      "[Sea of Japan (Japan Sea)] , output [the na e - S a of J pan  Jap n  ea) was in  Kunyu Wan uo  uantu  b  Ma teo Ri ci (1 02).  ]\n",
      "[Sea of Japan (Japan Sea) was an unknown sea area, and the shape of the Sea of Japan (Japan Sea)] , output [Sea of Japan (Japan Sea) was an unknown sea area, and the shape of the Sea of Japan (Japan Sea) ]\n",
      "[Sea of Japan] , output [Sea of Japan]\n",
      "[Sea of Japan (Japan Sea), Krusenstern described, based on an accurate map, that \"People also call this sea area the Sea of Korea, but because only a small part of this sea touches the Korean coast, it is appropriate to name it \"the Sea of Japan\"] , output [rne  a ound ]\n",
      "[Sea of Japan (Japan Sea)] , output [Sea ]\n",
      "[Sea of Japan (Japan Sea) became established internationally as the name] , output [Sea of Japan (Japan Sea) became established internationally as the name ]\n",
      "[Sea of Japan (Japan Sea)) have solely used the Sea of Japan (Japan Sea)] , output [Sea ]\n",
      "[Sea of Japan had already been internationally recognized and firmly established by the early 19th] , output [Sea of Japan had already been internationally recognized and firmly established by the early 19th ]\n",
      "[Sea of Japan (Japan Sea) established] , output [Sea of Japan (Japan Sea) established by \"Limits of Oceans and Seas\" eferenc  2  The n me   Sea  f J pan (Japan S a) e tab ishe   y \" im ts of Oceans and  ea \"  ]\n",
      "[has consistently used the name - Sea of Japan (Japan Sea) as the name] , output [has ]\n",
      "[Sea of Japan (Japan Sea) was already established internationally as the sole name indicating the Sea of Japan (Japan Sea)] , output [Sea of Japan (Japan Sea) was already established internationally as the sole name ]\n",
      "[Sea of Japan (Japan Sea)] , output [Sea of Japan (Japan Sea)).Ja an did no  part cip te  n the pro ess of t e  stablish en  of t is name at  ll ( efe en e 1:  ]\n",
      "[Sea of Japan] , output [Sea of Japan ]\n",
      "[Sea of Japan (Japan Sea) worldwide] , output [Sea of Japan (Japan Sea) worldwide]\n",
      "[Sea of Japan (Japan Sea) was used solely in the guideline] , output [Sea of Japan (Japan Sea) was used solely in the guideline]\n",
      "[Sea of Japan (Japan Sea) was used solely in the guideline] , output [          ]\n",
      "[Sea of Japan (Japan Sea) was used solely in the guideline] , output [Sea of Japan (Japan Sea) was used solely in the guideline]\n",
      "[The name Sea of Japan (Japan Sea) was described solely in the draft] , output [The ]\n",
      "[ROK did not make an objection against the name - Sea of Japan (Japan Sea).] , output [RO]\n",
      "[Sea of Japan] , output [Sea of Japan]\n",
      "[Sea of Japan (Japan Sea) has been consistently and solely used in \"Limits of Oceans and Seas\"] , output [Sea of Japan (Japan Sea) has been consistently and solely used in \"Limits of Oceans and Seas\" ]\n",
      "[ROK used the name of Sea of Japan (Japan Sea)] , output [RO]\n",
      "[there is the historic fact that the ROK did not make an objection against the name - Sea of Japan (Japan Sea) even after World War II.] , output []\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(test_dataset)):\n",
    "    if test_dataset[i]['sentiment'] != 11510:\n",
    "        print('['+test_dataset[i]['orig_selected']+']',\", output\", '[' +final_output[i] + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
